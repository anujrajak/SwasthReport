(globalThis.TURBOPACK || (globalThis.TURBOPACK = [])).push([typeof document === "object" ? document.currentScript : undefined,
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bson.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.toUTF8 = exports.getBigInt64LE = exports.getFloat64LE = exports.getInt32LE = exports.UUID = exports.Timestamp = exports.serialize = exports.ObjectId = exports.MinKey = exports.MaxKey = exports.Long = exports.Int32 = exports.EJSON = exports.Double = exports.deserialize = exports.Decimal128 = exports.DBRef = exports.Code = exports.calculateObjectSize = exports.BSONType = exports.BSONSymbol = exports.BSONRegExp = exports.BSONError = exports.BSON = exports.Binary = void 0;
exports.parseToElementsToArray = parseToElementsToArray;
exports.pluckBSONSerializeOptions = pluckBSONSerializeOptions;
exports.resolveBSONOptions = resolveBSONOptions;
exports.parseUtf8ValidationOption = parseUtf8ValidationOption;
/* eslint-disable no-restricted-imports */ const bson_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/bson@7.0.0/node_modules/bson/lib/bson.mjs [app-client] (ecmascript)");
var bson_2 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/bson@7.0.0/node_modules/bson/lib/bson.mjs [app-client] (ecmascript)");
Object.defineProperty(exports, "Binary", {
    enumerable: true,
    get: function() {
        return bson_2.Binary;
    }
});
Object.defineProperty(exports, "BSON", {
    enumerable: true,
    get: function() {
        return bson_2.BSON;
    }
});
Object.defineProperty(exports, "BSONError", {
    enumerable: true,
    get: function() {
        return bson_2.BSONError;
    }
});
Object.defineProperty(exports, "BSONRegExp", {
    enumerable: true,
    get: function() {
        return bson_2.BSONRegExp;
    }
});
Object.defineProperty(exports, "BSONSymbol", {
    enumerable: true,
    get: function() {
        return bson_2.BSONSymbol;
    }
});
Object.defineProperty(exports, "BSONType", {
    enumerable: true,
    get: function() {
        return bson_2.BSONType;
    }
});
Object.defineProperty(exports, "calculateObjectSize", {
    enumerable: true,
    get: function() {
        return bson_2.calculateObjectSize;
    }
});
Object.defineProperty(exports, "Code", {
    enumerable: true,
    get: function() {
        return bson_2.Code;
    }
});
Object.defineProperty(exports, "DBRef", {
    enumerable: true,
    get: function() {
        return bson_2.DBRef;
    }
});
Object.defineProperty(exports, "Decimal128", {
    enumerable: true,
    get: function() {
        return bson_2.Decimal128;
    }
});
Object.defineProperty(exports, "deserialize", {
    enumerable: true,
    get: function() {
        return bson_2.deserialize;
    }
});
Object.defineProperty(exports, "Double", {
    enumerable: true,
    get: function() {
        return bson_2.Double;
    }
});
Object.defineProperty(exports, "EJSON", {
    enumerable: true,
    get: function() {
        return bson_2.EJSON;
    }
});
Object.defineProperty(exports, "Int32", {
    enumerable: true,
    get: function() {
        return bson_2.Int32;
    }
});
Object.defineProperty(exports, "Long", {
    enumerable: true,
    get: function() {
        return bson_2.Long;
    }
});
Object.defineProperty(exports, "MaxKey", {
    enumerable: true,
    get: function() {
        return bson_2.MaxKey;
    }
});
Object.defineProperty(exports, "MinKey", {
    enumerable: true,
    get: function() {
        return bson_2.MinKey;
    }
});
Object.defineProperty(exports, "ObjectId", {
    enumerable: true,
    get: function() {
        return bson_2.ObjectId;
    }
});
Object.defineProperty(exports, "serialize", {
    enumerable: true,
    get: function() {
        return bson_2.serialize;
    }
});
Object.defineProperty(exports, "Timestamp", {
    enumerable: true,
    get: function() {
        return bson_2.Timestamp;
    }
});
Object.defineProperty(exports, "UUID", {
    enumerable: true,
    get: function() {
        return bson_2.UUID;
    }
});
function parseToElementsToArray(bytes, offset) {
    const res = bson_1.BSON.onDemand.parseToElements(bytes, offset);
    return Array.isArray(res) ? res : [
        ...res
    ];
}
exports.getInt32LE = bson_1.BSON.onDemand.NumberUtils.getInt32LE;
exports.getFloat64LE = bson_1.BSON.onDemand.NumberUtils.getFloat64LE;
exports.getBigInt64LE = bson_1.BSON.onDemand.NumberUtils.getBigInt64LE;
exports.toUTF8 = bson_1.BSON.onDemand.ByteUtils.toUTF8;
function pluckBSONSerializeOptions(options) {
    const { fieldsAsRaw, useBigInt64, promoteValues, promoteBuffers, promoteLongs, serializeFunctions, ignoreUndefined, bsonRegExp, raw, enableUtf8Validation } = options;
    return {
        fieldsAsRaw,
        useBigInt64,
        promoteValues,
        promoteBuffers,
        promoteLongs,
        serializeFunctions,
        ignoreUndefined,
        bsonRegExp,
        raw,
        enableUtf8Validation
    };
}
/**
 * Merge the given BSONSerializeOptions, preferring options over the parent's options, and
 * substituting defaults for values not set.
 *
 * @internal
 */ function resolveBSONOptions(options, parent) {
    const parentOptions = parent?.bsonOptions;
    return {
        raw: options?.raw ?? parentOptions?.raw ?? false,
        useBigInt64: options?.useBigInt64 ?? parentOptions?.useBigInt64 ?? false,
        promoteLongs: options?.promoteLongs ?? parentOptions?.promoteLongs ?? true,
        promoteValues: options?.promoteValues ?? parentOptions?.promoteValues ?? true,
        promoteBuffers: options?.promoteBuffers ?? parentOptions?.promoteBuffers ?? false,
        ignoreUndefined: options?.ignoreUndefined ?? parentOptions?.ignoreUndefined ?? false,
        bsonRegExp: options?.bsonRegExp ?? parentOptions?.bsonRegExp ?? false,
        serializeFunctions: options?.serializeFunctions ?? parentOptions?.serializeFunctions ?? false,
        fieldsAsRaw: options?.fieldsAsRaw ?? parentOptions?.fieldsAsRaw ?? {},
        enableUtf8Validation: options?.enableUtf8Validation ?? parentOptions?.enableUtf8Validation ?? true
    };
}
/** @internal */ function parseUtf8ValidationOption(options) {
    const enableUtf8Validation = options?.enableUtf8Validation;
    if (enableUtf8Validation === false) {
        return {
            utf8: false
        };
    }
    return {
        utf8: {
            writeErrors: false
        }
    };
} //# sourceMappingURL=bson.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.MongoWriteConcernError = exports.MongoServerSelectionError = exports.MongoSystemError = exports.MongoMissingDependencyError = exports.MongoMissingCredentialsError = exports.MongoCompatibilityError = exports.MongoInvalidArgumentError = exports.MongoParseError = exports.MongoNetworkTimeoutError = exports.MongoNetworkError = exports.MongoClientClosedError = exports.MongoTopologyClosedError = exports.MongoCursorExhaustedError = exports.MongoServerClosedError = exports.MongoCursorInUseError = exports.MongoOperationTimeoutError = exports.MongoUnexpectedServerResponseError = exports.MongoGridFSChunkError = exports.MongoGridFSStreamError = exports.MongoTailableCursorError = exports.MongoChangeStreamError = exports.MongoClientBulkWriteExecutionError = exports.MongoClientBulkWriteCursorError = exports.MongoClientBulkWriteError = exports.MongoGCPError = exports.MongoAzureError = exports.MongoOIDCError = exports.MongoAWSError = exports.MongoKerberosError = exports.MongoExpiredSessionError = exports.MongoTransactionError = exports.MongoNotConnectedError = exports.MongoDecompressionError = exports.MongoBatchReExecutionError = exports.MongoStalePrimaryError = exports.MongoRuntimeError = exports.MongoAPIError = exports.MongoDriverError = exports.MongoServerError = exports.MongoError = exports.MongoErrorLabel = exports.GET_MORE_RESUMABLE_CODES = exports.MONGODB_ERROR_CODES = exports.NODE_IS_RECOVERING_ERROR_MESSAGE = exports.LEGACY_NOT_PRIMARY_OR_SECONDARY_ERROR_MESSAGE = exports.LEGACY_NOT_WRITABLE_PRIMARY_ERROR_MESSAGE = void 0;
exports.needsRetryableWriteLabel = needsRetryableWriteLabel;
exports.isRetryableWriteError = isRetryableWriteError;
exports.isRetryableReadError = isRetryableReadError;
exports.isNodeShuttingDownError = isNodeShuttingDownError;
exports.isSDAMUnrecoverableError = isSDAMUnrecoverableError;
exports.isNetworkTimeoutError = isNetworkTimeoutError;
exports.isResumableError = isResumableError;
/**
 * @internal
 * The legacy error message from the server that indicates the node is not a writable primary
 * https://github.com/mongodb/specifications/blob/921232976f9913cf17415b5ef937ee772e45e6ae/source/server-discovery-and-monitoring/server-discovery-and-monitoring.md#not-writable-primary-and-node-is-recovering
 */ exports.LEGACY_NOT_WRITABLE_PRIMARY_ERROR_MESSAGE = new RegExp('not master', 'i');
/**
 * @internal
 * The legacy error message from the server that indicates the node is not a primary or secondary
 * https://github.com/mongodb/specifications/blob/921232976f9913cf17415b5ef937ee772e45e6ae/source/server-discovery-and-monitoring/server-discovery-and-monitoring.md#not-writable-primary-and-node-is-recovering
 */ exports.LEGACY_NOT_PRIMARY_OR_SECONDARY_ERROR_MESSAGE = new RegExp('not master or secondary', 'i');
/**
 * @internal
 * The error message from the server that indicates the node is recovering
 * https://github.com/mongodb/specifications/blob/921232976f9913cf17415b5ef937ee772e45e6ae/source/server-discovery-and-monitoring/server-discovery-and-monitoring.md#not-writable-primary-and-node-is-recovering
 */ exports.NODE_IS_RECOVERING_ERROR_MESSAGE = new RegExp('node is recovering', 'i');
/** @internal MongoDB Error Codes */ exports.MONGODB_ERROR_CODES = Object.freeze({
    HostUnreachable: 6,
    HostNotFound: 7,
    AuthenticationFailed: 18,
    NetworkTimeout: 89,
    ShutdownInProgress: 91,
    PrimarySteppedDown: 189,
    ExceededTimeLimit: 262,
    SocketException: 9001,
    NotWritablePrimary: 10107,
    InterruptedAtShutdown: 11600,
    InterruptedDueToReplStateChange: 11602,
    NotPrimaryNoSecondaryOk: 13435,
    NotPrimaryOrSecondary: 13436,
    StaleShardVersion: 63,
    StaleEpoch: 150,
    StaleConfig: 13388,
    RetryChangeStream: 234,
    FailedToSatisfyReadPreference: 133,
    CursorNotFound: 43,
    LegacyNotPrimary: 10058,
    // WriteConcernTimeout is WriteConcernFailed on pre-8.1 servers
    WriteConcernTimeout: 64,
    NamespaceNotFound: 26,
    IllegalOperation: 20,
    MaxTimeMSExpired: 50,
    UnknownReplWriteConcern: 79,
    UnsatisfiableWriteConcern: 100,
    Reauthenticate: 391,
    ReadConcernMajorityNotAvailableYet: 134
});
// From spec https://github.com/mongodb/specifications/blob/921232976f9913cf17415b5ef937ee772e45e6ae/source/change-streams/change-streams.md#resumable-error
exports.GET_MORE_RESUMABLE_CODES = new Set([
    exports.MONGODB_ERROR_CODES.HostUnreachable,
    exports.MONGODB_ERROR_CODES.HostNotFound,
    exports.MONGODB_ERROR_CODES.NetworkTimeout,
    exports.MONGODB_ERROR_CODES.ShutdownInProgress,
    exports.MONGODB_ERROR_CODES.PrimarySteppedDown,
    exports.MONGODB_ERROR_CODES.ExceededTimeLimit,
    exports.MONGODB_ERROR_CODES.SocketException,
    exports.MONGODB_ERROR_CODES.NotWritablePrimary,
    exports.MONGODB_ERROR_CODES.InterruptedAtShutdown,
    exports.MONGODB_ERROR_CODES.InterruptedDueToReplStateChange,
    exports.MONGODB_ERROR_CODES.NotPrimaryNoSecondaryOk,
    exports.MONGODB_ERROR_CODES.NotPrimaryOrSecondary,
    exports.MONGODB_ERROR_CODES.StaleShardVersion,
    exports.MONGODB_ERROR_CODES.StaleEpoch,
    exports.MONGODB_ERROR_CODES.StaleConfig,
    exports.MONGODB_ERROR_CODES.RetryChangeStream,
    exports.MONGODB_ERROR_CODES.FailedToSatisfyReadPreference,
    exports.MONGODB_ERROR_CODES.CursorNotFound
]);
/** @public */ exports.MongoErrorLabel = Object.freeze({
    RetryableWriteError: 'RetryableWriteError',
    TransientTransactionError: 'TransientTransactionError',
    UnknownTransactionCommitResult: 'UnknownTransactionCommitResult',
    ResumableChangeStreamError: 'ResumableChangeStreamError',
    HandshakeError: 'HandshakeError',
    ResetPool: 'ResetPool',
    PoolRequestedRetry: 'PoolRequestedRetry',
    InterruptInUseConnections: 'InterruptInUseConnections',
    NoWritesPerformed: 'NoWritesPerformed'
});
function isAggregateError(e) {
    return e != null && typeof e === 'object' && 'errors' in e && Array.isArray(e.errors);
}
/**
 * @public
 * @category Error
 *
 * @privateRemarks
 * mongodb-client-encryption has a dependency on this error, it uses the constructor with a string argument
 */ class MongoError extends Error {
    get errorLabels() {
        return Array.from(this.errorLabelSet);
    }
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, options){
        super(message, options);
        /** @internal */ this.errorLabelSet = new Set();
    }
    /** @internal */ static buildErrorMessage(e) {
        if (typeof e === 'string') {
            return e;
        }
        if (isAggregateError(e) && e.message.length === 0) {
            return e.errors.length === 0 ? 'AggregateError has an empty errors array. Please check the `cause` property for more information.' : e.errors.map(({ message })=>message).join(', ');
        }
        return e != null && typeof e === 'object' && 'message' in e && typeof e.message === 'string' ? e.message : 'empty error message';
    }
    get name() {
        return 'MongoError';
    }
    /** Legacy name for server error responses */ get errmsg() {
        return this.message;
    }
    /**
     * Checks the error to see if it has an error label
     *
     * @param label - The error label to check for
     * @returns returns true if the error has the provided error label
     */ hasErrorLabel(label) {
        return this.errorLabelSet.has(label);
    }
    addErrorLabel(label) {
        this.errorLabelSet.add(label);
    }
}
exports.MongoError = MongoError;
/**
 * An error coming from the mongo server
 *
 * @public
 * @category Error
 */ class MongoServerError extends MongoError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message.message || message.errmsg || message.$err || 'n/a');
        if (message.errorLabels) {
            for (const label of message.errorLabels)this.addErrorLabel(label);
        }
        this.errorResponse = message;
        for(const name in message){
            if (name !== 'errorLabels' && name !== 'errmsg' && name !== 'message' && name !== 'errorResponse') {
                this[name] = message[name];
            }
        }
    }
    get name() {
        return 'MongoServerError';
    }
}
exports.MongoServerError = MongoServerError;
/**
 * An error generated by the driver
 *
 * @public
 * @category Error
 */ class MongoDriverError extends MongoError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, options){
        super(message, options);
    }
    get name() {
        return 'MongoDriverError';
    }
}
exports.MongoDriverError = MongoDriverError;
/**
 * An error generated when the driver API is used incorrectly
 *
 * @privateRemarks
 * Should **never** be directly instantiated
 *
 * @public
 * @category Error
 */ class MongoAPIError extends MongoDriverError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, options){
        super(message, options);
    }
    get name() {
        return 'MongoAPIError';
    }
}
exports.MongoAPIError = MongoAPIError;
/**
 * An error generated when the driver encounters unexpected input
 * or reaches an unexpected/invalid internal state.
 *
 * @privateRemarks
 * Should **never** be directly instantiated.
 *
 * @public
 * @category Error
 */ class MongoRuntimeError extends MongoDriverError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, options){
        super(message, options);
    }
    get name() {
        return 'MongoRuntimeError';
    }
}
exports.MongoRuntimeError = MongoRuntimeError;
/**
 * An error generated when a primary server is marked stale, never directly thrown
 *
 * @public
 * @category Error
 */ class MongoStalePrimaryError extends MongoRuntimeError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, options){
        super(message, options);
    }
    get name() {
        return 'MongoStalePrimaryError';
    }
}
exports.MongoStalePrimaryError = MongoStalePrimaryError;
/**
 * An error generated when a batch command is re-executed after one of the commands in the batch
 * has failed
 *
 * @public
 * @category Error
 */ class MongoBatchReExecutionError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message = 'This batch has already been executed, create new batch to execute'){
        super(message);
    }
    get name() {
        return 'MongoBatchReExecutionError';
    }
}
exports.MongoBatchReExecutionError = MongoBatchReExecutionError;
/**
 * An error generated when the driver fails to decompress
 * data received from the server.
 *
 * @public
 * @category Error
 */ class MongoDecompressionError extends MongoRuntimeError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoDecompressionError';
    }
}
exports.MongoDecompressionError = MongoDecompressionError;
/**
 * An error thrown when the user attempts to operate on a database or collection through a MongoClient
 * that has not yet successfully called the "connect" method
 *
 * @public
 * @category Error
 */ class MongoNotConnectedError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoNotConnectedError';
    }
}
exports.MongoNotConnectedError = MongoNotConnectedError;
/**
 * An error generated when the user makes a mistake in the usage of transactions.
 * (e.g. attempting to commit a transaction with a readPreference other than primary)
 *
 * @public
 * @category Error
 */ class MongoTransactionError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoTransactionError';
    }
}
exports.MongoTransactionError = MongoTransactionError;
/**
 * An error generated when the user attempts to operate
 * on a session that has expired or has been closed.
 *
 * @public
 * @category Error
 */ class MongoExpiredSessionError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message = 'Cannot use a session that has ended'){
        super(message);
    }
    get name() {
        return 'MongoExpiredSessionError';
    }
}
exports.MongoExpiredSessionError = MongoExpiredSessionError;
/**
 * A error generated when the user attempts to authenticate
 * via Kerberos, but fails to connect to the Kerberos client.
 *
 * @public
 * @category Error
 */ class MongoKerberosError extends MongoRuntimeError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoKerberosError';
    }
}
exports.MongoKerberosError = MongoKerberosError;
/**
 * A error generated when the user attempts to authenticate
 * via AWS, but fails
 *
 * @public
 * @category Error
 */ class MongoAWSError extends MongoRuntimeError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, options){
        super(message, options);
    }
    get name() {
        return 'MongoAWSError';
    }
}
exports.MongoAWSError = MongoAWSError;
/**
 * A error generated when the user attempts to authenticate
 * via OIDC callbacks, but fails.
 *
 * @public
 * @category Error
 */ class MongoOIDCError extends MongoRuntimeError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoOIDCError';
    }
}
exports.MongoOIDCError = MongoOIDCError;
/**
 * A error generated when the user attempts to authenticate
 * via Azure, but fails.
 *
 * @public
 * @category Error
 */ class MongoAzureError extends MongoOIDCError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoAzureError';
    }
}
exports.MongoAzureError = MongoAzureError;
/**
 * A error generated when the user attempts to authenticate
 * via GCP, but fails.
 *
 * @public
 * @category Error
 */ class MongoGCPError extends MongoOIDCError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoGCPError';
    }
}
exports.MongoGCPError = MongoGCPError;
/**
 * An error indicating that an error occurred when executing the bulk write.
 *
 * @public
 * @category Error
 */ class MongoClientBulkWriteError extends MongoServerError {
    /**
     * Initialize the client bulk write error.
     * @param message - The error message.
     */ constructor(message){
        super(message);
        this.writeConcernErrors = [];
        this.writeErrors = new Map();
    }
    get name() {
        return 'MongoClientBulkWriteError';
    }
}
exports.MongoClientBulkWriteError = MongoClientBulkWriteError;
/**
 * An error indicating that an error occurred when processing bulk write results.
 *
 * @public
 * @category Error
 */ class MongoClientBulkWriteCursorError extends MongoRuntimeError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoClientBulkWriteCursorError';
    }
}
exports.MongoClientBulkWriteCursorError = MongoClientBulkWriteCursorError;
/**
 * An error indicating that an error occurred on the client when executing a client bulk write.
 *
 * @public
 * @category Error
 */ class MongoClientBulkWriteExecutionError extends MongoRuntimeError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoClientBulkWriteExecutionError';
    }
}
exports.MongoClientBulkWriteExecutionError = MongoClientBulkWriteExecutionError;
/**
 * An error generated when a ChangeStream operation fails to execute.
 *
 * @public
 * @category Error
 */ class MongoChangeStreamError extends MongoRuntimeError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoChangeStreamError';
    }
}
exports.MongoChangeStreamError = MongoChangeStreamError;
/**
 * An error thrown when the user calls a function or method not supported on a tailable cursor
 *
 * @public
 * @category Error
 */ class MongoTailableCursorError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message = 'Tailable cursor does not support this operation'){
        super(message);
    }
    get name() {
        return 'MongoTailableCursorError';
    }
}
exports.MongoTailableCursorError = MongoTailableCursorError;
/** An error generated when a GridFSStream operation fails to execute.
 *
 * @public
 * @category Error
 */ class MongoGridFSStreamError extends MongoRuntimeError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoGridFSStreamError';
    }
}
exports.MongoGridFSStreamError = MongoGridFSStreamError;
/**
 * An error generated when a malformed or invalid chunk is
 * encountered when reading from a GridFSStream.
 *
 * @public
 * @category Error
 */ class MongoGridFSChunkError extends MongoRuntimeError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoGridFSChunkError';
    }
}
exports.MongoGridFSChunkError = MongoGridFSChunkError;
/**
 * An error generated when a **parsable** unexpected response comes from the server.
 * This is generally an error where the driver in a state expecting a certain behavior to occur in
 * the next message from MongoDB but it receives something else.
 * This error **does not** represent an issue with wire message formatting.
 *
 * #### Example
 * When an operation fails, it is the driver's job to retry it. It must perform serverSelection
 * again to make sure that it attempts the operation against a server in a good state. If server
 * selection returns a server that does not support retryable operations, this error is used.
 * This scenario is unlikely as retryable support would also have been determined on the first attempt
 * but it is possible the state change could report a selectable server that does not support retries.
 *
 * @public
 * @category Error
 */ class MongoUnexpectedServerResponseError extends MongoRuntimeError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, options){
        super(message, options);
    }
    get name() {
        return 'MongoUnexpectedServerResponseError';
    }
}
exports.MongoUnexpectedServerResponseError = MongoUnexpectedServerResponseError;
/**
 * @public
 * @category Error
 *
 * The `MongoOperationTimeoutError` class represents an error that occurs when an operation could not be completed within the specified `timeoutMS`.
 * It is generated by the driver in support of the "client side operation timeout" feature so inherits from `MongoDriverError`.
 * When `timeoutMS` is enabled `MongoServerError`s relating to `MaxTimeExpired` errors will be converted to `MongoOperationTimeoutError`
 *
 * @example
 * ```ts
 * try {
 *   await blogs.insertOne(blogPost, { timeoutMS: 60_000 })
 * } catch (error) {
 *   if (error instanceof MongoOperationTimeoutError) {
 *     console.log(`Oh no! writer's block!`, error);
 *   }
 * }
 * ```
 */ class MongoOperationTimeoutError extends MongoDriverError {
    get name() {
        return 'MongoOperationTimeoutError';
    }
}
exports.MongoOperationTimeoutError = MongoOperationTimeoutError;
/**
 * An error thrown when the user attempts to add options to a cursor that has already been
 * initialized
 *
 * @public
 * @category Error
 */ class MongoCursorInUseError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message = 'Cursor is already initialized'){
        super(message);
    }
    get name() {
        return 'MongoCursorInUseError';
    }
}
exports.MongoCursorInUseError = MongoCursorInUseError;
/**
 * An error generated when an attempt is made to operate
 * on a closed/closing server.
 *
 * @public
 * @category Error
 */ class MongoServerClosedError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message = 'Server is closed'){
        super(message);
    }
    get name() {
        return 'MongoServerClosedError';
    }
}
exports.MongoServerClosedError = MongoServerClosedError;
/**
 * An error thrown when an attempt is made to read from a cursor that has been exhausted
 *
 * @public
 * @category Error
 */ class MongoCursorExhaustedError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message || 'Cursor is exhausted');
    }
    get name() {
        return 'MongoCursorExhaustedError';
    }
}
exports.MongoCursorExhaustedError = MongoCursorExhaustedError;
/**
 * An error generated when an attempt is made to operate on a
 * dropped, or otherwise unavailable, database.
 *
 * @public
 * @category Error
 */ class MongoTopologyClosedError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message = 'Topology is closed'){
        super(message);
    }
    get name() {
        return 'MongoTopologyClosedError';
    }
}
exports.MongoTopologyClosedError = MongoTopologyClosedError;
/**
 * An error generated when the MongoClient is closed and async
 * operations are interrupted.
 *
 * @public
 * @category Error
 */ class MongoClientClosedError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(){
        super('Operation interrupted because client was closed');
    }
    get name() {
        return 'MongoClientClosedError';
    }
}
exports.MongoClientClosedError = MongoClientClosedError;
/**
 * An error indicating an issue with the network, including TCP errors and timeouts.
 * @public
 * @category Error
 */ class MongoNetworkError extends MongoError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, options){
        super(message, {
            cause: options?.cause
        });
        this.beforeHandshake = !!options?.beforeHandshake;
    }
    get name() {
        return 'MongoNetworkError';
    }
}
exports.MongoNetworkError = MongoNetworkError;
/**
 * An error indicating a network timeout occurred
 * @public
 * @category Error
 *
 * @privateRemarks
 * mongodb-client-encryption has a dependency on this error with an instanceof check
 */ class MongoNetworkTimeoutError extends MongoNetworkError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, options){
        super(message, options);
    }
    get name() {
        return 'MongoNetworkTimeoutError';
    }
}
exports.MongoNetworkTimeoutError = MongoNetworkTimeoutError;
/**
 * An error used when attempting to parse a value (like a connection string)
 * @public
 * @category Error
 */ class MongoParseError extends MongoDriverError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoParseError';
    }
}
exports.MongoParseError = MongoParseError;
/**
 * An error generated when the user supplies malformed or unexpected arguments
 * or when a required argument or field is not provided.
 *
 *
 * @public
 * @category Error
 */ class MongoInvalidArgumentError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, options){
        super(message, options);
    }
    get name() {
        return 'MongoInvalidArgumentError';
    }
}
exports.MongoInvalidArgumentError = MongoInvalidArgumentError;
/**
 * An error generated when a feature that is not enabled or allowed for the current server
 * configuration is used
 *
 *
 * @public
 * @category Error
 */ class MongoCompatibilityError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoCompatibilityError';
    }
}
exports.MongoCompatibilityError = MongoCompatibilityError;
/**
 * An error generated when the user fails to provide authentication credentials before attempting
 * to connect to a mongo server instance.
 *
 *
 * @public
 * @category Error
 */ class MongoMissingCredentialsError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoMissingCredentialsError';
    }
}
exports.MongoMissingCredentialsError = MongoMissingCredentialsError;
/**
 * An error generated when a required module or dependency is not present in the local environment
 *
 * @public
 * @category Error
 */ class MongoMissingDependencyError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, options){
        super(message, options);
        this.dependencyName = options.dependencyName;
    }
    get name() {
        return 'MongoMissingDependencyError';
    }
}
exports.MongoMissingDependencyError = MongoMissingDependencyError;
/**
 * An error signifying a general system issue
 * @public
 * @category Error
 */ class MongoSystemError extends MongoError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, reason){
        if (reason && reason.error) {
            super(MongoError.buildErrorMessage(reason.error.message || reason.error), {
                cause: reason.error
            });
        } else {
            super(message);
        }
        if (reason) {
            this.reason = reason;
        }
        this.code = reason.error?.code;
    }
    get name() {
        return 'MongoSystemError';
    }
}
exports.MongoSystemError = MongoSystemError;
/**
 * An error signifying a client-side server selection error
 * @public
 * @category Error
 */ class MongoServerSelectionError extends MongoSystemError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, reason){
        super(message, reason);
    }
    get name() {
        return 'MongoServerSelectionError';
    }
}
exports.MongoServerSelectionError = MongoServerSelectionError;
/**
 * An error thrown when the server reports a writeConcernError
 * @public
 * @category Error
 */ class MongoWriteConcernError extends MongoServerError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(result){
        super({
            ...result.writeConcernError,
            ...result
        });
        this.errInfo = result.writeConcernError.errInfo;
        this.result = result;
    }
    get name() {
        return 'MongoWriteConcernError';
    }
}
exports.MongoWriteConcernError = MongoWriteConcernError;
// https://github.com/mongodb/specifications/blob/master/source/retryable-reads/retryable-reads.md#retryable-error
const RETRYABLE_READ_ERROR_CODES = new Set([
    exports.MONGODB_ERROR_CODES.HostUnreachable,
    exports.MONGODB_ERROR_CODES.HostNotFound,
    exports.MONGODB_ERROR_CODES.NetworkTimeout,
    exports.MONGODB_ERROR_CODES.ShutdownInProgress,
    exports.MONGODB_ERROR_CODES.PrimarySteppedDown,
    exports.MONGODB_ERROR_CODES.SocketException,
    exports.MONGODB_ERROR_CODES.NotWritablePrimary,
    exports.MONGODB_ERROR_CODES.InterruptedAtShutdown,
    exports.MONGODB_ERROR_CODES.InterruptedDueToReplStateChange,
    exports.MONGODB_ERROR_CODES.NotPrimaryNoSecondaryOk,
    exports.MONGODB_ERROR_CODES.NotPrimaryOrSecondary,
    exports.MONGODB_ERROR_CODES.ExceededTimeLimit,
    exports.MONGODB_ERROR_CODES.ReadConcernMajorityNotAvailableYet
]);
// see: https://github.com/mongodb/specifications/blob/master/source/retryable-writes/retryable-writes.md#terms
const RETRYABLE_WRITE_ERROR_CODES = RETRYABLE_READ_ERROR_CODES;
function needsRetryableWriteLabel(error, maxWireVersion, serverType) {
    // pre-4.4 server, then the driver adds an error label for every valid case
    // execute operation will only inspect the label, code/message logic is handled here
    if (error instanceof MongoNetworkError) {
        return true;
    }
    if (error instanceof MongoError) {
        if ((maxWireVersion >= 9 || isRetryableWriteError(error)) && !error.hasErrorLabel(exports.MongoErrorLabel.HandshakeError)) {
            // If we already have the error label no need to add it again. 4.4+ servers add the label.
            // In the case where we have a handshake error, need to fall down to the logic checking
            // the codes.
            return false;
        }
    }
    if (error instanceof MongoWriteConcernError) {
        if (serverType === 'Mongos' && maxWireVersion < 9) {
            // use original top-level code from server response
            return RETRYABLE_WRITE_ERROR_CODES.has(error.result.code ?? 0);
        }
        const code = error.result.writeConcernError.code ?? Number(error.code);
        return RETRYABLE_WRITE_ERROR_CODES.has(Number.isNaN(code) ? 0 : code);
    }
    if (error instanceof MongoError) {
        return RETRYABLE_WRITE_ERROR_CODES.has(Number(error.code));
    }
    const isNotWritablePrimaryError = exports.LEGACY_NOT_WRITABLE_PRIMARY_ERROR_MESSAGE.test(error.message);
    if (isNotWritablePrimaryError) {
        return true;
    }
    const isNodeIsRecoveringError = exports.NODE_IS_RECOVERING_ERROR_MESSAGE.test(error.message);
    if (isNodeIsRecoveringError) {
        return true;
    }
    return false;
}
function isRetryableWriteError(error) {
    return error.hasErrorLabel(exports.MongoErrorLabel.RetryableWriteError) || error.hasErrorLabel(exports.MongoErrorLabel.PoolRequestedRetry);
}
/** Determines whether an error is something the driver should attempt to retry */ function isRetryableReadError(error) {
    const hasRetryableErrorCode = typeof error.code === 'number' ? RETRYABLE_READ_ERROR_CODES.has(error.code) : false;
    if (hasRetryableErrorCode) {
        return true;
    }
    if (error instanceof MongoNetworkError) {
        return true;
    }
    const isNotWritablePrimaryError = exports.LEGACY_NOT_WRITABLE_PRIMARY_ERROR_MESSAGE.test(error.message);
    if (isNotWritablePrimaryError) {
        return true;
    }
    const isNodeIsRecoveringError = exports.NODE_IS_RECOVERING_ERROR_MESSAGE.test(error.message);
    if (isNodeIsRecoveringError) {
        return true;
    }
    return false;
}
const SDAM_RECOVERING_CODES = new Set([
    exports.MONGODB_ERROR_CODES.ShutdownInProgress,
    exports.MONGODB_ERROR_CODES.PrimarySteppedDown,
    exports.MONGODB_ERROR_CODES.InterruptedAtShutdown,
    exports.MONGODB_ERROR_CODES.InterruptedDueToReplStateChange,
    exports.MONGODB_ERROR_CODES.NotPrimaryOrSecondary
]);
const SDAM_NOT_PRIMARY_CODES = new Set([
    exports.MONGODB_ERROR_CODES.NotWritablePrimary,
    exports.MONGODB_ERROR_CODES.NotPrimaryNoSecondaryOk,
    exports.MONGODB_ERROR_CODES.LegacyNotPrimary
]);
const SDAM_NODE_SHUTTING_DOWN_ERROR_CODES = new Set([
    exports.MONGODB_ERROR_CODES.InterruptedAtShutdown,
    exports.MONGODB_ERROR_CODES.ShutdownInProgress
]);
function isRecoveringError(err) {
    if (typeof err.code === 'number') {
        // If any error code exists, we ignore the error.message
        return SDAM_RECOVERING_CODES.has(err.code);
    }
    return exports.LEGACY_NOT_PRIMARY_OR_SECONDARY_ERROR_MESSAGE.test(err.message) || exports.NODE_IS_RECOVERING_ERROR_MESSAGE.test(err.message);
}
function isNotWritablePrimaryError(err) {
    if (typeof err.code === 'number') {
        // If any error code exists, we ignore the error.message
        return SDAM_NOT_PRIMARY_CODES.has(err.code);
    }
    if (isRecoveringError(err)) {
        return false;
    }
    return exports.LEGACY_NOT_WRITABLE_PRIMARY_ERROR_MESSAGE.test(err.message);
}
function isNodeShuttingDownError(err) {
    return !!(typeof err.code === 'number' && SDAM_NODE_SHUTTING_DOWN_ERROR_CODES.has(err.code));
}
/**
 * Determines whether SDAM can recover from a given error. If it cannot
 * then the pool will be cleared, and server state will completely reset
 * locally.
 *
 * @see https://github.com/mongodb/specifications/blob/master/source/server-discovery-and-monitoring/server-discovery-and-monitoring.md#not-writable-primary-and-node-is-recovering
 */ function isSDAMUnrecoverableError(error) {
    // NOTE: null check is here for a strictly pre-CMAP world, a timeout or
    //       close event are considered unrecoverable
    if (error instanceof MongoParseError || error == null) {
        return true;
    }
    return isRecoveringError(error) || isNotWritablePrimaryError(error);
}
function isNetworkTimeoutError(err) {
    return !!(err instanceof MongoNetworkError && err.message.match(/timed out/));
}
function isResumableError(error, wireVersion) {
    if (error == null || !(error instanceof MongoError)) {
        return false;
    }
    if (error instanceof MongoNetworkError) {
        return true;
    }
    if (error instanceof MongoServerSelectionError) {
        return true;
    }
    if (wireVersion != null && wireVersion >= 9) {
        // DRIVERS-1308: For 4.4 drivers running against 4.4 servers, drivers will add a special case to treat the CursorNotFound error code as resumable
        if (error.code === exports.MONGODB_ERROR_CODES.CursorNotFound) {
            return true;
        }
        return error.hasErrorLabel(exports.MongoErrorLabel.ResumableChangeStreamError);
    }
    if (typeof error.code === 'number') {
        return exports.GET_MORE_RESUMABLE_CODES.has(error.code);
    }
    return false;
} //# sourceMappingURL=error.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/constants.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.END = exports.CHANGE = exports.INIT = exports.MORE = exports.RESPONSE = exports.SERVER_HEARTBEAT_FAILED = exports.SERVER_HEARTBEAT_SUCCEEDED = exports.SERVER_HEARTBEAT_STARTED = exports.COMMAND_FAILED = exports.COMMAND_SUCCEEDED = exports.COMMAND_STARTED = exports.CLUSTER_TIME_RECEIVED = exports.CONNECTION_CHECKED_IN = exports.CONNECTION_CHECKED_OUT = exports.CONNECTION_CHECK_OUT_FAILED = exports.CONNECTION_CHECK_OUT_STARTED = exports.CONNECTION_CLOSED = exports.CONNECTION_READY = exports.CONNECTION_CREATED = exports.CONNECTION_POOL_READY = exports.CONNECTION_POOL_CLEARED = exports.CONNECTION_POOL_CLOSED = exports.CONNECTION_POOL_CREATED = exports.WAITING_FOR_SUITABLE_SERVER = exports.SERVER_SELECTION_SUCCEEDED = exports.SERVER_SELECTION_FAILED = exports.SERVER_SELECTION_STARTED = exports.TOPOLOGY_DESCRIPTION_CHANGED = exports.TOPOLOGY_CLOSED = exports.TOPOLOGY_OPENING = exports.SERVER_DESCRIPTION_CHANGED = exports.SERVER_CLOSED = exports.SERVER_OPENING = exports.DESCRIPTION_RECEIVED = exports.UNPINNED = exports.PINNED = exports.MESSAGE = exports.ENDED = exports.CLOSED = exports.CONNECT = exports.OPEN = exports.CLOSE = exports.TIMEOUT = exports.ERROR = exports.SYSTEM_JS_COLLECTION = exports.SYSTEM_COMMAND_COLLECTION = exports.SYSTEM_USER_COLLECTION = exports.SYSTEM_PROFILE_COLLECTION = exports.SYSTEM_INDEX_COLLECTION = exports.SYSTEM_NAMESPACE_COLLECTION = void 0;
exports.kDecoratedKeys = exports.kDecorateResult = exports.LEGACY_HELLO_COMMAND_CAMEL_CASE = exports.LEGACY_HELLO_COMMAND = exports.MONGO_CLIENT_EVENTS = exports.LOCAL_SERVER_EVENTS = exports.SERVER_RELAY_EVENTS = exports.APM_EVENTS = exports.TOPOLOGY_EVENTS = exports.CMAP_EVENTS = exports.HEARTBEAT_EVENTS = exports.RESUME_TOKEN_CHANGED = void 0;
exports.SYSTEM_NAMESPACE_COLLECTION = 'system.namespaces';
exports.SYSTEM_INDEX_COLLECTION = 'system.indexes';
exports.SYSTEM_PROFILE_COLLECTION = 'system.profile';
exports.SYSTEM_USER_COLLECTION = 'system.users';
exports.SYSTEM_COMMAND_COLLECTION = '$cmd';
exports.SYSTEM_JS_COLLECTION = 'system.js';
// events
exports.ERROR = 'error';
exports.TIMEOUT = 'timeout';
exports.CLOSE = 'close';
exports.OPEN = 'open';
exports.CONNECT = 'connect';
exports.CLOSED = 'closed';
exports.ENDED = 'ended';
exports.MESSAGE = 'message';
exports.PINNED = 'pinned';
exports.UNPINNED = 'unpinned';
exports.DESCRIPTION_RECEIVED = 'descriptionReceived';
/** @internal */ exports.SERVER_OPENING = 'serverOpening';
/** @internal */ exports.SERVER_CLOSED = 'serverClosed';
/** @internal */ exports.SERVER_DESCRIPTION_CHANGED = 'serverDescriptionChanged';
/** @internal */ exports.TOPOLOGY_OPENING = 'topologyOpening';
/** @internal */ exports.TOPOLOGY_CLOSED = 'topologyClosed';
/** @internal */ exports.TOPOLOGY_DESCRIPTION_CHANGED = 'topologyDescriptionChanged';
/** @internal */ exports.SERVER_SELECTION_STARTED = 'serverSelectionStarted';
/** @internal */ exports.SERVER_SELECTION_FAILED = 'serverSelectionFailed';
/** @internal */ exports.SERVER_SELECTION_SUCCEEDED = 'serverSelectionSucceeded';
/** @internal */ exports.WAITING_FOR_SUITABLE_SERVER = 'waitingForSuitableServer';
/** @internal */ exports.CONNECTION_POOL_CREATED = 'connectionPoolCreated';
/** @internal */ exports.CONNECTION_POOL_CLOSED = 'connectionPoolClosed';
/** @internal */ exports.CONNECTION_POOL_CLEARED = 'connectionPoolCleared';
/** @internal */ exports.CONNECTION_POOL_READY = 'connectionPoolReady';
/** @internal */ exports.CONNECTION_CREATED = 'connectionCreated';
/** @internal */ exports.CONNECTION_READY = 'connectionReady';
/** @internal */ exports.CONNECTION_CLOSED = 'connectionClosed';
/** @internal */ exports.CONNECTION_CHECK_OUT_STARTED = 'connectionCheckOutStarted';
/** @internal */ exports.CONNECTION_CHECK_OUT_FAILED = 'connectionCheckOutFailed';
/** @internal */ exports.CONNECTION_CHECKED_OUT = 'connectionCheckedOut';
/** @internal */ exports.CONNECTION_CHECKED_IN = 'connectionCheckedIn';
exports.CLUSTER_TIME_RECEIVED = 'clusterTimeReceived';
/** @internal */ exports.COMMAND_STARTED = 'commandStarted';
/** @internal */ exports.COMMAND_SUCCEEDED = 'commandSucceeded';
/** @internal */ exports.COMMAND_FAILED = 'commandFailed';
/** @internal */ exports.SERVER_HEARTBEAT_STARTED = 'serverHeartbeatStarted';
/** @internal */ exports.SERVER_HEARTBEAT_SUCCEEDED = 'serverHeartbeatSucceeded';
/** @internal */ exports.SERVER_HEARTBEAT_FAILED = 'serverHeartbeatFailed';
exports.RESPONSE = 'response';
exports.MORE = 'more';
exports.INIT = 'init';
exports.CHANGE = 'change';
exports.END = 'end';
exports.RESUME_TOKEN_CHANGED = 'resumeTokenChanged';
/** @public */ exports.HEARTBEAT_EVENTS = Object.freeze([
    exports.SERVER_HEARTBEAT_STARTED,
    exports.SERVER_HEARTBEAT_SUCCEEDED,
    exports.SERVER_HEARTBEAT_FAILED
]);
/** @public */ exports.CMAP_EVENTS = Object.freeze([
    exports.CONNECTION_POOL_CREATED,
    exports.CONNECTION_POOL_READY,
    exports.CONNECTION_POOL_CLEARED,
    exports.CONNECTION_POOL_CLOSED,
    exports.CONNECTION_CREATED,
    exports.CONNECTION_READY,
    exports.CONNECTION_CLOSED,
    exports.CONNECTION_CHECK_OUT_STARTED,
    exports.CONNECTION_CHECK_OUT_FAILED,
    exports.CONNECTION_CHECKED_OUT,
    exports.CONNECTION_CHECKED_IN
]);
/** @public */ exports.TOPOLOGY_EVENTS = Object.freeze([
    exports.SERVER_OPENING,
    exports.SERVER_CLOSED,
    exports.SERVER_DESCRIPTION_CHANGED,
    exports.TOPOLOGY_OPENING,
    exports.TOPOLOGY_CLOSED,
    exports.TOPOLOGY_DESCRIPTION_CHANGED,
    exports.ERROR,
    exports.TIMEOUT,
    exports.CLOSE
]);
/** @public */ exports.APM_EVENTS = Object.freeze([
    exports.COMMAND_STARTED,
    exports.COMMAND_SUCCEEDED,
    exports.COMMAND_FAILED
]);
/**
 * All events that we relay to the `Topology`
 * @internal
 */ exports.SERVER_RELAY_EVENTS = Object.freeze([
    exports.SERVER_HEARTBEAT_STARTED,
    exports.SERVER_HEARTBEAT_SUCCEEDED,
    exports.SERVER_HEARTBEAT_FAILED,
    exports.COMMAND_STARTED,
    exports.COMMAND_SUCCEEDED,
    exports.COMMAND_FAILED,
    ...exports.CMAP_EVENTS
]);
/**
 * All events we listen to from `Server` instances, but do not forward to the client
 * @internal
 */ exports.LOCAL_SERVER_EVENTS = Object.freeze([
    exports.CONNECT,
    exports.DESCRIPTION_RECEIVED,
    exports.CLOSED,
    exports.ENDED
]);
/** @public */ exports.MONGO_CLIENT_EVENTS = Object.freeze([
    ...exports.CMAP_EVENTS,
    ...exports.APM_EVENTS,
    ...exports.TOPOLOGY_EVENTS,
    ...exports.HEARTBEAT_EVENTS
]);
/**
 * @internal
 * The legacy hello command that was deprecated in MongoDB 5.0.
 */ exports.LEGACY_HELLO_COMMAND = 'ismaster';
/**
 * @internal
 * The legacy hello command that was deprecated in MongoDB 5.0.
 */ exports.LEGACY_HELLO_COMMAND_CAMEL_CASE = 'isMaster';
// Typescript errors if we index objects with `Symbol.for(...)`, so
// to avoid TS errors we pull them out into variables.  Then we can type
// the objects (and class) that we expect to see them on and prevent TS
// errors.
/** @internal */ exports.kDecorateResult = Symbol.for('@@mdb.decorateDecryptionResult');
/** @internal */ exports.kDecoratedKeys = Symbol.for('@@mdb.decryptedKeys'); //# sourceMappingURL=constants.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/read_concern.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ReadConcern = exports.ReadConcernLevel = void 0;
/** @public */ exports.ReadConcernLevel = Object.freeze({
    local: 'local',
    majority: 'majority',
    linearizable: 'linearizable',
    available: 'available',
    snapshot: 'snapshot'
});
/**
 * The MongoDB ReadConcern, which allows for control of the consistency and isolation properties
 * of the data read from replica sets and replica set shards.
 * @public
 *
 * @see https://www.mongodb.com/docs/manual/reference/read-concern/index.html
 */ class ReadConcern {
    /** Constructs a ReadConcern from the read concern level.*/ constructor(level){
        /**
         * A spec test exists that allows level to be any string.
         * "invalid readConcern with out stage"
         * @see ./test/spec/crud/v2/aggregate-out-readConcern.json
         * @see https://github.com/mongodb/specifications/blob/master/source/read-write-concern/read-write-concern.md#unknown-levels-and-additional-options-for-string-based-readconcerns
         */ this.level = exports.ReadConcernLevel[level] ?? level;
    }
    /**
     * Construct a ReadConcern given an options object.
     *
     * @param options - The options object from which to extract the write concern.
     */ static fromOptions(options) {
        if (options == null) {
            return;
        }
        if (options.readConcern) {
            const { readConcern } = options;
            if (readConcern instanceof ReadConcern) {
                return readConcern;
            } else if (typeof readConcern === 'string') {
                return new ReadConcern(readConcern);
            } else if ('level' in readConcern && readConcern.level) {
                return new ReadConcern(readConcern.level);
            }
        }
        if (options.level) {
            return new ReadConcern(options.level);
        }
        return;
    }
    static get MAJORITY() {
        return exports.ReadConcernLevel.majority;
    }
    static get AVAILABLE() {
        return exports.ReadConcernLevel.available;
    }
    static get LINEARIZABLE() {
        return exports.ReadConcernLevel.linearizable;
    }
    static get SNAPSHOT() {
        return exports.ReadConcernLevel.snapshot;
    }
    toJSON() {
        return {
            level: this.level
        };
    }
}
exports.ReadConcern = ReadConcern; //# sourceMappingURL=read_concern.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/read_preference.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ReadPreference = exports.ReadPreferenceMode = void 0;
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
/** @public */ exports.ReadPreferenceMode = Object.freeze({
    primary: 'primary',
    primaryPreferred: 'primaryPreferred',
    secondary: 'secondary',
    secondaryPreferred: 'secondaryPreferred',
    nearest: 'nearest'
});
/**
 * The **ReadPreference** class is a class that represents a MongoDB ReadPreference and is
 * used to construct connections.
 * @public
 *
 * @see https://www.mongodb.com/docs/manual/core/read-preference/
 */ class ReadPreference {
    static{
        this.PRIMARY = exports.ReadPreferenceMode.primary;
    }
    static{
        this.PRIMARY_PREFERRED = exports.ReadPreferenceMode.primaryPreferred;
    }
    static{
        this.SECONDARY = exports.ReadPreferenceMode.secondary;
    }
    static{
        this.SECONDARY_PREFERRED = exports.ReadPreferenceMode.secondaryPreferred;
    }
    static{
        this.NEAREST = exports.ReadPreferenceMode.nearest;
    }
    static{
        this.primary = new ReadPreference(exports.ReadPreferenceMode.primary);
    }
    static{
        this.primaryPreferred = new ReadPreference(exports.ReadPreferenceMode.primaryPreferred);
    }
    static{
        this.secondary = new ReadPreference(exports.ReadPreferenceMode.secondary);
    }
    static{
        this.secondaryPreferred = new ReadPreference(exports.ReadPreferenceMode.secondaryPreferred);
    }
    static{
        this.nearest = new ReadPreference(exports.ReadPreferenceMode.nearest);
    }
    /**
     * @param mode - A string describing the read preference mode (primary|primaryPreferred|secondary|secondaryPreferred|nearest)
     * @param tags - A tag set used to target reads to members with the specified tag(s). tagSet is not available if using read preference mode primary.
     * @param options - Additional read preference options
     */ constructor(mode, tags, options){
        if (!ReadPreference.isValid(mode)) {
            throw new error_1.MongoInvalidArgumentError(`Invalid read preference mode ${JSON.stringify(mode)}`);
        }
        if (options == null && typeof tags === 'object' && !Array.isArray(tags)) {
            options = tags;
            tags = undefined;
        } else if (tags && !Array.isArray(tags)) {
            throw new error_1.MongoInvalidArgumentError('ReadPreference tags must be an array');
        }
        this.mode = mode;
        this.tags = tags;
        this.hedge = options?.hedge;
        this.maxStalenessSeconds = undefined;
        options = options ?? {};
        if (options.maxStalenessSeconds != null) {
            if (options.maxStalenessSeconds <= 0) {
                throw new error_1.MongoInvalidArgumentError('maxStalenessSeconds must be a positive integer');
            }
            this.maxStalenessSeconds = options.maxStalenessSeconds;
        }
        if (this.mode === ReadPreference.PRIMARY) {
            if (this.tags && Array.isArray(this.tags) && this.tags.length > 0) {
                throw new error_1.MongoInvalidArgumentError('Primary read preference cannot be combined with tags');
            }
            if (this.maxStalenessSeconds) {
                throw new error_1.MongoInvalidArgumentError('Primary read preference cannot be combined with maxStalenessSeconds');
            }
            if (this.hedge) {
                throw new error_1.MongoInvalidArgumentError('Primary read preference cannot be combined with hedge');
            }
        }
    }
    // Support the deprecated `preference` property introduced in the porcelain layer
    get preference() {
        return this.mode;
    }
    static fromString(mode) {
        return new ReadPreference(mode);
    }
    /**
     * Construct a ReadPreference given an options object.
     *
     * @param options - The options object from which to extract the read preference.
     */ static fromOptions(options) {
        if (!options) return;
        const readPreference = options.readPreference ?? options.session?.transaction.options.readPreference;
        const readPreferenceTags = options.readPreferenceTags;
        if (readPreference == null) {
            return;
        }
        if (typeof readPreference === 'string') {
            return new ReadPreference(readPreference, readPreferenceTags, {
                maxStalenessSeconds: options.maxStalenessSeconds,
                hedge: options.hedge
            });
        } else if (!(readPreference instanceof ReadPreference) && typeof readPreference === 'object') {
            const mode = readPreference.mode || readPreference.preference;
            if (mode && typeof mode === 'string') {
                return new ReadPreference(mode, readPreference.tags ?? readPreferenceTags, {
                    maxStalenessSeconds: readPreference.maxStalenessSeconds,
                    hedge: options.hedge
                });
            }
        }
        if (readPreferenceTags) {
            readPreference.tags = readPreferenceTags;
        }
        return readPreference;
    }
    /**
     * Replaces options.readPreference with a ReadPreference instance
     */ static translate(options) {
        if (options.readPreference == null) return options;
        const r = options.readPreference;
        if (typeof r === 'string') {
            options.readPreference = new ReadPreference(r);
        } else if (r && !(r instanceof ReadPreference) && typeof r === 'object') {
            const mode = r.mode || r.preference;
            if (mode && typeof mode === 'string') {
                options.readPreference = new ReadPreference(mode, r.tags, {
                    maxStalenessSeconds: r.maxStalenessSeconds
                });
            }
        } else if (!(r instanceof ReadPreference)) {
            throw new error_1.MongoInvalidArgumentError(`Invalid read preference: ${r}`);
        }
        return options;
    }
    /**
     * Validate if a mode is legal
     *
     * @param mode - The string representing the read preference mode.
     */ static isValid(mode) {
        const VALID_MODES = new Set([
            ReadPreference.PRIMARY,
            ReadPreference.PRIMARY_PREFERRED,
            ReadPreference.SECONDARY,
            ReadPreference.SECONDARY_PREFERRED,
            ReadPreference.NEAREST,
            null
        ]);
        return VALID_MODES.has(mode);
    }
    /**
     * Validate if a mode is legal
     *
     * @param mode - The string representing the read preference mode.
     */ isValid(mode) {
        return ReadPreference.isValid(typeof mode === 'string' ? mode : this.mode);
    }
    /**
     * Indicates that this readPreference needs the "SecondaryOk" bit when sent over the wire
     * @see https://www.mongodb.com/docs/manual/reference/mongodb-wire-protocol/#op-query
     */ secondaryOk() {
        const NEEDS_SECONDARYOK = new Set([
            ReadPreference.PRIMARY_PREFERRED,
            ReadPreference.SECONDARY,
            ReadPreference.SECONDARY_PREFERRED,
            ReadPreference.NEAREST
        ]);
        return NEEDS_SECONDARYOK.has(this.mode);
    }
    /**
     * Check if the two ReadPreferences are equivalent
     *
     * @param readPreference - The read preference with which to check equality
     */ equals(readPreference) {
        return readPreference.mode === this.mode;
    }
    /** Return JSON representation */ toJSON() {
        const readPreference = {
            mode: this.mode
        };
        if (Array.isArray(this.tags)) readPreference.tags = this.tags;
        if (this.maxStalenessSeconds) readPreference.maxStalenessSeconds = this.maxStalenessSeconds;
        if (this.hedge) readPreference.hedge = this.hedge;
        return readPreference;
    }
}
exports.ReadPreference = ReadPreference; //# sourceMappingURL=read_preference.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/write_concern.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.WriteConcern = exports.WRITE_CONCERN_KEYS = void 0;
exports.throwIfWriteConcernError = throwIfWriteConcernError;
const responses_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [app-client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
exports.WRITE_CONCERN_KEYS = [
    'w',
    'wtimeout',
    'j',
    'journal',
    'fsync'
];
/**
 * A MongoDB WriteConcern, which describes the level of acknowledgement
 * requested from MongoDB for write operations.
 * @public
 *
 * @see https://www.mongodb.com/docs/manual/reference/write-concern/
 */ class WriteConcern {
    /**
     * Constructs a WriteConcern from the write concern properties.
     * @param w - request acknowledgment that the write operation has propagated to a specified number of mongod instances or to mongod instances with specified tags.
     * @param wtimeoutMS - specify a time limit to prevent write operations from blocking indefinitely
     * @param journal - request acknowledgment that the write operation has been written to the on-disk journal
     * @param fsync - equivalent to the j option. Is deprecated and will be removed in the next major version.
     */ constructor(w, wtimeoutMS, journal, fsync){
        if (w != null) {
            if (!Number.isNaN(Number(w))) {
                this.w = Number(w);
            } else {
                this.w = w;
            }
        }
        if (wtimeoutMS != null) {
            this.wtimeoutMS = this.wtimeout = wtimeoutMS;
        }
        if (journal != null) {
            this.journal = this.j = journal;
        }
        if (fsync != null) {
            this.journal = this.j = fsync ? true : false;
        }
    }
    /**
     * Apply a write concern to a command document. Will modify and return the command.
     */ static apply(command, writeConcern) {
        const wc = {};
        // The write concern document sent to the server has w/wtimeout/j fields.
        if (writeConcern.w != null) wc.w = writeConcern.w;
        if (writeConcern.wtimeoutMS != null) wc.wtimeout = writeConcern.wtimeoutMS;
        if (writeConcern.journal != null) wc.j = writeConcern.j;
        command.writeConcern = wc;
        return command;
    }
    /** Construct a WriteConcern given an options object. */ static fromOptions(options, inherit) {
        if (options == null) return undefined;
        inherit = inherit ?? {};
        let opts;
        if (typeof options === 'string' || typeof options === 'number') {
            opts = {
                w: options
            };
        } else if (options instanceof WriteConcern) {
            opts = options;
        } else {
            opts = options.writeConcern;
        }
        const parentOpts = inherit instanceof WriteConcern ? inherit : inherit.writeConcern;
        const mergedOpts = {
            ...parentOpts,
            ...opts
        };
        const { w = undefined, wtimeout = undefined, j = undefined, fsync = undefined, journal = undefined, wtimeoutMS = undefined } = mergedOpts;
        if (w != null || wtimeout != null || wtimeoutMS != null || j != null || journal != null || fsync != null) {
            return new WriteConcern(w, wtimeout ?? wtimeoutMS, j ?? journal, fsync);
        }
        return undefined;
    }
}
exports.WriteConcern = WriteConcern;
/** Called with either a plain object or MongoDBResponse */ function throwIfWriteConcernError(response) {
    if (typeof response === 'object' && response != null) {
        const writeConcernError = responses_1.MongoDBResponse.is(response) && response.has('writeConcernError') ? response.toObject() : !responses_1.MongoDBResponse.is(response) && 'writeConcernError' in response ? response : null;
        if (writeConcernError != null) {
            throw new error_1.MongoWriteConcernError(writeConcernError);
        }
    }
} //# sourceMappingURL=write_concern.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/utils.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/Pathology/patho-client/node_modules/.pnpm/next@16.0.6_@babel+core@7.28.5_react-dom@19.2.0_react@19.2.0__react@19.2.0/node_modules/next/dist/compiled/buffer/index.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/Pathology/patho-client/node_modules/.pnpm/next@16.0.6_@babel+core@7.28.5_react-dom@19.2.0_react@19.2.0__react@19.2.0/node_modules/next/dist/build/polyfills/process.js [app-client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.kDispose = exports.randomBytes = exports.COSMOS_DB_MSG = exports.DOCUMENT_DB_MSG = exports.COSMOS_DB_CHECK = exports.DOCUMENT_DB_CHECK = exports.MONGODB_WARNING_CODE = exports.DEFAULT_PK_FACTORY = exports.HostAddress = exports.BufferPool = exports.List = exports.MongoDBCollectionNamespace = exports.MongoDBNamespace = exports.ByteUtils = void 0;
exports.isUint8Array = isUint8Array;
exports.hostMatchesWildcards = hostMatchesWildcards;
exports.normalizeHintField = normalizeHintField;
exports.isObject = isObject;
exports.mergeOptions = mergeOptions;
exports.filterOptions = filterOptions;
exports.isPromiseLike = isPromiseLike;
exports.decorateWithCollation = decorateWithCollation;
exports.decorateWithReadConcern = decorateWithReadConcern;
exports.getTopology = getTopology;
exports.ns = ns;
exports.makeCounter = makeCounter;
exports.uuidV4 = uuidV4;
exports.maxWireVersion = maxWireVersion;
exports.arrayStrictEqual = arrayStrictEqual;
exports.errorStrictEqual = errorStrictEqual;
exports.makeStateMachine = makeStateMachine;
exports.now = now;
exports.calculateDurationInMs = calculateDurationInMs;
exports.hasAtomicOperators = hasAtomicOperators;
exports.resolveTimeoutOptions = resolveTimeoutOptions;
exports.resolveOptions = resolveOptions;
exports.isSuperset = isSuperset;
exports.isHello = isHello;
exports.setDifference = setDifference;
exports.isRecord = isRecord;
exports.emitWarning = emitWarning;
exports.emitWarningOnce = emitWarningOnce;
exports.enumToString = enumToString;
exports.supportsRetryableWrites = supportsRetryableWrites;
exports.shuffle = shuffle;
exports.commandSupportsReadConcern = commandSupportsReadConcern;
exports.compareObjectId = compareObjectId;
exports.parseInteger = parseInteger;
exports.parseUnsignedInteger = parseUnsignedInteger;
exports.checkParentDomainMatch = checkParentDomainMatch;
exports.get = get;
exports.request = request;
exports.isHostMatch = isHostMatch;
exports.promiseWithResolvers = promiseWithResolvers;
exports.squashError = squashError;
exports.once = once;
exports.maybeAddIdToDocuments = maybeAddIdToDocuments;
exports.fileIsAccessible = fileIsAccessible;
exports.csotMin = csotMin;
exports.noop = noop;
exports.decorateDecryptionResult = decorateDecryptionResult;
exports.addAbortListener = addAbortListener;
exports.abortable = abortable;
const crypto = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/next@16.0.6_@babel+core@7.28.5_react-dom@19.2.0_react@19.2.0__react@19.2.0/node_modules/next/dist/compiled/crypto-browserify/index.js [app-client] (ecmascript)");
const fs_1 = (()=>{
    const e = new Error("Cannot find module 'fs'");
    e.code = 'MODULE_NOT_FOUND';
    throw e;
})();
const http = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/next@16.0.6_@babel+core@7.28.5_react-dom@19.2.0_react@19.2.0__react@19.2.0/node_modules/next/dist/compiled/stream-http/index.js [app-client] (ecmascript)");
const timers_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/next@16.0.6_@babel+core@7.28.5_react-dom@19.2.0_react@19.2.0__react@19.2.0/node_modules/next/dist/compiled/timers-browserify/main.js [app-client] (ecmascript)");
const url = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/next@16.0.6_@babel+core@7.28.5_react-dom@19.2.0_react@19.2.0__react@19.2.0/node_modules/next/dist/compiled/native-url/index.js [app-client] (ecmascript)");
const url_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/next@16.0.6_@babel+core@7.28.5_react-dom@19.2.0_react@19.2.0__react@19.2.0/node_modules/next/dist/compiled/native-url/index.js [app-client] (ecmascript)");
const util_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/next@16.0.6_@babel+core@7.28.5_react-dom@19.2.0_react@19.2.0__react@19.2.0/node_modules/next/dist/compiled/util/util.js [app-client] (ecmascript)");
const bson_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bson.js [app-client] (ecmascript)");
const constants_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cmap/wire_protocol/constants.js [app-client] (ecmascript)");
const constants_2 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/constants.js [app-client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
const read_concern_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/read_concern.js [app-client] (ecmascript)");
const read_preference_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/read_preference.js [app-client] (ecmascript)");
const common_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/sdam/common.js [app-client] (ecmascript)");
const write_concern_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/write_concern.js [app-client] (ecmascript)");
exports.ByteUtils = {
    toLocalBufferType (buffer) {
        return __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["Buffer"].isBuffer(buffer) ? buffer : __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["Buffer"].from(buffer.buffer, buffer.byteOffset, buffer.byteLength);
    },
    equals (seqA, seqB) {
        return exports.ByteUtils.toLocalBufferType(seqA).equals(seqB);
    },
    compare (seqA, seqB) {
        return exports.ByteUtils.toLocalBufferType(seqA).compare(seqB);
    },
    toBase64 (uint8array) {
        return exports.ByteUtils.toLocalBufferType(uint8array).toString('base64');
    }
};
/**
 * Returns true if value is a Uint8Array or a Buffer
 * @param value - any value that may be a Uint8Array
 */ function isUint8Array(value) {
    return value != null && typeof value === 'object' && Symbol.toStringTag in value && value[Symbol.toStringTag] === 'Uint8Array';
}
/**
 * Determines if a connection's address matches a user provided list
 * of domain wildcards.
 */ function hostMatchesWildcards(host, wildcards) {
    for (const wildcard of wildcards){
        if (host === wildcard || wildcard.startsWith('*.') && host?.endsWith(wildcard.substring(2, wildcard.length)) || wildcard.startsWith('*/') && host?.endsWith(wildcard.substring(2, wildcard.length))) {
            return true;
        }
    }
    return false;
}
/**
 * Ensure Hint field is in a shape we expect:
 * - object of index names mapping to 1 or -1
 * - just an index name
 * @internal
 */ function normalizeHintField(hint) {
    let finalHint = undefined;
    if (typeof hint === 'string') {
        finalHint = hint;
    } else if (Array.isArray(hint)) {
        finalHint = {};
        hint.forEach((param)=>{
            finalHint[param] = 1;
        });
    } else if (hint != null && typeof hint === 'object') {
        finalHint = {};
        for(const name in hint){
            finalHint[name] = hint[name];
        }
    }
    return finalHint;
}
const TO_STRING = (object)=>Object.prototype.toString.call(object);
/**
 * Checks if arg is an Object:
 * - **NOTE**: the check is based on the `[Symbol.toStringTag]() === 'Object'`
 * @internal
 */ function isObject(arg) {
    return '[object Object]' === TO_STRING(arg);
}
/** @internal */ function mergeOptions(target, source) {
    return {
        ...target,
        ...source
    };
}
/** @internal */ function filterOptions(options, names) {
    const filterOptions = {};
    for(const name in options){
        if (names.includes(name)) {
            filterOptions[name] = options[name];
        }
    }
    // Filtered options
    return filterOptions;
}
/**
 * Applies a write concern to a command based on well defined inheritance rules, optionally
 * detecting support for the write concern in the first place.
 * @internal
 *
 * @param target - the target command we will be applying the write concern to
 * @param sources - sources where we can inherit default write concerns from
 * @param options - optional settings passed into a command for write concern overrides
 */ /**
 * Checks if a given value is a Promise
 *
 * @typeParam T - The resolution type of the possible promise
 * @param value - An object that could be a promise
 * @returns true if the provided value is a Promise
 */ function isPromiseLike(value) {
    return value != null && typeof value === 'object' && 'then' in value && typeof value.then === 'function';
}
/**
 * Applies collation to a given command.
 * @internal
 *
 * @param command - the command on which to apply collation
 * @param target - target of command
 * @param options - options containing collation settings
 */ function decorateWithCollation(command, options) {
    if (options.collation && typeof options.collation === 'object') {
        command.collation = options.collation;
    }
}
/**
 * Applies a read concern to a given command.
 * @internal
 *
 * @param command - the command on which to apply the read concern
 * @param coll - the parent collection of the operation calling this method
 */ function decorateWithReadConcern(command, coll, options) {
    if (options && options.session && options.session.inTransaction()) {
        return;
    }
    const readConcern = Object.assign({}, command.readConcern || {});
    if (coll.s.readConcern) {
        Object.assign(readConcern, coll.s.readConcern);
    }
    if (Object.keys(readConcern).length > 0) {
        Object.assign(command, {
            readConcern: readConcern
        });
    }
}
/**
 * A helper function to get the topology from a given provider. Throws
 * if the topology cannot be found.
 * @throws MongoNotConnectedError
 * @internal
 */ function getTopology(provider) {
    // MongoClient or ClientSession or AbstractCursor
    if ('topology' in provider && provider.topology) {
        return provider.topology;
    } else if ('client' in provider && provider.client.topology) {
        return provider.client.topology;
    }
    throw new error_1.MongoNotConnectedError('MongoClient must be connected to perform this operation');
}
/** @internal */ function ns(ns) {
    return MongoDBNamespace.fromString(ns);
}
/** @public */ class MongoDBNamespace {
    /**
     * Create a namespace object
     *
     * @param db - database name
     * @param collection - collection name
     */ constructor(db, collection){
        this.db = db;
        this.collection = collection === '' ? undefined : collection;
    }
    toString() {
        return this.collection ? `${this.db}.${this.collection}` : this.db;
    }
    withCollection(collection) {
        return new MongoDBCollectionNamespace(this.db, collection);
    }
    static fromString(namespace) {
        if (typeof namespace !== 'string' || namespace === '') {
            // TODO(NODE-3483): Replace with MongoNamespaceError
            throw new error_1.MongoRuntimeError(`Cannot parse namespace from "${namespace}"`);
        }
        const [db, ...collectionParts] = namespace.split('.');
        const collection = collectionParts.join('.');
        return new MongoDBNamespace(db, collection === '' ? undefined : collection);
    }
}
exports.MongoDBNamespace = MongoDBNamespace;
/**
 * @public
 *
 * A class representing a collection's namespace.  This class enforces (through Typescript) that
 * the `collection` portion of the namespace is defined and should only be
 * used in scenarios where this can be guaranteed.
 */ class MongoDBCollectionNamespace extends MongoDBNamespace {
    constructor(db, collection){
        super(db, collection);
        this.collection = collection;
    }
    static fromString(namespace) {
        return super.fromString(namespace);
    }
}
exports.MongoDBCollectionNamespace = MongoDBCollectionNamespace;
/** @internal */ function* makeCounter(seed = 0) {
    let count = seed;
    while(true){
        const newCount = count;
        count += 1;
        yield newCount;
    }
}
/**
 * Synchronously Generate a UUIDv4
 * @internal
 */ function uuidV4() {
    const result = crypto.randomBytes(16);
    result[6] = result[6] & 0x0f | 0x40;
    result[8] = result[8] & 0x3f | 0x80;
    return result;
}
/**
 * A helper function for determining `maxWireVersion` between legacy and new topology instances
 * @internal
 */ function maxWireVersion(handshakeAware) {
    if (handshakeAware) {
        if (handshakeAware.hello) {
            return handshakeAware.hello.maxWireVersion;
        }
        if (handshakeAware.serverApi?.version) {
            // We return the max supported wire version for serverAPI.
            return constants_1.MAX_SUPPORTED_WIRE_VERSION;
        }
        // This is the fallback case for load balanced mode. If we are building commands the
        // object being checked will be a connection, and we will have a hello response on
        // it. For other cases, such as retryable writes, the object will be a server or
        // topology, and there will be no hello response on those objects, so we return
        // the max wire version so we support retryability. Once we have a min supported
        // wire version of 9, then the needsRetryableWriteLabel() check can remove the
        // usage of passing the wire version into it.
        if (handshakeAware.loadBalanced) {
            return constants_1.MAX_SUPPORTED_WIRE_VERSION;
        }
        if ('lastHello' in handshakeAware && typeof handshakeAware.lastHello === 'function') {
            const lastHello = handshakeAware.lastHello();
            if (lastHello) {
                return lastHello.maxWireVersion;
            }
        }
        if (handshakeAware.description && 'maxWireVersion' in handshakeAware.description && handshakeAware.description.maxWireVersion != null) {
            return handshakeAware.description.maxWireVersion;
        }
    }
    return 0;
}
/** @internal */ function arrayStrictEqual(arr, arr2) {
    if (!Array.isArray(arr) || !Array.isArray(arr2)) {
        return false;
    }
    return arr.length === arr2.length && arr.every((elt, idx)=>elt === arr2[idx]);
}
/** @internal */ function errorStrictEqual(lhs, rhs) {
    if (lhs === rhs) {
        return true;
    }
    if (!lhs || !rhs) {
        return lhs === rhs;
    }
    if (lhs == null && rhs != null || lhs != null && rhs == null) {
        return false;
    }
    if (lhs.constructor.name !== rhs.constructor.name) {
        return false;
    }
    if (lhs.message !== rhs.message) {
        return false;
    }
    return true;
}
/** @internal */ function makeStateMachine(stateTable) {
    return function stateTransition(target, newState) {
        const legalStates = stateTable[target.s.state];
        if (legalStates && legalStates.indexOf(newState) < 0) {
            throw new error_1.MongoRuntimeError(`illegal state transition from [${target.s.state}] => [${newState}], allowed: [${legalStates}]`);
        }
        target.emit('stateChanged', target.s.state, newState);
        target.s.state = newState;
    };
}
/** @internal */ function now() {
    const hrtime = __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].hrtime();
    return Math.floor(hrtime[0] * 1000 + hrtime[1] / 1000000);
}
/** @internal */ function calculateDurationInMs(started) {
    if (typeof started !== 'number') {
        return -1;
    }
    const elapsed = now() - started;
    return elapsed < 0 ? 0 : elapsed;
}
/** @internal */ function hasAtomicOperators(doc, options) {
    if (Array.isArray(doc)) {
        for (const document of doc){
            if (hasAtomicOperators(document)) {
                return true;
            }
        }
        return false;
    }
    const keys = Object.keys(doc);
    // In this case we need to throw if all the atomic operators are undefined.
    if (options?.ignoreUndefined) {
        let allUndefined = true;
        for (const key of keys){
            // eslint-disable-next-line no-restricted-syntax
            if (doc[key] !== undefined) {
                allUndefined = false;
                break;
            }
        }
        if (allUndefined) {
            throw new error_1.MongoInvalidArgumentError('Update operations require that all atomic operators have defined values, but none were provided.');
        }
    }
    return keys.length > 0 && keys[0][0] === '$';
}
function resolveTimeoutOptions(client, options) {
    const { socketTimeoutMS, serverSelectionTimeoutMS, waitQueueTimeoutMS, timeoutMS } = client.s.options;
    return {
        socketTimeoutMS,
        serverSelectionTimeoutMS,
        waitQueueTimeoutMS,
        timeoutMS,
        ...options
    };
}
/**
 * Merge inherited properties from parent into options, prioritizing values from options,
 * then values from parent.
 *
 * @param parent - An optional owning class of the operation being run. ex. Db/Collection/MongoClient.
 * @param options - The options passed to the operation method.
 *
 * @internal
 */ function resolveOptions(parent, options) {
    const result = Object.assign({}, options, (0, bson_1.resolveBSONOptions)(options, parent));
    const timeoutMS = options?.timeoutMS ?? parent?.timeoutMS;
    // Users cannot pass a readConcern/writeConcern to operations in a transaction
    const session = options?.session;
    if (!session?.inTransaction()) {
        const readConcern = read_concern_1.ReadConcern.fromOptions(options) ?? parent?.readConcern;
        if (readConcern) {
            result.readConcern = readConcern;
        }
        let writeConcern = write_concern_1.WriteConcern.fromOptions(options) ?? parent?.writeConcern;
        if (writeConcern) {
            if (timeoutMS != null) {
                writeConcern = write_concern_1.WriteConcern.fromOptions({
                    writeConcern: {
                        ...writeConcern,
                        wtimeout: undefined,
                        wtimeoutMS: undefined
                    }
                });
            }
            result.writeConcern = writeConcern;
        }
    }
    result.timeoutMS = timeoutMS;
    const readPreference = read_preference_1.ReadPreference.fromOptions(options) ?? parent?.readPreference;
    if (readPreference) {
        result.readPreference = readPreference;
    }
    const isConvenientTransaction = session?.explicit && session?.timeoutContext != null;
    if (isConvenientTransaction && options?.timeoutMS != null) {
        throw new error_1.MongoInvalidArgumentError('An operation cannot be given a timeoutMS setting when inside a withTransaction call that has a timeoutMS setting');
    }
    return result;
}
function isSuperset(set, subset) {
    set = Array.isArray(set) ? new Set(set) : set;
    subset = Array.isArray(subset) ? new Set(subset) : subset;
    for (const elem of subset){
        if (!set.has(elem)) {
            return false;
        }
    }
    return true;
}
/**
 * Checks if the document is a Hello request
 * @internal
 */ function isHello(doc) {
    return doc[constants_2.LEGACY_HELLO_COMMAND] || doc.hello ? true : false;
}
/** Returns the items that are uniquely in setA */ function setDifference(setA, setB) {
    const difference = new Set(setA);
    for (const elem of setB){
        difference.delete(elem);
    }
    return difference;
}
const HAS_OWN = (object, prop)=>Object.prototype.hasOwnProperty.call(object, prop);
function isRecord(value, requiredKeys = undefined) {
    if (!isObject(value)) {
        return false;
    }
    const ctor = value.constructor;
    if (ctor && ctor.prototype) {
        if (!isObject(ctor.prototype)) {
            return false;
        }
        // Check to see if some method exists from the Object exists
        if (!HAS_OWN(ctor.prototype, 'isPrototypeOf')) {
            return false;
        }
    }
    if (requiredKeys) {
        const keys = Object.keys(value);
        return isSuperset(keys, requiredKeys);
    }
    return true;
}
/**
 * A sequential list of items in a circularly linked list
 * @remarks
 * The head node is special, it is always defined and has a value of null.
 * It is never "included" in the list, in that, it is not returned by pop/shift or yielded by the iterator.
 * The circular linkage and always defined head node are to reduce checks for null next/prev references to zero.
 * New nodes are declared as object literals with keys always in the same order: next, prev, value.
 * @internal
 */ class List {
    get length() {
        return this.count;
    }
    get [Symbol.toStringTag]() {
        return 'List';
    }
    constructor(){
        this.count = 0;
        // this is carefully crafted:
        // declaring a complete and consistently key ordered
        // object is beneficial to the runtime optimizations
        this.head = {
            next: null,
            prev: null,
            value: null
        };
        this.head.next = this.head;
        this.head.prev = this.head;
    }
    toArray() {
        return Array.from(this);
    }
    toString() {
        return `head <=> ${this.toArray().join(' <=> ')} <=> head`;
    }
    *[Symbol.iterator]() {
        for (const node of this.nodes()){
            yield node.value;
        }
    }
    *nodes() {
        let ptr = this.head.next;
        while(ptr !== this.head){
            // Save next before yielding so that we make removing within iteration safe
            const { next } = ptr;
            yield ptr;
            ptr = next;
        }
    }
    /** Insert at end of list */ push(value) {
        this.count += 1;
        const newNode = {
            next: this.head,
            prev: this.head.prev,
            value
        };
        this.head.prev.next = newNode;
        this.head.prev = newNode;
    }
    /** Inserts every item inside an iterable instead of the iterable itself */ pushMany(iterable) {
        for (const value of iterable){
            this.push(value);
        }
    }
    /** Insert at front of list */ unshift(value) {
        this.count += 1;
        const newNode = {
            next: this.head.next,
            prev: this.head,
            value
        };
        this.head.next.prev = newNode;
        this.head.next = newNode;
    }
    remove(node) {
        if (node === this.head || this.length === 0) {
            return null;
        }
        this.count -= 1;
        const prevNode = node.prev;
        const nextNode = node.next;
        prevNode.next = nextNode;
        nextNode.prev = prevNode;
        return node.value;
    }
    /** Removes the first node at the front of the list */ shift() {
        return this.remove(this.head.next);
    }
    /** Removes the last node at the end of the list */ pop() {
        return this.remove(this.head.prev);
    }
    /** Iterates through the list and removes nodes where filter returns true */ prune(filter) {
        for (const node of this.nodes()){
            if (filter(node.value)) {
                this.remove(node);
            }
        }
    }
    clear() {
        this.count = 0;
        this.head.next = this.head;
        this.head.prev = this.head;
    }
    /** Returns the first item in the list, does not remove */ first() {
        // If the list is empty, value will be the head's null
        return this.head.next.value;
    }
    /** Returns the last item in the list, does not remove */ last() {
        // If the list is empty, value will be the head's null
        return this.head.prev.value;
    }
}
exports.List = List;
/**
 * A pool of Buffers which allow you to read them as if they were one
 * @internal
 */ class BufferPool {
    constructor(){
        this.buffers = new List();
        this.totalByteLength = 0;
    }
    get length() {
        return this.totalByteLength;
    }
    /** Adds a buffer to the internal buffer pool list */ append(buffer) {
        this.buffers.push(buffer);
        this.totalByteLength += buffer.length;
    }
    /**
     * If BufferPool contains 4 bytes or more construct an int32 from the leading bytes,
     * otherwise return null. Size can be negative, caller should error check.
     */ getInt32() {
        if (this.totalByteLength < 4) {
            return null;
        }
        const firstBuffer = this.buffers.first();
        if (firstBuffer != null && firstBuffer.byteLength >= 4) {
            return firstBuffer.readInt32LE(0);
        }
        // Unlikely case: an int32 is split across buffers.
        // Use read and put the returned buffer back on top
        const top4Bytes = this.read(4);
        const value = top4Bytes.readInt32LE(0);
        // Put it back.
        this.totalByteLength += 4;
        this.buffers.unshift(top4Bytes);
        return value;
    }
    /** Reads the requested number of bytes, optionally consuming them */ read(size) {
        if (typeof size !== 'number' || size < 0) {
            throw new error_1.MongoInvalidArgumentError('Argument "size" must be a non-negative number');
        }
        // oversized request returns empty buffer
        if (size > this.totalByteLength) {
            return __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["Buffer"].alloc(0);
        }
        // We know we have enough, we just don't know how it is spread across chunks
        // TODO(NODE-4732): alloc API should change based on raw option
        const result = __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["Buffer"].allocUnsafe(size);
        for(let bytesRead = 0; bytesRead < size;){
            const buffer = this.buffers.shift();
            if (buffer == null) {
                break;
            }
            const bytesRemaining = size - bytesRead;
            const bytesReadable = Math.min(bytesRemaining, buffer.byteLength);
            const bytes = buffer.subarray(0, bytesReadable);
            result.set(bytes, bytesRead);
            bytesRead += bytesReadable;
            this.totalByteLength -= bytesReadable;
            if (bytesReadable < buffer.byteLength) {
                this.buffers.unshift(buffer.subarray(bytesReadable));
            }
        }
        return result;
    }
}
exports.BufferPool = BufferPool;
/** @public */ class HostAddress {
    constructor(hostString){
        this.host = undefined;
        this.port = undefined;
        this.socketPath = undefined;
        this.isIPv6 = false;
        const escapedHost = hostString.split(' ').join('%20'); // escape spaces, for socket path hosts
        if (escapedHost.endsWith('.sock')) {
            // heuristically determine if we're working with a domain socket
            this.socketPath = decodeURIComponent(escapedHost);
            return;
        }
        const urlString = `iLoveJS://${escapedHost}`;
        let url;
        try {
            url = new url_1.URL(urlString);
        } catch (urlError) {
            const runtimeError = new error_1.MongoRuntimeError(`Unable to parse ${escapedHost} with URL`);
            runtimeError.cause = urlError;
            throw runtimeError;
        }
        const hostname = url.hostname;
        const port = url.port;
        let normalized = decodeURIComponent(hostname).toLowerCase();
        if (normalized.startsWith('[') && normalized.endsWith(']')) {
            this.isIPv6 = true;
            normalized = normalized.substring(1, hostname.length - 1);
        }
        this.host = normalized.toLowerCase();
        if (typeof port === 'number') {
            this.port = port;
        } else if (typeof port === 'string' && port !== '') {
            this.port = Number.parseInt(port, 10);
        } else {
            this.port = 27017;
        }
        if (this.port === 0) {
            throw new error_1.MongoParseError('Invalid port (zero) with hostname');
        }
        Object.freeze(this);
    }
    [Symbol.for('nodejs.util.inspect.custom')]() {
        return this.inspect();
    }
    inspect() {
        return `new HostAddress('${this.toString()}')`;
    }
    toString() {
        if (typeof this.host === 'string') {
            if (this.isIPv6) {
                return `[${this.host}]:${this.port}`;
            }
            return `${this.host}:${this.port}`;
        }
        return `${this.socketPath}`;
    }
    static fromString(s) {
        return new HostAddress(s);
    }
    static fromHostPort(host, port) {
        if (host.includes(':')) {
            host = `[${host}]`; // IPv6 address
        }
        return HostAddress.fromString(`${host}:${port}`);
    }
    static fromSrvRecord({ name, port }) {
        return HostAddress.fromHostPort(name, port);
    }
    toHostPort() {
        if (this.socketPath) {
            return {
                host: this.socketPath,
                port: 0
            };
        }
        const host = this.host ?? '';
        const port = this.port ?? 0;
        return {
            host,
            port
        };
    }
}
exports.HostAddress = HostAddress;
exports.DEFAULT_PK_FACTORY = {
    // We prefer not to rely on ObjectId having a createPk method
    createPk () {
        return new bson_1.ObjectId();
    }
};
/**
 * When the driver used emitWarning the code will be equal to this.
 * @public
 *
 * @example
 * ```ts
 * process.on('warning', (warning) => {
 *  if (warning.code === MONGODB_WARNING_CODE) console.error('Ah an important warning! :)')
 * })
 * ```
 */ exports.MONGODB_WARNING_CODE = 'MONGODB DRIVER';
/** @internal */ function emitWarning(message) {
    return __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].emitWarning(message, {
        code: exports.MONGODB_WARNING_CODE
    });
}
const emittedWarnings = new Set();
/**
 * Will emit a warning once for the duration of the application.
 * Uses the message to identify if it has already been emitted
 * so using string interpolation can cause multiple emits
 * @internal
 */ function emitWarningOnce(message) {
    if (!emittedWarnings.has(message)) {
        emittedWarnings.add(message);
        return emitWarning(message);
    }
}
/**
 * Takes a JS object and joins the values into a string separated by ', '
 */ function enumToString(en) {
    return Object.values(en).join(', ');
}
/**
 * Determine if a server supports retryable writes.
 *
 * @internal
 */ function supportsRetryableWrites(server) {
    if (!server) {
        return false;
    }
    if (server.loadBalanced) {
        // Loadbalanced topologies will always support retry writes
        return true;
    }
    if (server.description.logicalSessionTimeoutMinutes != null) {
        // that supports sessions
        if (server.description.type !== common_1.ServerType.Standalone) {
            // and that is not a standalone
            return true;
        }
    }
    return false;
}
/**
 * FisherYates Shuffle
 *
 * Reference: https://bost.ocks.org/mike/shuffle/
 * @param sequence - items to be shuffled
 * @param limit - Defaults to `0`. If nonzero shuffle will slice the randomized array e.g, `.slice(0, limit)` otherwise will return the entire randomized array.
 */ function shuffle(sequence, limit = 0) {
    const items = Array.from(sequence); // shallow copy in order to never shuffle the input
    if (limit > items.length) {
        throw new error_1.MongoRuntimeError('Limit must be less than the number of items');
    }
    let remainingItemsToShuffle = items.length;
    const lowerBound = limit % items.length === 0 ? 1 : items.length - limit;
    while(remainingItemsToShuffle > lowerBound){
        // Pick a remaining element
        const randomIndex = Math.floor(Math.random() * remainingItemsToShuffle);
        remainingItemsToShuffle -= 1;
        // And swap it with the current element
        const swapHold = items[remainingItemsToShuffle];
        items[remainingItemsToShuffle] = items[randomIndex];
        items[randomIndex] = swapHold;
    }
    return limit % items.length === 0 ? items : items.slice(lowerBound);
}
/**
 * TODO(NODE-4936): read concern eligibility for commands should be codified in command construction
 * @internal
 * @see https://github.com/mongodb/specifications/blob/master/source/read-write-concern/read-write-concern.md#read-concern
 */ function commandSupportsReadConcern(command) {
    if (command.aggregate || command.count || command.distinct || command.find || command.geoNear) {
        return true;
    }
    return false;
}
/**
 * Compare objectIds. `null` is always less
 * - `+1 = oid1 is greater than oid2`
 * - `-1 = oid1 is less than oid2`
 * - `+0 = oid1 is equal oid2`
 */ function compareObjectId(oid1, oid2) {
    if (oid1 == null && oid2 == null) {
        return 0;
    }
    if (oid1 == null) {
        return -1;
    }
    if (oid2 == null) {
        return 1;
    }
    return exports.ByteUtils.compare(oid1.id, oid2.id);
}
function parseInteger(value) {
    if (typeof value === 'number') return Math.trunc(value);
    const parsedValue = Number.parseInt(String(value), 10);
    return Number.isNaN(parsedValue) ? null : parsedValue;
}
function parseUnsignedInteger(value) {
    const parsedInt = parseInteger(value);
    return parsedInt != null && parsedInt >= 0 ? parsedInt : null;
}
/**
 * This function throws a MongoAPIError in the event that either of the following is true:
 * * If the provided address domain does not match the provided parent domain
 * * If the parent domain contains less than three `.` separated parts and the provided address does not contain at least one more domain level than its parent
 *
 * If a DNS server were to become compromised SRV records would still need to
 * advertise addresses that are under the same domain as the srvHost.
 *
 * @param address - The address to check against a domain
 * @param srvHost - The domain to check the provided address against
 * @returns void
 */ function checkParentDomainMatch(address, srvHost) {
    // Remove trailing dot if exists on either the resolved address or the srv hostname
    const normalizedAddress = address.endsWith('.') ? address.slice(0, address.length - 1) : address;
    const normalizedSrvHost = srvHost.endsWith('.') ? srvHost.slice(0, srvHost.length - 1) : srvHost;
    const allCharacterBeforeFirstDot = /^.*?\./;
    const srvIsLessThanThreeParts = normalizedSrvHost.split('.').length < 3;
    // Remove all characters before first dot
    // Add leading dot back to string so
    //   an srvHostDomain = '.trusted.site'
    //   will not satisfy an addressDomain that endsWith '.fake-trusted.site'
    const addressDomain = `.${normalizedAddress.replace(allCharacterBeforeFirstDot, '')}`;
    let srvHostDomain = srvIsLessThanThreeParts ? normalizedSrvHost : `.${normalizedSrvHost.replace(allCharacterBeforeFirstDot, '')}`;
    if (!srvHostDomain.startsWith('.')) {
        srvHostDomain = '.' + srvHostDomain;
    }
    if (srvIsLessThanThreeParts && normalizedAddress.split('.').length <= normalizedSrvHost.split('.').length) {
        throw new error_1.MongoAPIError('Server record does not have at least one more domain level than parent URI');
    }
    if (!addressDomain.endsWith(srvHostDomain)) {
        throw new error_1.MongoAPIError('Server record does not share hostname with parent URI');
    }
}
/**
 * Perform a get request that returns status and body.
 * @internal
 */ function get(url, options = {}) {
    return new Promise((resolve, reject)=>{
        /* eslint-disable prefer-const */ let timeoutId;
        const request = http.get(url, options, (response)=>{
            response.setEncoding('utf8');
            let body = '';
            response.on('data', (chunk)=>body += chunk);
            response.on('end', ()=>{
                (0, timers_1.clearTimeout)(timeoutId);
                resolve({
                    status: response.statusCode,
                    body
                });
            });
        }).on('error', (error)=>{
            (0, timers_1.clearTimeout)(timeoutId);
            reject(error);
        }).end();
        timeoutId = (0, timers_1.setTimeout)(()=>{
            request.destroy(new error_1.MongoNetworkTimeoutError(`request timed out after 10 seconds`));
        }, 10000);
    });
}
async function request(uri, options = {}) {
    return await new Promise((resolve, reject)=>{
        const requestOptions = {
            method: 'GET',
            timeout: 10000,
            json: true,
            ...url.parse(uri),
            ...options
        };
        const req = http.request(requestOptions, (res)=>{
            res.setEncoding('utf8');
            let data = '';
            res.on('data', (d)=>{
                data += d;
            });
            res.once('end', ()=>{
                if (options.json === false) {
                    resolve(data);
                    return;
                }
                try {
                    const parsed = JSON.parse(data);
                    resolve(parsed);
                } catch  {
                    // TODO(NODE-3483)
                    reject(new error_1.MongoRuntimeError(`Invalid JSON response: "${data}"`));
                }
            });
        });
        req.once('timeout', ()=>req.destroy(new error_1.MongoNetworkTimeoutError(`Network request to ${uri} timed out after ${options.timeout} ms`)));
        req.once('error', (error)=>reject(error));
        req.end();
    });
}
/** @internal */ exports.DOCUMENT_DB_CHECK = /(\.docdb\.amazonaws\.com$)|(\.docdb-elastic\.amazonaws\.com$)/;
/** @internal */ exports.COSMOS_DB_CHECK = /\.cosmos\.azure\.com$/;
/** @internal */ exports.DOCUMENT_DB_MSG = 'You appear to be connected to a DocumentDB cluster. For more information regarding feature compatibility and support please visit https://www.mongodb.com/supportability/documentdb';
/** @internal */ exports.COSMOS_DB_MSG = 'You appear to be connected to a CosmosDB cluster. For more information regarding feature compatibility and support please visit https://www.mongodb.com/supportability/cosmosdb';
/** @internal */ function isHostMatch(match, host) {
    return host && match.test(host.toLowerCase()) ? true : false;
}
function promiseWithResolvers() {
    let resolve;
    let reject;
    const promise = new Promise(function withResolversExecutor(promiseResolve, promiseReject) {
        resolve = promiseResolve;
        reject = promiseReject;
    });
    return {
        promise,
        resolve,
        reject
    };
}
/**
 * A noop function intended for use in preventing unhandled rejections.
 *
 * @example
 * ```js
 * const promise = myAsyncTask();
 * // eslint-disable-next-line github/no-then
 * promise.then(undefined, squashError);
 * ```
 */ function squashError(_error) {
    return;
}
exports.randomBytes = (0, util_1.promisify)(crypto.randomBytes);
/**
 * Replicates the events.once helper.
 *
 * Removes unused signal logic and It **only** supports 0 or 1 argument events.
 *
 * @param ee - An event emitter that may emit `ev`
 * @param name - An event name to wait for
 */ async function once(ee, name, options) {
    options?.signal?.throwIfAborted();
    const { promise, resolve, reject } = promiseWithResolvers();
    const onEvent = (data)=>resolve(data);
    const onError = (error)=>reject(error);
    const abortListener = addAbortListener(options?.signal, function() {
        reject(this.reason);
    });
    ee.once(name, onEvent).once('error', onError);
    try {
        return await promise;
    } finally{
        ee.off(name, onEvent);
        ee.off('error', onError);
        abortListener?.[exports.kDispose]();
    }
}
function maybeAddIdToDocuments(collection, document, options) {
    const forceServerObjectId = options.forceServerObjectId ?? collection.db.options?.forceServerObjectId ?? false;
    // no need to modify the docs if server sets the ObjectId
    if (forceServerObjectId) {
        return document;
    }
    if (document._id == null) {
        document._id = collection.s.pkFactory.createPk();
    }
    return document;
}
async function fileIsAccessible(fileName, mode) {
    try {
        await fs_1.promises.access(fileName, mode);
        return true;
    } catch  {
        return false;
    }
}
function csotMin(duration1, duration2) {
    if (duration1 === 0) return duration2;
    if (duration2 === 0) return duration1;
    return Math.min(duration1, duration2);
}
function noop() {
    return;
}
/**
 * Recurse through the (identically-shaped) `decrypted` and `original`
 * objects and attach a `decryptedKeys` property on each sub-object that
 * contained encrypted fields. Because we only call this on BSON responses,
 * we do not need to worry about circular references.
 *
 * @internal
 */ function decorateDecryptionResult(decrypted, original, isTopLevelDecorateCall = true) {
    if (isTopLevelDecorateCall) {
        // The original value could have been either a JS object or a BSON buffer
        if (__TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["Buffer"].isBuffer(original)) {
            original = (0, bson_1.deserialize)(original);
        }
        if (__TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["Buffer"].isBuffer(decrypted)) {
            throw new error_1.MongoRuntimeError('Expected result of decryption to be deserialized BSON object');
        }
    }
    if (!decrypted || typeof decrypted !== 'object') return;
    for (const k of Object.keys(decrypted)){
        const originalValue = original[k];
        // An object was decrypted by libmongocrypt if and only if it was
        // a BSON Binary object with subtype 6.
        if (originalValue && originalValue._bsontype === 'Binary' && originalValue.sub_type === 6) {
            if (!decrypted[constants_2.kDecoratedKeys]) {
                Object.defineProperty(decrypted, constants_2.kDecoratedKeys, {
                    value: [],
                    configurable: true,
                    enumerable: false,
                    writable: false
                });
            }
            // this is defined in the preceding if-statement
            // eslint-disable-next-line @typescript-eslint/no-non-null-assertion
            decrypted[constants_2.kDecoratedKeys].push(k);
            continue;
        }
        decorateDecryptionResult(decrypted[k], originalValue, false);
    }
}
/** @internal */ exports.kDispose = Symbol.dispose ?? Symbol('dispose');
/**
 * A utility that helps with writing listener code idiomatically
 *
 * @example
 * ```js
 * using listener = addAbortListener(signal, function () {
 *   console.log('aborted', this.reason);
 * });
 * ```
 *
 * @param signal - if exists adds an abort listener
 * @param listener - the listener to be added to signal
 * @returns A disposable that will remove the abort listener
 */ function addAbortListener(signal, listener) {
    if (signal == null) return;
    signal.addEventListener('abort', listener, {
        once: true
    });
    return {
        [exports.kDispose]: ()=>signal.removeEventListener('abort', listener)
    };
}
/**
 * Takes a promise and races it with a promise wrapping the abort event of the optionally provided signal.
 * The given promise is _always_ ordered before the signal's abort promise.
 * When given an already rejected promise and an already aborted signal, the promise's rejection takes precedence.
 *
 * Any asynchronous processing in `promise` will continue even after the abort signal has fired,
 * but control will be returned to the caller
 *
 * @see https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise/race
 *
 * @param promise - A promise to discard if the signal aborts
 * @param options - An options object carrying an optional signal
 */ async function abortable(promise, { signal }) {
    if (signal == null) {
        return await promise;
    }
    const { promise: aborted, reject } = promiseWithResolvers();
    const abortListener = signal.aborted ? reject(signal.reason) : addAbortListener(signal, function() {
        reject(this.reason);
    });
    try {
        return await Promise.race([
            promise,
            aborted
        ]);
    } finally{
        abortListener?.[exports.kDispose]();
    }
} //# sourceMappingURL=utils.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/explain.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.Explain = exports.ExplainVerbosity = void 0;
exports.validateExplainTimeoutOptions = validateExplainTimeoutOptions;
exports.decorateWithExplain = decorateWithExplain;
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
/** @public */ exports.ExplainVerbosity = Object.freeze({
    queryPlanner: 'queryPlanner',
    queryPlannerExtended: 'queryPlannerExtended',
    executionStats: 'executionStats',
    allPlansExecution: 'allPlansExecution'
});
/** @internal */ class Explain {
    constructor(verbosity, maxTimeMS){
        if (typeof verbosity === 'boolean') {
            this.verbosity = verbosity ? exports.ExplainVerbosity.allPlansExecution : exports.ExplainVerbosity.queryPlanner;
        } else {
            this.verbosity = verbosity;
        }
        this.maxTimeMS = maxTimeMS;
    }
    static fromOptions({ explain } = {}) {
        if (explain == null) return;
        if (typeof explain === 'boolean' || typeof explain === 'string') {
            return new Explain(explain);
        }
        const { verbosity, maxTimeMS } = explain;
        return new Explain(verbosity, maxTimeMS);
    }
}
exports.Explain = Explain;
function validateExplainTimeoutOptions(options, explain) {
    const { maxTimeMS, timeoutMS } = options;
    if (timeoutMS != null && (maxTimeMS != null || explain?.maxTimeMS != null)) {
        throw new error_1.MongoAPIError('Cannot use maxTimeMS with timeoutMS for explain commands.');
    }
}
/**
 * Applies an explain to a given command.
 * @internal
 *
 * @param command - the command on which to apply the explain
 * @param options - the options containing the explain verbosity
 */ function decorateWithExplain(command, explain) {
    const { verbosity, maxTimeMS } = explain;
    const baseCommand = {
        explain: command,
        verbosity
    };
    if (typeof maxTimeMS === 'number') {
        baseCommand.maxTimeMS = maxTimeMS;
    }
    return baseCommand;
} //# sourceMappingURL=explain.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/timeout.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.LegacyTimeoutContext = exports.CSOTTimeoutContext = exports.TimeoutContext = exports.Timeout = exports.TimeoutError = void 0;
const timers_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/next@16.0.6_@babel+core@7.28.5_react-dom@19.2.0_react@19.2.0__react@19.2.0/node_modules/next/dist/compiled/timers-browserify/main.js [app-client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/utils.js [app-client] (ecmascript)");
/** @internal */ class TimeoutError extends Error {
    get name() {
        return 'TimeoutError';
    }
    constructor(message, options){
        super(message, options);
        this.duration = options.duration;
    }
    static is(error) {
        return error != null && typeof error === 'object' && 'name' in error && error.name === 'TimeoutError';
    }
}
exports.TimeoutError = TimeoutError;
/**
 * @internal
 * This class is an abstraction over timeouts
 * The Timeout class can only be in the pending or rejected states. It is guaranteed not to resolve
 * if interacted with exclusively through its public API
 * */ class Timeout extends Promise {
    get remainingTime() {
        if (this.timedOut) return 0;
        if (this.duration === 0) return Infinity;
        return this.start + this.duration - Math.trunc(performance.now());
    }
    get timeElapsed() {
        return Math.trunc(performance.now()) - this.start;
    }
    /** Create a new timeout that expires in `duration` ms */ constructor(executor = ()=>null, options){
        const duration = options?.duration ?? 0;
        const unref = !!options?.unref;
        const rejection = options?.rejection;
        if (duration < 0) {
            throw new error_1.MongoInvalidArgumentError('Cannot create a Timeout with a negative duration');
        }
        let reject;
        super((_, promiseReject)=>{
            reject = promiseReject;
            executor(utils_1.noop, promiseReject);
        });
        this.ended = null;
        this.timedOut = false;
        this.cleared = false;
        this.duration = duration;
        this.start = Math.trunc(performance.now());
        if (rejection == null && this.duration > 0) {
            this.id = (0, timers_1.setTimeout)(()=>{
                this.ended = Math.trunc(performance.now());
                this.timedOut = true;
                reject(new TimeoutError(`Expired after ${duration}ms`, {
                    duration
                }));
            }, this.duration);
            if (typeof this.id.unref === 'function' && unref) {
                // Ensure we do not keep the Node.js event loop running
                this.id.unref();
            }
        } else if (rejection != null) {
            this.ended = Math.trunc(performance.now());
            this.timedOut = true;
            reject(rejection);
        }
    }
    /**
     * Clears the underlying timeout. This method is idempotent
     */ clear() {
        (0, timers_1.clearTimeout)(this.id);
        this.id = undefined;
        this.timedOut = false;
        this.cleared = true;
    }
    throwIfExpired() {
        if (this.timedOut) {
            // This method is invoked when someone wants to throw immediately instead of await the result of this promise
            // Since they won't be handling the rejection from the promise (because we're about to throw here)
            // attach handling to prevent this from bubbling up to Node.js
            this.then(undefined, utils_1.squashError);
            throw new TimeoutError('Timed out', {
                duration: this.duration
            });
        }
    }
    static expires(duration, unref) {
        return new Timeout(undefined, {
            duration,
            unref
        });
    }
    static reject(rejection) {
        return new Timeout(undefined, {
            duration: 0,
            unref: true,
            rejection
        });
    }
}
exports.Timeout = Timeout;
function isLegacyTimeoutContextOptions(v) {
    return v != null && typeof v === 'object' && 'serverSelectionTimeoutMS' in v && typeof v.serverSelectionTimeoutMS === 'number' && 'waitQueueTimeoutMS' in v && typeof v.waitQueueTimeoutMS === 'number';
}
function isCSOTTimeoutContextOptions(v) {
    return v != null && typeof v === 'object' && 'serverSelectionTimeoutMS' in v && typeof v.serverSelectionTimeoutMS === 'number' && 'timeoutMS' in v && typeof v.timeoutMS === 'number';
}
/** @internal */ class TimeoutContext {
    static create(options) {
        if (options.session?.timeoutContext != null) return options.session?.timeoutContext;
        if (isCSOTTimeoutContextOptions(options)) return new CSOTTimeoutContext(options);
        else if (isLegacyTimeoutContextOptions(options)) return new LegacyTimeoutContext(options);
        else throw new error_1.MongoRuntimeError('Unrecognized options');
    }
}
exports.TimeoutContext = TimeoutContext;
/** @internal */ class CSOTTimeoutContext extends TimeoutContext {
    constructor(options){
        super();
        this.minRoundTripTime = 0;
        this.start = Math.trunc(performance.now());
        this.timeoutMS = options.timeoutMS;
        this.serverSelectionTimeoutMS = options.serverSelectionTimeoutMS;
        this.socketTimeoutMS = options.socketTimeoutMS;
        this.clearServerSelectionTimeout = false;
    }
    get maxTimeMS() {
        return this.remainingTimeMS - this.minRoundTripTime;
    }
    get remainingTimeMS() {
        const timePassed = Math.trunc(performance.now()) - this.start;
        return this.timeoutMS <= 0 ? Infinity : this.timeoutMS - timePassed;
    }
    csotEnabled() {
        return true;
    }
    get serverSelectionTimeout() {
        // check for undefined
        if (typeof this._serverSelectionTimeout !== 'object' || this._serverSelectionTimeout?.cleared) {
            const { remainingTimeMS, serverSelectionTimeoutMS } = this;
            if (remainingTimeMS <= 0) return Timeout.reject(new error_1.MongoOperationTimeoutError(`Timed out in server selection after ${this.timeoutMS}ms`));
            const usingServerSelectionTimeoutMS = serverSelectionTimeoutMS !== 0 && (0, utils_1.csotMin)(remainingTimeMS, serverSelectionTimeoutMS) === serverSelectionTimeoutMS;
            if (usingServerSelectionTimeoutMS) {
                this._serverSelectionTimeout = Timeout.expires(serverSelectionTimeoutMS);
            } else {
                if (remainingTimeMS > 0 && Number.isFinite(remainingTimeMS)) {
                    this._serverSelectionTimeout = Timeout.expires(remainingTimeMS);
                } else {
                    this._serverSelectionTimeout = null;
                }
            }
        }
        return this._serverSelectionTimeout;
    }
    get connectionCheckoutTimeout() {
        if (typeof this._connectionCheckoutTimeout !== 'object' || this._connectionCheckoutTimeout?.cleared) {
            if (typeof this._serverSelectionTimeout === 'object') {
                // null or Timeout
                this._connectionCheckoutTimeout = this._serverSelectionTimeout;
            } else {
                throw new error_1.MongoRuntimeError('Unreachable. If you are seeing this error, please file a ticket on the NODE driver project on Jira');
            }
        }
        return this._connectionCheckoutTimeout;
    }
    get timeoutForSocketWrite() {
        const { remainingTimeMS } = this;
        if (!Number.isFinite(remainingTimeMS)) return null;
        if (remainingTimeMS > 0) return Timeout.expires(remainingTimeMS);
        return Timeout.reject(new error_1.MongoOperationTimeoutError('Timed out before socket write'));
    }
    get timeoutForSocketRead() {
        const { remainingTimeMS } = this;
        if (!Number.isFinite(remainingTimeMS)) return null;
        if (remainingTimeMS > 0) return Timeout.expires(remainingTimeMS);
        return Timeout.reject(new error_1.MongoOperationTimeoutError('Timed out before socket read'));
    }
    refresh() {
        this.start = Math.trunc(performance.now());
        this.minRoundTripTime = 0;
        this._serverSelectionTimeout?.clear();
        this._connectionCheckoutTimeout?.clear();
    }
    clear() {
        this._serverSelectionTimeout?.clear();
        this._connectionCheckoutTimeout?.clear();
    }
    /**
     * @internal
     * Throws a MongoOperationTimeoutError if the context has expired.
     * If the context has not expired, returns the `remainingTimeMS`
     **/ getRemainingTimeMSOrThrow(message) {
        const { remainingTimeMS } = this;
        if (remainingTimeMS <= 0) throw new error_1.MongoOperationTimeoutError(message ?? `Expired after ${this.timeoutMS}ms`);
        return remainingTimeMS;
    }
    /**
     * @internal
     * This method is intended to be used in situations where concurrent operation are on the same deadline, but cannot share a single `TimeoutContext` instance.
     * Returns a new instance of `CSOTTimeoutContext` constructed with identical options, but setting the `start` property to `this.start`.
     */ clone() {
        const timeoutContext = new CSOTTimeoutContext({
            timeoutMS: this.timeoutMS,
            serverSelectionTimeoutMS: this.serverSelectionTimeoutMS
        });
        timeoutContext.start = this.start;
        return timeoutContext;
    }
    refreshed() {
        return new CSOTTimeoutContext(this);
    }
    addMaxTimeMSToCommand(command, options) {
        if (options.omitMaxTimeMS) return;
        const maxTimeMS = this.remainingTimeMS - this.minRoundTripTime;
        if (maxTimeMS > 0 && Number.isFinite(maxTimeMS)) command.maxTimeMS = maxTimeMS;
    }
    getSocketTimeoutMS() {
        return 0;
    }
}
exports.CSOTTimeoutContext = CSOTTimeoutContext;
/** @internal */ class LegacyTimeoutContext extends TimeoutContext {
    constructor(options){
        super();
        this.options = options;
        this.clearServerSelectionTimeout = true;
    }
    csotEnabled() {
        return false;
    }
    get serverSelectionTimeout() {
        if (this.options.serverSelectionTimeoutMS != null && this.options.serverSelectionTimeoutMS > 0) return Timeout.expires(this.options.serverSelectionTimeoutMS);
        return null;
    }
    get connectionCheckoutTimeout() {
        if (this.options.waitQueueTimeoutMS != null && this.options.waitQueueTimeoutMS > 0) return Timeout.expires(this.options.waitQueueTimeoutMS);
        return null;
    }
    get timeoutForSocketWrite() {
        return null;
    }
    get timeoutForSocketRead() {
        return null;
    }
    refresh() {
        return;
    }
    clear() {
        return;
    }
    get maxTimeMS() {
        return null;
    }
    refreshed() {
        return new LegacyTimeoutContext(this.options);
    }
    addMaxTimeMSToCommand(_command, _options) {
    // No max timeMS is added to commands in legacy timeout mode.
    }
    getSocketTimeoutMS() {
        return this.options.socketTimeoutMS;
    }
}
exports.LegacyTimeoutContext = LegacyTimeoutContext; //# sourceMappingURL=timeout.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/sort.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.formatSort = formatSort;
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
/** @internal */ function prepareDirection(direction = 1) {
    const value = `${direction}`.toLowerCase();
    if (isMeta(direction)) return direction;
    switch(value){
        case 'ascending':
        case 'asc':
        case '1':
            return 1;
        case 'descending':
        case 'desc':
        case '-1':
            return -1;
        default:
            throw new error_1.MongoInvalidArgumentError(`Invalid sort direction: ${JSON.stringify(direction)}`);
    }
}
/** @internal */ function isMeta(t) {
    return typeof t === 'object' && t != null && '$meta' in t && typeof t.$meta === 'string';
}
/** @internal */ function isPair(t) {
    if (Array.isArray(t) && t.length === 2) {
        try {
            prepareDirection(t[1]);
            return true;
        } catch  {
            return false;
        }
    }
    return false;
}
function isDeep(t) {
    return Array.isArray(t) && Array.isArray(t[0]);
}
function isMap(t) {
    return t instanceof Map && t.size > 0;
}
function isReadonlyArray(value) {
    return Array.isArray(value);
}
/** @internal */ function pairToMap(v) {
    return new Map([
        [
            `${v[0]}`,
            prepareDirection([
                v[1]
            ])
        ]
    ]);
}
/** @internal */ function deepToMap(t) {
    const sortEntries = t.map(([k, v])=>[
            `${k}`,
            prepareDirection(v)
        ]);
    return new Map(sortEntries);
}
/** @internal */ function stringsToMap(t) {
    const sortEntries = t.map((key)=>[
            `${key}`,
            1
        ]);
    return new Map(sortEntries);
}
/** @internal */ function objectToMap(t) {
    const sortEntries = Object.entries(t).map(([k, v])=>[
            `${k}`,
            prepareDirection(v)
        ]);
    return new Map(sortEntries);
}
/** @internal */ function mapToMap(t) {
    const sortEntries = Array.from(t).map(([k, v])=>[
            `${k}`,
            prepareDirection(v)
        ]);
    return new Map(sortEntries);
}
/** converts a Sort type into a type that is valid for the server (SortForCmd) */ function formatSort(sort, direction) {
    if (sort == null) return undefined;
    if (typeof sort === 'string') return new Map([
        [
            sort,
            prepareDirection(direction)
        ]
    ]); // 'fieldName'
    if (typeof sort !== 'object') {
        throw new error_1.MongoInvalidArgumentError(`Invalid sort format: ${JSON.stringify(sort)} Sort must be a valid object`);
    }
    if (!isReadonlyArray(sort)) {
        if (isMap(sort)) return mapToMap(sort); // Map<fieldName, SortDirection>
        if (Object.keys(sort).length) return objectToMap(sort); // { [fieldName: string]: SortDirection }
        return undefined;
    }
    if (!sort.length) return undefined;
    if (isDeep(sort)) return deepToMap(sort); // [ [fieldName, sortDir], [fieldName, sortDir] ... ]
    if (isPair(sort)) return pairToMap(sort); // [ fieldName, sortDir ]
    return stringsToMap(sort); // [ fieldName, fieldName ]
} //# sourceMappingURL=sort.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bulk/common.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.BulkOperationBase = exports.FindOperators = exports.MongoBulkWriteError = exports.WriteError = exports.WriteConcernError = exports.BulkWriteResult = exports.Batch = exports.BatchType = void 0;
exports.mergeBatchResults = mergeBatchResults;
const bson_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bson.js [app-client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
const delete_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/delete.js [app-client] (ecmascript)");
const execute_operation_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/execute_operation.js [app-client] (ecmascript)");
const insert_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/insert.js [app-client] (ecmascript)");
const update_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/update.js [app-client] (ecmascript)");
const timeout_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/timeout.js [app-client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/utils.js [app-client] (ecmascript)");
const write_concern_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/write_concern.js [app-client] (ecmascript)");
/** @public */ exports.BatchType = Object.freeze({
    INSERT: 1,
    UPDATE: 2,
    DELETE: 3
});
/**
 * Keeps the state of a unordered batch so we can rewrite the results
 * correctly after command execution
 *
 * @public
 */ class Batch {
    constructor(batchType, originalZeroIndex){
        this.originalZeroIndex = originalZeroIndex;
        this.currentIndex = 0;
        this.originalIndexes = [];
        this.batchType = batchType;
        this.operations = [];
        this.size = 0;
        this.sizeBytes = 0;
    }
}
exports.Batch = Batch;
/**
 * @public
 * The result of a bulk write.
 */ class BulkWriteResult {
    static generateIdMap(ids) {
        const idMap = {};
        for (const doc of ids){
            idMap[doc.index] = doc._id;
        }
        return idMap;
    }
    /**
     * Create a new BulkWriteResult instance
     * @internal
     */ constructor(bulkResult, isOrdered){
        this.result = bulkResult;
        this.insertedCount = this.result.nInserted ?? 0;
        this.matchedCount = this.result.nMatched ?? 0;
        this.modifiedCount = this.result.nModified ?? 0;
        this.deletedCount = this.result.nRemoved ?? 0;
        this.upsertedCount = this.result.upserted.length ?? 0;
        this.upsertedIds = BulkWriteResult.generateIdMap(this.result.upserted);
        this.insertedIds = BulkWriteResult.generateIdMap(this.getSuccessfullyInsertedIds(bulkResult, isOrdered));
        Object.defineProperty(this, 'result', {
            value: this.result,
            enumerable: false
        });
    }
    /** Evaluates to true if the bulk operation correctly executes */ get ok() {
        return this.result.ok;
    }
    /**
     * Returns document_ids that were actually inserted
     * @internal
     */ getSuccessfullyInsertedIds(bulkResult, isOrdered) {
        if (bulkResult.writeErrors.length === 0) return bulkResult.insertedIds;
        if (isOrdered) {
            return bulkResult.insertedIds.slice(0, bulkResult.writeErrors[0].index);
        }
        return bulkResult.insertedIds.filter(({ index })=>!bulkResult.writeErrors.some((writeError)=>index === writeError.index));
    }
    /** Returns the upserted id at the given index */ getUpsertedIdAt(index) {
        return this.result.upserted[index];
    }
    /** Returns raw internal result */ getRawResponse() {
        return this.result;
    }
    /** Returns true if the bulk operation contains a write error */ hasWriteErrors() {
        return this.result.writeErrors.length > 0;
    }
    /** Returns the number of write errors from the bulk operation */ getWriteErrorCount() {
        return this.result.writeErrors.length;
    }
    /** Returns a specific write error object */ getWriteErrorAt(index) {
        return index < this.result.writeErrors.length ? this.result.writeErrors[index] : undefined;
    }
    /** Retrieve all write errors */ getWriteErrors() {
        return this.result.writeErrors;
    }
    /** Retrieve the write concern error if one exists */ getWriteConcernError() {
        if (this.result.writeConcernErrors.length === 0) {
            return;
        } else if (this.result.writeConcernErrors.length === 1) {
            // Return the error
            return this.result.writeConcernErrors[0];
        } else {
            // Combine the errors
            let errmsg = '';
            for(let i = 0; i < this.result.writeConcernErrors.length; i++){
                const err = this.result.writeConcernErrors[i];
                errmsg = errmsg + err.errmsg;
                // TODO: Something better
                if (i === 0) errmsg = errmsg + ' and ';
            }
            return new WriteConcernError({
                errmsg,
                code: error_1.MONGODB_ERROR_CODES.WriteConcernTimeout
            });
        }
    }
    toString() {
        return `BulkWriteResult(${bson_1.EJSON.stringify(this.result)})`;
    }
    isOk() {
        return this.result.ok === 1;
    }
}
exports.BulkWriteResult = BulkWriteResult;
/**
 * An error representing a failure by the server to apply the requested write concern to the bulk operation.
 * @public
 * @category Error
 */ class WriteConcernError {
    constructor(error){
        this.serverError = error;
    }
    /** Write concern error code. */ get code() {
        return this.serverError.code;
    }
    /** Write concern error message. */ get errmsg() {
        return this.serverError.errmsg;
    }
    /** Write concern error info. */ get errInfo() {
        return this.serverError.errInfo;
    }
    toJSON() {
        return this.serverError;
    }
    toString() {
        return `WriteConcernError(${this.errmsg})`;
    }
}
exports.WriteConcernError = WriteConcernError;
/**
 * An error that occurred during a BulkWrite on the server.
 * @public
 * @category Error
 */ class WriteError {
    constructor(err){
        this.err = err;
    }
    /** WriteError code. */ get code() {
        return this.err.code;
    }
    /** WriteError original bulk operation index. */ get index() {
        return this.err.index;
    }
    /** WriteError message. */ get errmsg() {
        return this.err.errmsg;
    }
    /** WriteError details. */ get errInfo() {
        return this.err.errInfo;
    }
    /** Returns the underlying operation that caused the error */ getOperation() {
        return this.err.op;
    }
    toJSON() {
        return {
            code: this.err.code,
            index: this.err.index,
            errmsg: this.err.errmsg,
            op: this.err.op
        };
    }
    toString() {
        return `WriteError(${JSON.stringify(this.toJSON())})`;
    }
}
exports.WriteError = WriteError;
/** Merges results into shared data structure */ function mergeBatchResults(batch, bulkResult, err, result) {
    // If we have an error set the result to be the err object
    if (err) {
        result = err;
    } else if (result && result.result) {
        result = result.result;
    }
    if (result == null) {
        return;
    }
    // Do we have a top level error stop processing and return
    if (result.ok === 0 && bulkResult.ok === 1) {
        bulkResult.ok = 0;
        const writeError = {
            index: 0,
            code: result.code || 0,
            errmsg: result.message,
            errInfo: result.errInfo,
            op: batch.operations[0]
        };
        bulkResult.writeErrors.push(new WriteError(writeError));
        return;
    } else if (result.ok === 0 && bulkResult.ok === 0) {
        return;
    }
    // If we have an insert Batch type
    if (isInsertBatch(batch) && result.n) {
        bulkResult.nInserted = bulkResult.nInserted + result.n;
    }
    // If we have an insert Batch type
    if (isDeleteBatch(batch) && result.n) {
        bulkResult.nRemoved = bulkResult.nRemoved + result.n;
    }
    let nUpserted = 0;
    // We have an array of upserted values, we need to rewrite the indexes
    if (Array.isArray(result.upserted)) {
        nUpserted = result.upserted.length;
        for(let i = 0; i < result.upserted.length; i++){
            bulkResult.upserted.push({
                index: result.upserted[i].index + batch.originalZeroIndex,
                _id: result.upserted[i]._id
            });
        }
    } else if (result.upserted) {
        nUpserted = 1;
        bulkResult.upserted.push({
            index: batch.originalZeroIndex,
            _id: result.upserted
        });
    }
    // If we have an update Batch type
    if (isUpdateBatch(batch) && result.n) {
        const nModified = result.nModified;
        bulkResult.nUpserted = bulkResult.nUpserted + nUpserted;
        bulkResult.nMatched = bulkResult.nMatched + (result.n - nUpserted);
        if (typeof nModified === 'number') {
            bulkResult.nModified = bulkResult.nModified + nModified;
        } else {
            bulkResult.nModified = 0;
        }
    }
    if (Array.isArray(result.writeErrors)) {
        for(let i = 0; i < result.writeErrors.length; i++){
            const writeError = {
                index: batch.originalIndexes[result.writeErrors[i].index],
                code: result.writeErrors[i].code,
                errmsg: result.writeErrors[i].errmsg,
                errInfo: result.writeErrors[i].errInfo,
                op: batch.operations[result.writeErrors[i].index]
            };
            bulkResult.writeErrors.push(new WriteError(writeError));
        }
    }
    if (result.writeConcernError) {
        bulkResult.writeConcernErrors.push(new WriteConcernError(result.writeConcernError));
    }
}
async function executeCommands(bulkOperation, options) {
    if (bulkOperation.s.batches.length === 0) {
        return new BulkWriteResult(bulkOperation.s.bulkResult, bulkOperation.isOrdered);
    }
    for (const batch of bulkOperation.s.batches){
        const finalOptions = (0, utils_1.resolveOptions)(bulkOperation, {
            ...options,
            ordered: bulkOperation.isOrdered
        });
        if (finalOptions.bypassDocumentValidation !== true) {
            delete finalOptions.bypassDocumentValidation;
        }
        // Is the bypassDocumentValidation options specific
        if (bulkOperation.s.bypassDocumentValidation === true) {
            finalOptions.bypassDocumentValidation = true;
        }
        // Is the checkKeys option disabled
        if (bulkOperation.s.checkKeys === false) {
            finalOptions.checkKeys = false;
        }
        if (bulkOperation.retryWrites) {
            if (isUpdateBatch(batch)) {
                bulkOperation.retryWrites = bulkOperation.retryWrites && !batch.operations.some((op)=>op.multi);
            }
            if (isDeleteBatch(batch)) {
                bulkOperation.retryWrites = bulkOperation.retryWrites && !batch.operations.some((op)=>op.limit === 0);
            }
        }
        const operation = isInsertBatch(batch) ? new insert_1.InsertOperation(bulkOperation.s.namespace, batch.operations, finalOptions) : isUpdateBatch(batch) ? new update_1.UpdateOperation(bulkOperation.s.namespace, batch.operations, finalOptions) : isDeleteBatch(batch) ? new delete_1.DeleteOperation(bulkOperation.s.namespace, batch.operations, finalOptions) : null;
        if (operation == null) throw new error_1.MongoRuntimeError(`Unknown batchType: ${batch.batchType}`);
        let thrownError = null;
        let result;
        try {
            result = await (0, execute_operation_1.executeOperation)(bulkOperation.s.collection.client, operation, finalOptions.timeoutContext);
        } catch (error) {
            thrownError = error;
        }
        if (thrownError != null) {
            if (thrownError instanceof error_1.MongoWriteConcernError) {
                mergeBatchResults(batch, bulkOperation.s.bulkResult, thrownError, result);
                const writeResult = new BulkWriteResult(bulkOperation.s.bulkResult, bulkOperation.isOrdered);
                throw new MongoBulkWriteError({
                    message: thrownError.result.writeConcernError.errmsg,
                    code: thrownError.result.writeConcernError.code
                }, writeResult);
            } else {
                // Error is a driver related error not a bulk op error, return early
                throw new MongoBulkWriteError(thrownError, new BulkWriteResult(bulkOperation.s.bulkResult, bulkOperation.isOrdered));
            }
        }
        mergeBatchResults(batch, bulkOperation.s.bulkResult, thrownError, result);
        const writeResult = new BulkWriteResult(bulkOperation.s.bulkResult, bulkOperation.isOrdered);
        bulkOperation.handleWriteError(writeResult);
    }
    bulkOperation.s.batches.length = 0;
    const writeResult = new BulkWriteResult(bulkOperation.s.bulkResult, bulkOperation.isOrdered);
    bulkOperation.handleWriteError(writeResult);
    return writeResult;
}
/**
 * An error indicating an unsuccessful Bulk Write
 * @public
 * @category Error
 */ class MongoBulkWriteError extends error_1.MongoServerError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(error, result){
        super(error);
        this.writeErrors = [];
        if (error instanceof WriteConcernError) this.err = error;
        else if (!(error instanceof Error)) {
            this.message = error.message;
            this.code = error.code;
            this.writeErrors = error.writeErrors ?? [];
        }
        this.result = result;
        Object.assign(this, error);
    }
    get name() {
        return 'MongoBulkWriteError';
    }
    /** Number of documents inserted. */ get insertedCount() {
        return this.result.insertedCount;
    }
    /** Number of documents matched for update. */ get matchedCount() {
        return this.result.matchedCount;
    }
    /** Number of documents modified. */ get modifiedCount() {
        return this.result.modifiedCount;
    }
    /** Number of documents deleted. */ get deletedCount() {
        return this.result.deletedCount;
    }
    /** Number of documents upserted. */ get upsertedCount() {
        return this.result.upsertedCount;
    }
    /** Inserted document generated Id's, hash key is the index of the originating operation */ get insertedIds() {
        return this.result.insertedIds;
    }
    /** Upserted document generated Id's, hash key is the index of the originating operation */ get upsertedIds() {
        return this.result.upsertedIds;
    }
}
exports.MongoBulkWriteError = MongoBulkWriteError;
/**
 * A builder object that is returned from {@link BulkOperationBase#find}.
 * Is used to build a write operation that involves a query filter.
 *
 * @public
 */ class FindOperators {
    /**
     * Creates a new FindOperators object.
     * @internal
     */ constructor(bulkOperation){
        this.bulkOperation = bulkOperation;
    }
    /** Add a multiple update operation to the bulk operation */ update(updateDocument) {
        const currentOp = buildCurrentOp(this.bulkOperation);
        return this.bulkOperation.addToOperationsList(exports.BatchType.UPDATE, (0, update_1.makeUpdateStatement)(currentOp.selector, updateDocument, {
            ...currentOp,
            multi: true
        }));
    }
    /** Add a single update operation to the bulk operation */ updateOne(updateDocument) {
        if (!(0, utils_1.hasAtomicOperators)(updateDocument, this.bulkOperation.bsonOptions)) {
            throw new error_1.MongoInvalidArgumentError('Update document requires atomic operators');
        }
        const currentOp = buildCurrentOp(this.bulkOperation);
        return this.bulkOperation.addToOperationsList(exports.BatchType.UPDATE, (0, update_1.makeUpdateStatement)(currentOp.selector, updateDocument, {
            ...currentOp,
            multi: false
        }));
    }
    /** Add a replace one operation to the bulk operation */ replaceOne(replacement) {
        if ((0, utils_1.hasAtomicOperators)(replacement)) {
            throw new error_1.MongoInvalidArgumentError('Replacement document must not use atomic operators');
        }
        const currentOp = buildCurrentOp(this.bulkOperation);
        return this.bulkOperation.addToOperationsList(exports.BatchType.UPDATE, (0, update_1.makeUpdateStatement)(currentOp.selector, replacement, {
            ...currentOp,
            multi: false
        }));
    }
    /** Add a delete one operation to the bulk operation */ deleteOne() {
        const currentOp = buildCurrentOp(this.bulkOperation);
        return this.bulkOperation.addToOperationsList(exports.BatchType.DELETE, (0, delete_1.makeDeleteStatement)(currentOp.selector, {
            ...currentOp,
            limit: 1
        }));
    }
    /** Add a delete many operation to the bulk operation */ delete() {
        const currentOp = buildCurrentOp(this.bulkOperation);
        return this.bulkOperation.addToOperationsList(exports.BatchType.DELETE, (0, delete_1.makeDeleteStatement)(currentOp.selector, {
            ...currentOp,
            limit: 0
        }));
    }
    /** Upsert modifier for update bulk operation, noting that this operation is an upsert. */ upsert() {
        if (!this.bulkOperation.s.currentOp) {
            this.bulkOperation.s.currentOp = {};
        }
        this.bulkOperation.s.currentOp.upsert = true;
        return this;
    }
    /** Specifies the collation for the query condition. */ collation(collation) {
        if (!this.bulkOperation.s.currentOp) {
            this.bulkOperation.s.currentOp = {};
        }
        this.bulkOperation.s.currentOp.collation = collation;
        return this;
    }
    /** Specifies arrayFilters for UpdateOne or UpdateMany bulk operations. */ arrayFilters(arrayFilters) {
        if (!this.bulkOperation.s.currentOp) {
            this.bulkOperation.s.currentOp = {};
        }
        this.bulkOperation.s.currentOp.arrayFilters = arrayFilters;
        return this;
    }
    /** Specifies hint for the bulk operation. */ hint(hint) {
        if (!this.bulkOperation.s.currentOp) {
            this.bulkOperation.s.currentOp = {};
        }
        this.bulkOperation.s.currentOp.hint = hint;
        return this;
    }
}
exports.FindOperators = FindOperators;
/** @public */ class BulkOperationBase {
    /**
     * Create a new OrderedBulkOperation or UnorderedBulkOperation instance
     * @internal
     */ constructor(collection, options, isOrdered){
        this.collection = collection;
        this.retryWrites = collection.db.options?.retryWrites;
        // determine whether bulkOperation is ordered or unordered
        this.isOrdered = isOrdered;
        const topology = (0, utils_1.getTopology)(collection);
        options = options == null ? {} : options;
        // TODO Bring from driver information in hello
        // Get the namespace for the write operations
        const namespace = collection.s.namespace;
        // Used to mark operation as executed
        const executed = false;
        // Current item
        const currentOp = undefined;
        // Set max byte size
        const hello = topology.lastHello();
        // If we have autoEncryption on, batch-splitting must be done on 2mb chunks, but single documents
        // over 2mb are still allowed
        const usingAutoEncryption = !!(topology.s.options && topology.s.options.autoEncrypter);
        const maxBsonObjectSize = hello && hello.maxBsonObjectSize ? hello.maxBsonObjectSize : 1024 * 1024 * 16;
        const maxBatchSizeBytes = usingAutoEncryption ? 1024 * 1024 * 2 : maxBsonObjectSize;
        const maxWriteBatchSize = hello && hello.maxWriteBatchSize ? hello.maxWriteBatchSize : 1000;
        // Calculates the largest possible size of an Array key, represented as a BSON string
        // element. This calculation:
        //     1 byte for BSON type
        //     # of bytes = length of (string representation of (maxWriteBatchSize - 1))
        //   + 1 bytes for null terminator
        const maxKeySize = (maxWriteBatchSize - 1).toString(10).length + 2;
        // Final results
        const bulkResult = {
            ok: 1,
            writeErrors: [],
            writeConcernErrors: [],
            insertedIds: [],
            nInserted: 0,
            nUpserted: 0,
            nMatched: 0,
            nModified: 0,
            nRemoved: 0,
            upserted: []
        };
        // Internal state
        this.s = {
            // Final result
            bulkResult,
            // Current batch state
            currentBatch: undefined,
            currentIndex: 0,
            // ordered specific
            currentBatchSize: 0,
            currentBatchSizeBytes: 0,
            // unordered specific
            currentInsertBatch: undefined,
            currentUpdateBatch: undefined,
            currentRemoveBatch: undefined,
            batches: [],
            // Write concern
            writeConcern: write_concern_1.WriteConcern.fromOptions(options),
            // Max batch size options
            maxBsonObjectSize,
            maxBatchSizeBytes,
            maxWriteBatchSize,
            maxKeySize,
            // Namespace
            namespace,
            // Topology
            topology,
            // Options
            options: options,
            // BSON options
            bsonOptions: (0, bson_1.resolveBSONOptions)(options),
            // Current operation
            currentOp,
            // Executed
            executed,
            // Collection
            collection,
            // Fundamental error
            err: undefined,
            // check keys
            checkKeys: typeof options.checkKeys === 'boolean' ? options.checkKeys : false
        };
        // bypass Validation
        if (options.bypassDocumentValidation === true) {
            this.s.bypassDocumentValidation = true;
        }
    }
    /**
     * Add a single insert document to the bulk operation
     *
     * @example
     * ```ts
     * const bulkOp = collection.initializeOrderedBulkOp();
     *
     * // Adds three inserts to the bulkOp.
     * bulkOp
     *   .insert({ a: 1 })
     *   .insert({ b: 2 })
     *   .insert({ c: 3 });
     * await bulkOp.execute();
     * ```
     */ insert(document) {
        (0, utils_1.maybeAddIdToDocuments)(this.collection, document, {
            forceServerObjectId: this.shouldForceServerObjectId()
        });
        return this.addToOperationsList(exports.BatchType.INSERT, document);
    }
    /**
     * Builds a find operation for an update/updateOne/delete/deleteOne/replaceOne.
     * Returns a builder object used to complete the definition of the operation.
     *
     * @example
     * ```ts
     * const bulkOp = collection.initializeOrderedBulkOp();
     *
     * // Add an updateOne to the bulkOp
     * bulkOp.find({ a: 1 }).updateOne({ $set: { b: 2 } });
     *
     * // Add an updateMany to the bulkOp
     * bulkOp.find({ c: 3 }).update({ $set: { d: 4 } });
     *
     * // Add an upsert
     * bulkOp.find({ e: 5 }).upsert().updateOne({ $set: { f: 6 } });
     *
     * // Add a deletion
     * bulkOp.find({ g: 7 }).deleteOne();
     *
     * // Add a multi deletion
     * bulkOp.find({ h: 8 }).delete();
     *
     * // Add a replaceOne
     * bulkOp.find({ i: 9 }).replaceOne({writeConcern: { j: 10 }});
     *
     * // Update using a pipeline (requires Mongodb 4.2 or higher)
     * bulk.find({ k: 11, y: { $exists: true }, z: { $exists: true } }).updateOne([
     *   { $set: { total: { $sum: [ '$y', '$z' ] } } }
     * ]);
     *
     * // All of the ops will now be executed
     * await bulkOp.execute();
     * ```
     */ find(selector) {
        if (!selector) {
            throw new error_1.MongoInvalidArgumentError('Bulk find operation must specify a selector');
        }
        // Save a current selector
        this.s.currentOp = {
            selector: selector
        };
        return new FindOperators(this);
    }
    /** Specifies a raw operation to perform in the bulk write. */ raw(op) {
        if (op == null || typeof op !== 'object') {
            throw new error_1.MongoInvalidArgumentError('Operation must be an object with an operation key');
        }
        if ('insertOne' in op) {
            const forceServerObjectId = this.shouldForceServerObjectId();
            const document = op.insertOne && op.insertOne.document == null ? op.insertOne : op.insertOne.document;
            (0, utils_1.maybeAddIdToDocuments)(this.collection, document, {
                forceServerObjectId
            });
            return this.addToOperationsList(exports.BatchType.INSERT, document);
        }
        if ('replaceOne' in op || 'updateOne' in op || 'updateMany' in op) {
            if ('replaceOne' in op) {
                if ('q' in op.replaceOne) {
                    throw new error_1.MongoInvalidArgumentError('Raw operations are not allowed');
                }
                const updateStatement = (0, update_1.makeUpdateStatement)(op.replaceOne.filter, op.replaceOne.replacement, {
                    ...op.replaceOne,
                    multi: false
                });
                if ((0, utils_1.hasAtomicOperators)(updateStatement.u)) {
                    throw new error_1.MongoInvalidArgumentError('Replacement document must not use atomic operators');
                }
                return this.addToOperationsList(exports.BatchType.UPDATE, updateStatement);
            }
            if ('updateOne' in op) {
                if ('q' in op.updateOne) {
                    throw new error_1.MongoInvalidArgumentError('Raw operations are not allowed');
                }
                const updateStatement = (0, update_1.makeUpdateStatement)(op.updateOne.filter, op.updateOne.update, {
                    ...op.updateOne,
                    multi: false
                });
                if (!(0, utils_1.hasAtomicOperators)(updateStatement.u, this.bsonOptions)) {
                    throw new error_1.MongoInvalidArgumentError('Update document requires atomic operators');
                }
                return this.addToOperationsList(exports.BatchType.UPDATE, updateStatement);
            }
            if ('updateMany' in op) {
                if ('q' in op.updateMany) {
                    throw new error_1.MongoInvalidArgumentError('Raw operations are not allowed');
                }
                const updateStatement = (0, update_1.makeUpdateStatement)(op.updateMany.filter, op.updateMany.update, {
                    ...op.updateMany,
                    multi: true
                });
                if (!(0, utils_1.hasAtomicOperators)(updateStatement.u, this.bsonOptions)) {
                    throw new error_1.MongoInvalidArgumentError('Update document requires atomic operators');
                }
                return this.addToOperationsList(exports.BatchType.UPDATE, updateStatement);
            }
        }
        if ('deleteOne' in op) {
            if ('q' in op.deleteOne) {
                throw new error_1.MongoInvalidArgumentError('Raw operations are not allowed');
            }
            return this.addToOperationsList(exports.BatchType.DELETE, (0, delete_1.makeDeleteStatement)(op.deleteOne.filter, {
                ...op.deleteOne,
                limit: 1
            }));
        }
        if ('deleteMany' in op) {
            if ('q' in op.deleteMany) {
                throw new error_1.MongoInvalidArgumentError('Raw operations are not allowed');
            }
            return this.addToOperationsList(exports.BatchType.DELETE, (0, delete_1.makeDeleteStatement)(op.deleteMany.filter, {
                ...op.deleteMany,
                limit: 0
            }));
        }
        // otherwise an unknown operation was provided
        throw new error_1.MongoInvalidArgumentError('bulkWrite only supports insertOne, updateOne, updateMany, deleteOne, deleteMany');
    }
    get length() {
        return this.s.currentIndex;
    }
    get bsonOptions() {
        return this.s.bsonOptions;
    }
    get writeConcern() {
        return this.s.writeConcern;
    }
    get batches() {
        const batches = [
            ...this.s.batches
        ];
        if (this.isOrdered) {
            if (this.s.currentBatch) batches.push(this.s.currentBatch);
        } else {
            if (this.s.currentInsertBatch) batches.push(this.s.currentInsertBatch);
            if (this.s.currentUpdateBatch) batches.push(this.s.currentUpdateBatch);
            if (this.s.currentRemoveBatch) batches.push(this.s.currentRemoveBatch);
        }
        return batches;
    }
    async execute(options = {}) {
        if (this.s.executed) {
            throw new error_1.MongoBatchReExecutionError();
        }
        const writeConcern = write_concern_1.WriteConcern.fromOptions(options);
        if (writeConcern) {
            this.s.writeConcern = writeConcern;
        }
        // If we have current batch
        if (this.isOrdered) {
            if (this.s.currentBatch) this.s.batches.push(this.s.currentBatch);
        } else {
            if (this.s.currentInsertBatch) this.s.batches.push(this.s.currentInsertBatch);
            if (this.s.currentUpdateBatch) this.s.batches.push(this.s.currentUpdateBatch);
            if (this.s.currentRemoveBatch) this.s.batches.push(this.s.currentRemoveBatch);
        }
        // If we have no operations in the bulk raise an error
        if (this.s.batches.length === 0) {
            throw new error_1.MongoInvalidArgumentError('Invalid BulkOperation, Batch cannot be empty');
        }
        this.s.executed = true;
        const finalOptions = (0, utils_1.resolveOptions)(this.collection, {
            ...this.s.options,
            ...options
        });
        // if there is no timeoutContext provided, create a timeoutContext and use it for
        // all batches in the bulk operation
        finalOptions.timeoutContext ??= timeout_1.TimeoutContext.create({
            session: finalOptions.session,
            timeoutMS: finalOptions.timeoutMS,
            serverSelectionTimeoutMS: this.collection.client.s.options.serverSelectionTimeoutMS,
            waitQueueTimeoutMS: this.collection.client.s.options.waitQueueTimeoutMS
        });
        if (finalOptions.session == null) {
            // if there is not an explicit session provided to `execute()`, create
            // an implicit session and use that for all batches in the bulk operation
            return await this.collection.client.withSession({
                explicit: false
            }, async (session)=>{
                return await executeCommands(this, {
                    ...finalOptions,
                    session
                });
            });
        }
        return await executeCommands(this, {
            ...finalOptions
        });
    }
    /**
     * Handles the write error before executing commands
     * @internal
     */ handleWriteError(writeResult) {
        if (this.s.bulkResult.writeErrors.length > 0) {
            const msg = this.s.bulkResult.writeErrors[0].errmsg ? this.s.bulkResult.writeErrors[0].errmsg : 'write operation failed';
            throw new MongoBulkWriteError({
                message: msg,
                code: this.s.bulkResult.writeErrors[0].code,
                writeErrors: this.s.bulkResult.writeErrors
            }, writeResult);
        }
        const writeConcernError = writeResult.getWriteConcernError();
        if (writeConcernError) {
            throw new MongoBulkWriteError(writeConcernError, writeResult);
        }
    }
    shouldForceServerObjectId() {
        return this.s.options.forceServerObjectId === true || this.s.collection.db.options?.forceServerObjectId === true;
    }
}
exports.BulkOperationBase = BulkOperationBase;
function isInsertBatch(batch) {
    return batch.batchType === exports.BatchType.INSERT;
}
function isUpdateBatch(batch) {
    return batch.batchType === exports.BatchType.UPDATE;
}
function isDeleteBatch(batch) {
    return batch.batchType === exports.BatchType.DELETE;
}
function buildCurrentOp(bulkOp) {
    let { currentOp } = bulkOp.s;
    bulkOp.s.currentOp = undefined;
    if (!currentOp) currentOp = {};
    return currentOp;
} //# sourceMappingURL=common.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bulk/ordered.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.OrderedBulkOperation = void 0;
const BSON = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bson.js [app-client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
const common_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bulk/common.js [app-client] (ecmascript)");
/** @public */ class OrderedBulkOperation extends common_1.BulkOperationBase {
    /** @internal */ constructor(collection, options){
        super(collection, options, true);
    }
    addToOperationsList(batchType, document) {
        // Get the bsonSize
        const bsonSize = BSON.calculateObjectSize(document, {
            checkKeys: false,
            // Since we don't know what the user selected for BSON options here,
            // err on the safe side, and check the size with ignoreUndefined: false.
            ignoreUndefined: false
        });
        // Throw error if the doc is bigger than the max BSON size
        if (bsonSize >= this.s.maxBsonObjectSize) // TODO(NODE-3483): Change this to MongoBSONError
        throw new error_1.MongoInvalidArgumentError(`Document is larger than the maximum size ${this.s.maxBsonObjectSize}`);
        // Create a new batch object if we don't have a current one
        if (this.s.currentBatch == null) {
            this.s.currentBatch = new common_1.Batch(batchType, this.s.currentIndex);
        }
        const maxKeySize = this.s.maxKeySize;
        // Check if we need to create a new batch
        if (// New batch if we exceed the max batch op size
        this.s.currentBatchSize + 1 >= this.s.maxWriteBatchSize || this.s.currentBatchSize > 0 && this.s.currentBatchSizeBytes + maxKeySize + bsonSize >= this.s.maxBatchSizeBytes || // New batch if the new op does not have the same op type as the current batch
        this.s.currentBatch.batchType !== batchType) {
            // Save the batch to the execution stack
            this.s.batches.push(this.s.currentBatch);
            // Create a new batch
            this.s.currentBatch = new common_1.Batch(batchType, this.s.currentIndex);
            // Reset the current size trackers
            this.s.currentBatchSize = 0;
            this.s.currentBatchSizeBytes = 0;
        }
        if (batchType === common_1.BatchType.INSERT) {
            this.s.bulkResult.insertedIds.push({
                index: this.s.currentIndex,
                _id: document._id
            });
        }
        // We have an array of documents
        if (Array.isArray(document)) {
            throw new error_1.MongoInvalidArgumentError('Operation passed in cannot be an Array');
        }
        this.s.currentBatch.originalIndexes.push(this.s.currentIndex);
        this.s.currentBatch.operations.push(document);
        this.s.currentBatchSize += 1;
        this.s.currentBatchSizeBytes += maxKeySize + bsonSize;
        this.s.currentIndex += 1;
        return this;
    }
}
exports.OrderedBulkOperation = OrderedBulkOperation; //# sourceMappingURL=ordered.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bulk/unordered.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.UnorderedBulkOperation = void 0;
const BSON = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bson.js [app-client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
const common_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bulk/common.js [app-client] (ecmascript)");
/** @public */ class UnorderedBulkOperation extends common_1.BulkOperationBase {
    /** @internal */ constructor(collection, options){
        super(collection, options, false);
    }
    handleWriteError(writeResult) {
        if (this.s.batches.length) {
            return;
        }
        return super.handleWriteError(writeResult);
    }
    addToOperationsList(batchType, document) {
        // Get the bsonSize
        const bsonSize = BSON.calculateObjectSize(document, {
            checkKeys: false,
            // Since we don't know what the user selected for BSON options here,
            // err on the safe side, and check the size with ignoreUndefined: false.
            ignoreUndefined: false
        });
        // Throw error if the doc is bigger than the max BSON size
        if (bsonSize >= this.s.maxBsonObjectSize) {
            // TODO(NODE-3483): Change this to MongoBSONError
            throw new error_1.MongoInvalidArgumentError(`Document is larger than the maximum size ${this.s.maxBsonObjectSize}`);
        }
        // Holds the current batch
        this.s.currentBatch = undefined;
        // Get the right type of batch
        if (batchType === common_1.BatchType.INSERT) {
            this.s.currentBatch = this.s.currentInsertBatch;
        } else if (batchType === common_1.BatchType.UPDATE) {
            this.s.currentBatch = this.s.currentUpdateBatch;
        } else if (batchType === common_1.BatchType.DELETE) {
            this.s.currentBatch = this.s.currentRemoveBatch;
        }
        const maxKeySize = this.s.maxKeySize;
        // Create a new batch object if we don't have a current one
        if (this.s.currentBatch == null) {
            this.s.currentBatch = new common_1.Batch(batchType, this.s.currentIndex);
        }
        // Check if we need to create a new batch
        if (// New batch if we exceed the max batch op size
        this.s.currentBatch.size + 1 >= this.s.maxWriteBatchSize || this.s.currentBatch.size > 0 && this.s.currentBatch.sizeBytes + maxKeySize + bsonSize >= this.s.maxBatchSizeBytes || // New batch if the new op does not have the same op type as the current batch
        this.s.currentBatch.batchType !== batchType) {
            // Save the batch to the execution stack
            this.s.batches.push(this.s.currentBatch);
            // Create a new batch
            this.s.currentBatch = new common_1.Batch(batchType, this.s.currentIndex);
        }
        // We have an array of documents
        if (Array.isArray(document)) {
            throw new error_1.MongoInvalidArgumentError('Operation passed in cannot be an Array');
        }
        this.s.currentBatch.operations.push(document);
        this.s.currentBatch.originalIndexes.push(this.s.currentIndex);
        this.s.currentIndex = this.s.currentIndex + 1;
        // Save back the current Batch to the right type
        if (batchType === common_1.BatchType.INSERT) {
            this.s.currentInsertBatch = this.s.currentBatch;
            this.s.bulkResult.insertedIds.push({
                index: this.s.bulkResult.insertedIds.length,
                _id: document._id
            });
        } else if (batchType === common_1.BatchType.UPDATE) {
            this.s.currentUpdateBatch = this.s.currentBatch;
        } else if (batchType === common_1.BatchType.DELETE) {
            this.s.currentRemoveBatch = this.s.currentBatch;
        }
        // Update current batch size
        this.s.currentBatch.size += 1;
        this.s.currentBatch.sizeBytes += maxKeySize + bsonSize;
        return this;
    }
}
exports.UnorderedBulkOperation = UnorderedBulkOperation; //# sourceMappingURL=unordered.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/admin.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.Admin = void 0;
const bson_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bson.js [app-client] (ecmascript)");
const execute_operation_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/execute_operation.js [app-client] (ecmascript)");
const list_databases_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/list_databases.js [app-client] (ecmascript)");
const remove_user_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/remove_user.js [app-client] (ecmascript)");
const run_command_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/run_command.js [app-client] (ecmascript)");
const validate_collection_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/validate_collection.js [app-client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/utils.js [app-client] (ecmascript)");
/**
 * The **Admin** class is an internal class that allows convenient access to
 * the admin functionality and commands for MongoDB.
 *
 * **ADMIN Cannot directly be instantiated**
 * @public
 *
 * @example
 * ```ts
 * import { MongoClient } from 'mongodb';
 *
 * const client = new MongoClient('mongodb://localhost:27017');
 * const admin = client.db().admin();
 * const dbInfo = await admin.listDatabases();
 * for (const db of dbInfo.databases) {
 *   console.log(db.name);
 * }
 * ```
 */ class Admin {
    /**
     * Create a new Admin instance
     * @internal
     */ constructor(db){
        this.s = {
            db
        };
    }
    /**
     * Execute a command
     *
     * The driver will ensure the following fields are attached to the command sent to the server:
     * - `lsid` - sourced from an implicit session or options.session
     * - `$readPreference` - defaults to primary or can be configured by options.readPreference
     * - `$db` - sourced from the name of this database
     *
     * If the client has a serverApi setting:
     * - `apiVersion`
     * - `apiStrict`
     * - `apiDeprecationErrors`
     *
     * When in a transaction:
     * - `readConcern` - sourced from readConcern set on the TransactionOptions
     * - `writeConcern` - sourced from writeConcern set on the TransactionOptions
     *
     * Attaching any of the above fields to the command will have no effect as the driver will overwrite the value.
     *
     * @param command - The command to execute
     * @param options - Optional settings for the command
     */ async command(command, options) {
        return await (0, execute_operation_1.executeOperation)(this.s.db.client, new run_command_1.RunCommandOperation(new utils_1.MongoDBNamespace('admin'), command, {
            ...(0, bson_1.resolveBSONOptions)(options),
            session: options?.session,
            readPreference: options?.readPreference,
            timeoutMS: options?.timeoutMS ?? this.s.db.timeoutMS
        }));
    }
    /**
     * Retrieve the server build information
     *
     * @param options - Optional settings for the command
     */ async buildInfo(options) {
        return await this.command({
            buildinfo: 1
        }, options);
    }
    /**
     * Retrieve the server build information
     *
     * @param options - Optional settings for the command
     */ async serverInfo(options) {
        return await this.command({
            buildinfo: 1
        }, options);
    }
    /**
     * Retrieve this db's server status.
     *
     * @param options - Optional settings for the command
     */ async serverStatus(options) {
        return await this.command({
            serverStatus: 1
        }, options);
    }
    /**
     * Ping the MongoDB server and retrieve results
     *
     * @param options - Optional settings for the command
     */ async ping(options) {
        return await this.command({
            ping: 1
        }, options);
    }
    /**
     * Remove a user from a database
     *
     * @param username - The username to remove
     * @param options - Optional settings for the command
     */ async removeUser(username, options) {
        return await (0, execute_operation_1.executeOperation)(this.s.db.client, new remove_user_1.RemoveUserOperation(this.s.db, username, {
            dbName: 'admin',
            ...options
        }));
    }
    /**
     * Validate an existing collection
     *
     * @param collectionName - The name of the collection to validate.
     * @param options - Optional settings for the command
     */ async validateCollection(collectionName, options = {}) {
        return await (0, execute_operation_1.executeOperation)(this.s.db.client, new validate_collection_1.ValidateCollectionOperation(this, collectionName, options));
    }
    /**
     * List the available databases
     *
     * @param options - Optional settings for the command
     */ async listDatabases(options) {
        return await (0, execute_operation_1.executeOperation)(this.s.db.client, new list_databases_1.ListDatabasesOperation(this.s.db, {
            timeoutMS: this.s.db.timeoutMS,
            ...options
        }));
    }
    /**
     * Get ReplicaSet status
     *
     * @param options - Optional settings for the command
     */ async replSetGetStatus(options) {
        return await this.command({
            replSetGetStatus: 1
        }, options);
    }
}
exports.Admin = Admin; //# sourceMappingURL=admin.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/mongo_logger.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/Pathology/patho-client/node_modules/.pnpm/next@16.0.6_@babel+core@7.28.5_react-dom@19.2.0_react@19.2.0__react@19.2.0/node_modules/next/dist/build/polyfills/process.js [app-client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.MongoLogger = exports.MongoLoggableComponent = exports.SEVERITY_LEVEL_MAP = exports.DEFAULT_MAX_DOCUMENT_LENGTH = exports.SeverityLevel = void 0;
exports.parseSeverityFromString = parseSeverityFromString;
exports.createStdioLogger = createStdioLogger;
exports.stringifyWithMaxLen = stringifyWithMaxLen;
exports.defaultLogTransform = defaultLogTransform;
const util_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/next@16.0.6_@babel+core@7.28.5_react-dom@19.2.0_react@19.2.0__react@19.2.0/node_modules/next/dist/compiled/util/util.js [app-client] (ecmascript)");
const bson_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bson.js [app-client] (ecmascript)");
const constants_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/constants.js [app-client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/utils.js [app-client] (ecmascript)");
/**
 * @public
 * Severity levels align with unix syslog.
 * Most typical driver functions will log to debug.
 */ exports.SeverityLevel = Object.freeze({
    EMERGENCY: 'emergency',
    ALERT: 'alert',
    CRITICAL: 'critical',
    ERROR: 'error',
    WARNING: 'warn',
    NOTICE: 'notice',
    INFORMATIONAL: 'info',
    DEBUG: 'debug',
    TRACE: 'trace',
    OFF: 'off'
});
/** @internal */ exports.DEFAULT_MAX_DOCUMENT_LENGTH = 1000;
/** @internal */ class SeverityLevelMap extends Map {
    constructor(entries){
        const newEntries = [];
        for (const [level, value] of entries){
            newEntries.push([
                value,
                level
            ]);
        }
        newEntries.push(...entries);
        super(newEntries);
    }
    getNumericSeverityLevel(severity) {
        return this.get(severity);
    }
    getSeverityLevelName(level) {
        return this.get(level);
    }
}
/** @internal */ exports.SEVERITY_LEVEL_MAP = new SeverityLevelMap([
    [
        exports.SeverityLevel.OFF,
        -Infinity
    ],
    [
        exports.SeverityLevel.EMERGENCY,
        0
    ],
    [
        exports.SeverityLevel.ALERT,
        1
    ],
    [
        exports.SeverityLevel.CRITICAL,
        2
    ],
    [
        exports.SeverityLevel.ERROR,
        3
    ],
    [
        exports.SeverityLevel.WARNING,
        4
    ],
    [
        exports.SeverityLevel.NOTICE,
        5
    ],
    [
        exports.SeverityLevel.INFORMATIONAL,
        6
    ],
    [
        exports.SeverityLevel.DEBUG,
        7
    ],
    [
        exports.SeverityLevel.TRACE,
        8
    ]
]);
/** @public */ exports.MongoLoggableComponent = Object.freeze({
    COMMAND: 'command',
    TOPOLOGY: 'topology',
    SERVER_SELECTION: 'serverSelection',
    CONNECTION: 'connection',
    CLIENT: 'client'
});
/**
 * Parses a string as one of SeverityLevel
 * @internal
 *
 * @param s - the value to be parsed
 * @returns one of SeverityLevel if value can be parsed as such, otherwise null
 */ function parseSeverityFromString(s) {
    const validSeverities = Object.values(exports.SeverityLevel);
    const lowerSeverity = s?.toLowerCase();
    if (lowerSeverity != null && validSeverities.includes(lowerSeverity)) {
        return lowerSeverity;
    }
    return null;
}
/** @internal */ function createStdioLogger(stream) {
    return {
        write: (0, util_1.promisify)((log, cb)=>{
            const logLine = (0, util_1.inspect)(log, {
                compact: true,
                breakLength: Infinity
            });
            stream.write(`${logLine}\n`, 'utf-8', cb);
            return;
        })
    };
}
/**
 * resolves the MONGODB_LOG_PATH and mongodbLogPath options from the environment and the
 * mongo client options respectively. The mongodbLogPath can be either 'stdout', 'stderr', a NodeJS
 * Writable or an object which has a `write` method with the signature:
 * ```ts
 * write(log: Log): void
 * ```
 *
 * @returns the MongoDBLogWritable object to write logs to
 */ function resolveLogPath({ MONGODB_LOG_PATH }, { mongodbLogPath }) {
    if (typeof mongodbLogPath === 'string' && /^stderr$/i.test(mongodbLogPath)) {
        return {
            mongodbLogPath: createStdioLogger(__TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].stderr),
            mongodbLogPathIsStdErr: true
        };
    }
    if (typeof mongodbLogPath === 'string' && /^stdout$/i.test(mongodbLogPath)) {
        return {
            mongodbLogPath: createStdioLogger(__TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].stdout),
            mongodbLogPathIsStdErr: false
        };
    }
    if (typeof mongodbLogPath === 'object' && typeof mongodbLogPath?.write === 'function') {
        return {
            mongodbLogPath: mongodbLogPath,
            mongodbLogPathIsStdErr: false
        };
    }
    if (MONGODB_LOG_PATH && /^stderr$/i.test(MONGODB_LOG_PATH)) {
        return {
            mongodbLogPath: createStdioLogger(__TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].stderr),
            mongodbLogPathIsStdErr: true
        };
    }
    if (MONGODB_LOG_PATH && /^stdout$/i.test(MONGODB_LOG_PATH)) {
        return {
            mongodbLogPath: createStdioLogger(__TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].stdout),
            mongodbLogPathIsStdErr: false
        };
    }
    return {
        mongodbLogPath: createStdioLogger(__TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].stderr),
        mongodbLogPathIsStdErr: true
    };
}
function resolveSeverityConfiguration(clientOption, environmentOption, defaultSeverity) {
    return parseSeverityFromString(clientOption) ?? parseSeverityFromString(environmentOption) ?? defaultSeverity;
}
function compareSeverity(s0, s1) {
    const s0Num = exports.SEVERITY_LEVEL_MAP.getNumericSeverityLevel(s0);
    const s1Num = exports.SEVERITY_LEVEL_MAP.getNumericSeverityLevel(s1);
    return s0Num < s1Num ? -1 : s0Num > s1Num ? 1 : 0;
}
/** @internal */ function stringifyWithMaxLen(value, maxDocumentLength, options = {}) {
    let strToTruncate = '';
    let currentLength = 0;
    const maxDocumentLengthEnsurer = function maxDocumentLengthEnsurer(key, value) {
        if (currentLength >= maxDocumentLength) {
            return undefined;
        }
        // Account for root document
        if (key === '') {
            // Account for starting brace
            currentLength += 1;
            return value;
        }
        // +4 accounts for 2 quotation marks, colon and comma after value
        // Note that this potentially undercounts since it does not account for escape sequences which
        // will have an additional backslash added to them once passed through JSON.stringify.
        currentLength += key.length + 4;
        if (value == null) return value;
        switch(typeof value){
            case 'string':
                // +2 accounts for quotes
                // Note that this potentially undercounts similarly to the key length calculation
                currentLength += value.length + 2;
                break;
            case 'number':
            case 'bigint':
                currentLength += String(value).length;
                break;
            case 'boolean':
                currentLength += value ? 4 : 5;
                break;
            case 'object':
                if ((0, utils_1.isUint8Array)(value)) {
                    // '{"$binary":{"base64":"<base64 string>","subType":"XX"}}'
                    // This is an estimate based on the fact that the base64 is approximately 1.33x the length of
                    // the actual binary sequence https://en.wikipedia.org/wiki/Base64
                    currentLength += 22 + value.byteLength + value.byteLength * 0.33 + 18 | 0;
                } else if ('_bsontype' in value) {
                    const v = value;
                    switch(v._bsontype){
                        case 'Int32':
                            currentLength += String(v.value).length;
                            break;
                        case 'Double':
                            // Account for representing integers as <value>.0
                            currentLength += (v.value | 0) === v.value ? String(v.value).length + 2 : String(v.value).length;
                            break;
                        case 'Long':
                            currentLength += v.toString().length;
                            break;
                        case 'ObjectId':
                            // '{"$oid":"XXXXXXXXXXXXXXXXXXXXXXXX"}'
                            currentLength += 35;
                            break;
                        case 'MaxKey':
                        case 'MinKey':
                            // '{"$maxKey":1}' or '{"$minKey":1}'
                            currentLength += 13;
                            break;
                        case 'Binary':
                            // '{"$binary":{"base64":"<base64 string>","subType":"XX"}}'
                            // This is an estimate based on the fact that the base64 is approximately 1.33x the length of
                            // the actual binary sequence https://en.wikipedia.org/wiki/Base64
                            currentLength += 22 + value.position + value.position * 0.33 + 18 | 0;
                            break;
                        case 'Timestamp':
                            // '{"$timestamp":{"t":<t>,"i":<i>}}'
                            currentLength += 19 + String(v.t).length + 5 + String(v.i).length + 2;
                            break;
                        case 'Code':
                            // '{"$code":"<code>"}' or '{"$code":"<code>","$scope":<scope>}'
                            if (v.scope == null) {
                                currentLength += v.code.length + 10 + 2;
                            } else {
                                // Ignoring actual scope object, so this undercounts by a significant amount
                                currentLength += v.code.length + 10 + 11;
                            }
                            break;
                        case 'BSONRegExp':
                            // '{"$regularExpression":{"pattern":"<pattern>","options":"<options>"}}'
                            currentLength += 34 + v.pattern.length + 13 + v.options.length + 3;
                            break;
                    }
                }
        }
        return value;
    };
    if (typeof value === 'string') {
        strToTruncate = value;
    } else if (typeof value === 'function') {
        strToTruncate = value.name;
    } else {
        try {
            if (maxDocumentLength !== 0) {
                strToTruncate = bson_1.EJSON.stringify(value, maxDocumentLengthEnsurer, 0, options);
            } else {
                strToTruncate = bson_1.EJSON.stringify(value, options);
            }
        } catch (e) {
            strToTruncate = `Extended JSON serialization failed with: ${e.message}`;
        }
    }
    // handle truncation that occurs in the middle of multi-byte codepoints
    if (maxDocumentLength !== 0 && strToTruncate.length > maxDocumentLength && strToTruncate.charCodeAt(maxDocumentLength - 1) !== strToTruncate.codePointAt(maxDocumentLength - 1)) {
        maxDocumentLength--;
        if (maxDocumentLength === 0) {
            return '';
        }
    }
    return maxDocumentLength !== 0 && strToTruncate.length > maxDocumentLength ? `${strToTruncate.slice(0, maxDocumentLength)}...` : strToTruncate;
}
function isLogConvertible(obj) {
    const objAsLogConvertible = obj;
    // eslint-disable-next-line no-restricted-syntax
    return objAsLogConvertible.toLog !== undefined && typeof objAsLogConvertible.toLog === 'function';
}
function attachServerSelectionFields(log, serverSelectionEvent, maxDocumentLength = exports.DEFAULT_MAX_DOCUMENT_LENGTH) {
    const { selector, operation, topologyDescription, message } = serverSelectionEvent;
    log.selector = stringifyWithMaxLen(selector, maxDocumentLength);
    log.operation = operation;
    log.topologyDescription = stringifyWithMaxLen(topologyDescription, maxDocumentLength);
    log.message = message;
    return log;
}
function attachCommandFields(log, commandEvent) {
    log.commandName = commandEvent.commandName;
    log.requestId = commandEvent.requestId;
    log.driverConnectionId = commandEvent.connectionId;
    const { host, port } = utils_1.HostAddress.fromString(commandEvent.address).toHostPort();
    log.serverHost = host;
    log.serverPort = port;
    if (commandEvent?.serviceId) {
        log.serviceId = commandEvent.serviceId.toHexString();
    }
    log.databaseName = commandEvent.databaseName;
    log.serverConnectionId = commandEvent.serverConnectionId;
    return log;
}
function attachConnectionFields(log, event) {
    const { host, port } = utils_1.HostAddress.fromString(event.address).toHostPort();
    log.serverHost = host;
    log.serverPort = port;
    return log;
}
function attachSDAMFields(log, sdamEvent) {
    log.topologyId = sdamEvent.topologyId;
    return log;
}
function attachServerHeartbeatFields(log, serverHeartbeatEvent) {
    const { awaited, connectionId } = serverHeartbeatEvent;
    log.awaited = awaited;
    log.driverConnectionId = serverHeartbeatEvent.connectionId;
    const { host, port } = utils_1.HostAddress.fromString(connectionId).toHostPort();
    log.serverHost = host;
    log.serverPort = port;
    return log;
}
/** @internal */ function defaultLogTransform(logObject, maxDocumentLength = exports.DEFAULT_MAX_DOCUMENT_LENGTH) {
    let log = Object.create(null);
    switch(logObject.name){
        case constants_1.SERVER_SELECTION_STARTED:
            log = attachServerSelectionFields(log, logObject, maxDocumentLength);
            return log;
        case constants_1.SERVER_SELECTION_FAILED:
            log = attachServerSelectionFields(log, logObject, maxDocumentLength);
            log.failure = logObject.failure?.message;
            return log;
        case constants_1.SERVER_SELECTION_SUCCEEDED:
            log = attachServerSelectionFields(log, logObject, maxDocumentLength);
            log.serverHost = logObject.serverHost;
            log.serverPort = logObject.serverPort;
            return log;
        case constants_1.WAITING_FOR_SUITABLE_SERVER:
            log = attachServerSelectionFields(log, logObject, maxDocumentLength);
            log.remainingTimeMS = logObject.remainingTimeMS;
            return log;
        case constants_1.COMMAND_STARTED:
            log = attachCommandFields(log, logObject);
            log.message = 'Command started';
            log.command = stringifyWithMaxLen(logObject.command, maxDocumentLength, {
                relaxed: true
            });
            log.databaseName = logObject.databaseName;
            return log;
        case constants_1.COMMAND_SUCCEEDED:
            log = attachCommandFields(log, logObject);
            log.message = 'Command succeeded';
            log.durationMS = logObject.duration;
            log.reply = stringifyWithMaxLen(logObject.reply, maxDocumentLength, {
                relaxed: true
            });
            return log;
        case constants_1.COMMAND_FAILED:
            log = attachCommandFields(log, logObject);
            log.message = 'Command failed';
            log.durationMS = logObject.duration;
            log.failure = logObject.failure?.message ?? '(redacted)';
            return log;
        case constants_1.CONNECTION_POOL_CREATED:
            log = attachConnectionFields(log, logObject);
            log.message = 'Connection pool created';
            if (logObject.options) {
                const { maxIdleTimeMS, minPoolSize, maxPoolSize, maxConnecting, waitQueueTimeoutMS } = logObject.options;
                log = {
                    ...log,
                    maxIdleTimeMS,
                    minPoolSize,
                    maxPoolSize,
                    maxConnecting,
                    waitQueueTimeoutMS
                };
            }
            return log;
        case constants_1.CONNECTION_POOL_READY:
            log = attachConnectionFields(log, logObject);
            log.message = 'Connection pool ready';
            return log;
        case constants_1.CONNECTION_POOL_CLEARED:
            log = attachConnectionFields(log, logObject);
            log.message = 'Connection pool cleared';
            if (logObject.serviceId?._bsontype === 'ObjectId') {
                log.serviceId = logObject.serviceId?.toHexString();
            }
            return log;
        case constants_1.CONNECTION_POOL_CLOSED:
            log = attachConnectionFields(log, logObject);
            log.message = 'Connection pool closed';
            return log;
        case constants_1.CONNECTION_CREATED:
            log = attachConnectionFields(log, logObject);
            log.message = 'Connection created';
            log.driverConnectionId = logObject.connectionId;
            return log;
        case constants_1.CONNECTION_READY:
            log = attachConnectionFields(log, logObject);
            log.message = 'Connection ready';
            log.driverConnectionId = logObject.connectionId;
            log.durationMS = logObject.durationMS;
            return log;
        case constants_1.CONNECTION_CLOSED:
            log = attachConnectionFields(log, logObject);
            log.message = 'Connection closed';
            log.driverConnectionId = logObject.connectionId;
            switch(logObject.reason){
                case 'stale':
                    log.reason = 'Connection became stale because the pool was cleared';
                    break;
                case 'idle':
                    log.reason = 'Connection has been available but unused for longer than the configured max idle time';
                    break;
                case 'error':
                    log.reason = 'An error occurred while using the connection';
                    if (logObject.error) {
                        log.error = logObject.error;
                    }
                    break;
                case 'poolClosed':
                    log.reason = 'Connection pool was closed';
                    break;
                default:
                    log.reason = `Unknown close reason: ${logObject.reason}`;
            }
            return log;
        case constants_1.CONNECTION_CHECK_OUT_STARTED:
            log = attachConnectionFields(log, logObject);
            log.message = 'Connection checkout started';
            return log;
        case constants_1.CONNECTION_CHECK_OUT_FAILED:
            log = attachConnectionFields(log, logObject);
            log.message = 'Connection checkout failed';
            switch(logObject.reason){
                case 'poolClosed':
                    log.reason = 'Connection pool was closed';
                    break;
                case 'timeout':
                    log.reason = 'Wait queue timeout elapsed without a connection becoming available';
                    break;
                case 'connectionError':
                    log.reason = 'An error occurred while trying to establish a new connection';
                    if (logObject.error) {
                        log.error = logObject.error;
                    }
                    break;
                default:
                    log.reason = `Unknown close reason: ${logObject.reason}`;
            }
            log.durationMS = logObject.durationMS;
            return log;
        case constants_1.CONNECTION_CHECKED_OUT:
            log = attachConnectionFields(log, logObject);
            log.message = 'Connection checked out';
            log.driverConnectionId = logObject.connectionId;
            log.durationMS = logObject.durationMS;
            return log;
        case constants_1.CONNECTION_CHECKED_IN:
            log = attachConnectionFields(log, logObject);
            log.message = 'Connection checked in';
            log.driverConnectionId = logObject.connectionId;
            return log;
        case constants_1.SERVER_OPENING:
            log = attachSDAMFields(log, logObject);
            log = attachConnectionFields(log, logObject);
            log.message = 'Starting server monitoring';
            return log;
        case constants_1.SERVER_CLOSED:
            log = attachSDAMFields(log, logObject);
            log = attachConnectionFields(log, logObject);
            log.message = 'Stopped server monitoring';
            return log;
        case constants_1.SERVER_HEARTBEAT_STARTED:
            log = attachSDAMFields(log, logObject);
            log = attachServerHeartbeatFields(log, logObject);
            log.message = 'Server heartbeat started';
            return log;
        case constants_1.SERVER_HEARTBEAT_SUCCEEDED:
            log = attachSDAMFields(log, logObject);
            log = attachServerHeartbeatFields(log, logObject);
            log.message = 'Server heartbeat succeeded';
            log.durationMS = logObject.duration;
            log.serverConnectionId = logObject.serverConnectionId;
            log.reply = stringifyWithMaxLen(logObject.reply, maxDocumentLength, {
                relaxed: true
            });
            return log;
        case constants_1.SERVER_HEARTBEAT_FAILED:
            log = attachSDAMFields(log, logObject);
            log = attachServerHeartbeatFields(log, logObject);
            log.message = 'Server heartbeat failed';
            log.durationMS = logObject.duration;
            log.failure = logObject.failure?.message;
            return log;
        case constants_1.TOPOLOGY_OPENING:
            log = attachSDAMFields(log, logObject);
            log.message = 'Starting topology monitoring';
            return log;
        case constants_1.TOPOLOGY_CLOSED:
            log = attachSDAMFields(log, logObject);
            log.message = 'Stopped topology monitoring';
            return log;
        case constants_1.TOPOLOGY_DESCRIPTION_CHANGED:
            log = attachSDAMFields(log, logObject);
            log.message = 'Topology description changed';
            log.previousDescription = log.reply = stringifyWithMaxLen(logObject.previousDescription, maxDocumentLength);
            log.newDescription = log.reply = stringifyWithMaxLen(logObject.newDescription, maxDocumentLength);
            return log;
        default:
            for (const [key, value] of Object.entries(logObject)){
                if (value != null) log[key] = value;
            }
    }
    return log;
}
/** @internal */ class MongoLogger {
    constructor(options){
        this.pendingLog = null;
        /**
         * This method should be used when logging errors that do not have a public driver API for
         * reporting errors.
         */ this.error = this.log.bind(this, 'error');
        /**
         * This method should be used to log situations where undesirable application behaviour might
         * occur. For example, failing to end sessions on `MongoClient.close`.
         */ this.warn = this.log.bind(this, 'warn');
        /**
         * This method should be used to report high-level information about normal driver behaviour.
         * For example, the creation of a `MongoClient`.
         */ this.info = this.log.bind(this, 'info');
        /**
         * This method should be used to report information that would be helpful when debugging an
         * application. For example, a command starting, succeeding or failing.
         */ this.debug = this.log.bind(this, 'debug');
        /**
         * This method should be used to report fine-grained details related to logic flow. For example,
         * entering and exiting a function body.
         */ this.trace = this.log.bind(this, 'trace');
        this.componentSeverities = options.componentSeverities;
        this.maxDocumentLength = options.maxDocumentLength;
        this.logDestination = options.logDestination;
        this.logDestinationIsStdErr = options.logDestinationIsStdErr;
        this.severities = this.createLoggingSeverities();
    }
    createLoggingSeverities() {
        const severities = Object();
        for (const component of Object.values(exports.MongoLoggableComponent)){
            severities[component] = {};
            for (const severityLevel of Object.values(exports.SeverityLevel)){
                severities[component][severityLevel] = compareSeverity(severityLevel, this.componentSeverities[component]) <= 0;
            }
        }
        return severities;
    }
    turnOffSeverities() {
        for (const component of Object.values(exports.MongoLoggableComponent)){
            this.componentSeverities[component] = exports.SeverityLevel.OFF;
            for (const severityLevel of Object.values(exports.SeverityLevel)){
                this.severities[component][severityLevel] = false;
            }
        }
    }
    logWriteFailureHandler(error) {
        if (this.logDestinationIsStdErr) {
            this.turnOffSeverities();
            this.clearPendingLog();
            return;
        }
        this.logDestination = createStdioLogger(__TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].stderr);
        this.logDestinationIsStdErr = true;
        this.clearPendingLog();
        this.error(exports.MongoLoggableComponent.CLIENT, {
            toLog: function() {
                return {
                    message: 'User input for mongodbLogPath is now invalid. Logging is halted.',
                    error: error.message
                };
            }
        });
        this.turnOffSeverities();
        this.clearPendingLog();
    }
    clearPendingLog() {
        this.pendingLog = null;
    }
    willLog(component, severity) {
        if (severity === exports.SeverityLevel.OFF) return false;
        return this.severities[component][severity];
    }
    log(severity, component, message) {
        if (!this.willLog(component, severity)) return;
        let logMessage = {
            t: new Date(),
            c: component,
            s: severity
        };
        if (typeof message === 'string') {
            logMessage.message = message;
        } else if (typeof message === 'object') {
            if (isLogConvertible(message)) {
                logMessage = {
                    ...logMessage,
                    ...message.toLog()
                };
            } else {
                logMessage = {
                    ...logMessage,
                    ...defaultLogTransform(message, this.maxDocumentLength)
                };
            }
        }
        if ((0, utils_1.isPromiseLike)(this.pendingLog)) {
            this.pendingLog = this.pendingLog.then(()=>this.logDestination.write(logMessage)).then(this.clearPendingLog.bind(this), this.logWriteFailureHandler.bind(this));
            return;
        }
        try {
            const logResult = this.logDestination.write(logMessage);
            if ((0, utils_1.isPromiseLike)(logResult)) {
                this.pendingLog = logResult.then(this.clearPendingLog.bind(this), this.logWriteFailureHandler.bind(this));
            }
        } catch (error) {
            this.logWriteFailureHandler(error);
        }
    }
    /**
     * Merges options set through environment variables and the MongoClient, preferring environment
     * variables when both are set, and substituting defaults for values not set. Options set in
     * constructor take precedence over both environment variables and MongoClient options.
     *
     * @remarks
     * When parsing component severity levels, invalid values are treated as unset and replaced with
     * the default severity.
     *
     * @param envOptions - options set for the logger from the environment
     * @param clientOptions - options set for the logger in the MongoClient options
     * @returns a MongoLoggerOptions object to be used when instantiating a new MongoLogger
     */ static resolveOptions(envOptions, clientOptions) {
        // client options take precedence over env options
        const resolvedLogPath = resolveLogPath(envOptions, clientOptions);
        const combinedOptions = {
            ...envOptions,
            ...clientOptions,
            mongodbLogPath: resolvedLogPath.mongodbLogPath,
            mongodbLogPathIsStdErr: resolvedLogPath.mongodbLogPathIsStdErr
        };
        const defaultSeverity = resolveSeverityConfiguration(combinedOptions.mongodbLogComponentSeverities?.default, combinedOptions.MONGODB_LOG_ALL, exports.SeverityLevel.OFF);
        return {
            componentSeverities: {
                command: resolveSeverityConfiguration(combinedOptions.mongodbLogComponentSeverities?.command, combinedOptions.MONGODB_LOG_COMMAND, defaultSeverity),
                topology: resolveSeverityConfiguration(combinedOptions.mongodbLogComponentSeverities?.topology, combinedOptions.MONGODB_LOG_TOPOLOGY, defaultSeverity),
                serverSelection: resolveSeverityConfiguration(combinedOptions.mongodbLogComponentSeverities?.serverSelection, combinedOptions.MONGODB_LOG_SERVER_SELECTION, defaultSeverity),
                connection: resolveSeverityConfiguration(combinedOptions.mongodbLogComponentSeverities?.connection, combinedOptions.MONGODB_LOG_CONNECTION, defaultSeverity),
                client: resolveSeverityConfiguration(combinedOptions.mongodbLogComponentSeverities?.client, combinedOptions.MONGODB_LOG_CLIENT, defaultSeverity),
                default: defaultSeverity
            },
            maxDocumentLength: combinedOptions.mongodbLogMaxDocumentLength ?? (0, utils_1.parseUnsignedInteger)(combinedOptions.MONGODB_LOG_MAX_DOCUMENT_LENGTH) ?? 1000,
            logDestination: combinedOptions.mongodbLogPath,
            logDestinationIsStdErr: combinedOptions.mongodbLogPathIsStdErr
        };
    }
}
exports.MongoLogger = MongoLogger; //# sourceMappingURL=mongo_logger.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/mongo_types.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.CancellationToken = exports.TypedEventEmitter = void 0;
const events_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/next@16.0.6_@babel+core@7.28.5_react-dom@19.2.0_react@19.2.0__react@19.2.0/node_modules/next/dist/compiled/events/events.js [app-client] (ecmascript)");
const mongo_logger_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/mongo_logger.js [app-client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/utils.js [app-client] (ecmascript)");
/**
 * Typescript type safe event emitter
 * @public
 */ // eslint-disable-next-line @typescript-eslint/no-unsafe-declaration-merging
class TypedEventEmitter extends events_1.EventEmitter {
    /** @internal */ emitAndLog(event, ...args) {
        this.emit(event, ...args);
        if (this.component) this.mongoLogger?.debug(this.component, args[0]);
    }
    /** @internal */ emitAndLogHeartbeat(event, topologyId, serverConnectionId, ...args) {
        this.emit(event, ...args);
        if (this.component) {
            const loggableHeartbeatEvent = {
                topologyId: topologyId,
                serverConnectionId: serverConnectionId ?? null,
                ...args[0]
            };
            this.mongoLogger?.debug(this.component, loggableHeartbeatEvent);
        }
    }
    /** @internal */ emitAndLogCommand(monitorCommands, event, databaseName, connectionEstablished, ...args) {
        if (monitorCommands) {
            this.emit(event, ...args);
        }
        if (connectionEstablished) {
            const loggableCommandEvent = {
                databaseName: databaseName,
                ...args[0]
            };
            this.mongoLogger?.debug(mongo_logger_1.MongoLoggableComponent.COMMAND, loggableCommandEvent);
        }
    }
}
exports.TypedEventEmitter = TypedEventEmitter;
/**
 * @internal
 */ class CancellationToken extends TypedEventEmitter {
    constructor(...args){
        super(...args);
        this.on('error', utils_1.noop);
    }
}
exports.CancellationToken = CancellationToken; //# sourceMappingURL=mongo_types.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/transactions.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.Transaction = exports.TxnState = void 0;
exports.isTransactionCommand = isTransactionCommand;
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
const read_concern_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/read_concern.js [app-client] (ecmascript)");
const read_preference_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/read_preference.js [app-client] (ecmascript)");
const write_concern_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/write_concern.js [app-client] (ecmascript)");
/** @internal */ exports.TxnState = Object.freeze({
    NO_TRANSACTION: 'NO_TRANSACTION',
    STARTING_TRANSACTION: 'STARTING_TRANSACTION',
    TRANSACTION_IN_PROGRESS: 'TRANSACTION_IN_PROGRESS',
    TRANSACTION_COMMITTED: 'TRANSACTION_COMMITTED',
    TRANSACTION_COMMITTED_EMPTY: 'TRANSACTION_COMMITTED_EMPTY',
    TRANSACTION_ABORTED: 'TRANSACTION_ABORTED'
});
const stateMachine = {
    [exports.TxnState.NO_TRANSACTION]: [
        exports.TxnState.NO_TRANSACTION,
        exports.TxnState.STARTING_TRANSACTION
    ],
    [exports.TxnState.STARTING_TRANSACTION]: [
        exports.TxnState.TRANSACTION_IN_PROGRESS,
        exports.TxnState.TRANSACTION_COMMITTED,
        exports.TxnState.TRANSACTION_COMMITTED_EMPTY,
        exports.TxnState.TRANSACTION_ABORTED
    ],
    [exports.TxnState.TRANSACTION_IN_PROGRESS]: [
        exports.TxnState.TRANSACTION_IN_PROGRESS,
        exports.TxnState.TRANSACTION_COMMITTED,
        exports.TxnState.TRANSACTION_ABORTED
    ],
    [exports.TxnState.TRANSACTION_COMMITTED]: [
        exports.TxnState.TRANSACTION_COMMITTED,
        exports.TxnState.TRANSACTION_COMMITTED_EMPTY,
        exports.TxnState.STARTING_TRANSACTION,
        exports.TxnState.NO_TRANSACTION
    ],
    [exports.TxnState.TRANSACTION_ABORTED]: [
        exports.TxnState.STARTING_TRANSACTION,
        exports.TxnState.NO_TRANSACTION
    ],
    [exports.TxnState.TRANSACTION_COMMITTED_EMPTY]: [
        exports.TxnState.TRANSACTION_COMMITTED_EMPTY,
        exports.TxnState.NO_TRANSACTION
    ]
};
const ACTIVE_STATES = new Set([
    exports.TxnState.STARTING_TRANSACTION,
    exports.TxnState.TRANSACTION_IN_PROGRESS
]);
const COMMITTED_STATES = new Set([
    exports.TxnState.TRANSACTION_COMMITTED,
    exports.TxnState.TRANSACTION_COMMITTED_EMPTY,
    exports.TxnState.TRANSACTION_ABORTED
]);
/**
 * @internal
 */ class Transaction {
    /** Create a transaction */ constructor(options){
        options = options ?? {};
        this.state = exports.TxnState.NO_TRANSACTION;
        this.options = {};
        const writeConcern = write_concern_1.WriteConcern.fromOptions(options);
        if (writeConcern) {
            if (writeConcern.w === 0) {
                throw new error_1.MongoTransactionError('Transactions do not support unacknowledged write concern');
            }
            this.options.writeConcern = writeConcern;
        }
        if (options.readConcern) {
            this.options.readConcern = read_concern_1.ReadConcern.fromOptions(options);
        }
        if (options.readPreference) {
            this.options.readPreference = read_preference_1.ReadPreference.fromOptions(options);
        }
        if (options.maxCommitTimeMS) {
            this.options.maxTimeMS = options.maxCommitTimeMS;
        }
        // TODO: This isn't technically necessary
        this._pinnedServer = undefined;
        this._recoveryToken = undefined;
    }
    get server() {
        return this._pinnedServer;
    }
    get recoveryToken() {
        return this._recoveryToken;
    }
    get isPinned() {
        return !!this.server;
    }
    /**
     * @returns Whether the transaction has started
     */ get isStarting() {
        return this.state === exports.TxnState.STARTING_TRANSACTION;
    }
    /**
     * @returns Whether this session is presently in a transaction
     */ get isActive() {
        return ACTIVE_STATES.has(this.state);
    }
    get isCommitted() {
        return COMMITTED_STATES.has(this.state);
    }
    /**
     * Transition the transaction in the state machine
     * @param nextState - The new state to transition to
     */ transition(nextState) {
        const nextStates = stateMachine[this.state];
        if (nextStates && nextStates.includes(nextState)) {
            this.state = nextState;
            if (this.state === exports.TxnState.NO_TRANSACTION || this.state === exports.TxnState.STARTING_TRANSACTION || this.state === exports.TxnState.TRANSACTION_ABORTED) {
                this.unpinServer();
            }
            return;
        }
        throw new error_1.MongoRuntimeError(`Attempted illegal state transition from [${this.state}] to [${nextState}]`);
    }
    pinServer(server) {
        if (this.isActive) {
            this._pinnedServer = server;
        }
    }
    unpinServer() {
        this._pinnedServer = undefined;
    }
}
exports.Transaction = Transaction;
function isTransactionCommand(command) {
    return !!(command.commitTransaction || command.abortTransaction);
} //# sourceMappingURL=transactions.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/sessions.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/Pathology/patho-client/node_modules/.pnpm/next@16.0.6_@babel+core@7.28.5_react-dom@19.2.0_react@19.2.0__react@19.2.0/node_modules/next/dist/compiled/buffer/index.js [app-client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ServerSessionPool = exports.ServerSession = exports.ClientSession = void 0;
exports.maybeClearPinnedConnection = maybeClearPinnedConnection;
exports.applySession = applySession;
exports.updateSessionFromResponse = updateSessionFromResponse;
const bson_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bson.js [app-client] (ecmascript)");
const metrics_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cmap/metrics.js [app-client] (ecmascript)");
const constants_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/constants.js [app-client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
const mongo_types_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/mongo_types.js [app-client] (ecmascript)");
const execute_operation_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/execute_operation.js [app-client] (ecmascript)");
const run_command_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/run_command.js [app-client] (ecmascript)");
const read_concern_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/read_concern.js [app-client] (ecmascript)");
const read_preference_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/read_preference.js [app-client] (ecmascript)");
const common_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/sdam/common.js [app-client] (ecmascript)");
const timeout_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/timeout.js [app-client] (ecmascript)");
const transactions_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/transactions.js [app-client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/utils.js [app-client] (ecmascript)");
const write_concern_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/write_concern.js [app-client] (ecmascript)");
/**
 * A class representing a client session on the server
 *
 * NOTE: not meant to be instantiated directly.
 * @public
 */ class ClientSession extends mongo_types_1.TypedEventEmitter {
    /**
     * Create a client session.
     * @internal
     * @param client - The current client
     * @param sessionPool - The server session pool (Internal Class)
     * @param options - Optional settings
     * @param clientOptions - Optional settings provided when creating a MongoClient
     */ constructor(client, sessionPool, options, clientOptions){
        super();
        /** @internal */ this.timeoutContext = null;
        this.on('error', utils_1.noop);
        if (client == null) {
            // TODO(NODE-3483)
            throw new error_1.MongoRuntimeError('ClientSession requires a MongoClient');
        }
        if (sessionPool == null || !(sessionPool instanceof ServerSessionPool)) {
            // TODO(NODE-3483)
            throw new error_1.MongoRuntimeError('ClientSession requires a ServerSessionPool');
        }
        options = options ?? {};
        this.snapshotEnabled = options.snapshot === true;
        if (options.causalConsistency === true && this.snapshotEnabled) {
            throw new error_1.MongoInvalidArgumentError('Properties "causalConsistency" and "snapshot" are mutually exclusive');
        }
        this.client = client;
        this.sessionPool = sessionPool;
        this.hasEnded = false;
        this.clientOptions = clientOptions;
        this.timeoutMS = options.defaultTimeoutMS ?? client.s.options?.timeoutMS;
        this.explicit = !!options.explicit;
        this._serverSession = this.explicit ? this.sessionPool.acquire() : null;
        this.txnNumberIncrement = 0;
        const defaultCausalConsistencyValue = this.explicit && options.snapshot !== true;
        this.supports = {
            // if we can enable causal consistency, do so by default
            causalConsistency: options.causalConsistency ?? defaultCausalConsistencyValue
        };
        this.clusterTime = options.initialClusterTime;
        this.operationTime = undefined;
        this.owner = options.owner;
        this.defaultTransactionOptions = {
            ...options.defaultTransactionOptions
        };
        this.transaction = new transactions_1.Transaction();
    }
    /** The server id associated with this session */ get id() {
        return this.serverSession?.id;
    }
    get serverSession() {
        let serverSession = this._serverSession;
        if (serverSession == null) {
            if (this.explicit) {
                throw new error_1.MongoRuntimeError('Unexpected null serverSession for an explicit session');
            }
            if (this.hasEnded) {
                throw new error_1.MongoRuntimeError('Unexpected null serverSession for an ended implicit session');
            }
            serverSession = this.sessionPool.acquire();
            this._serverSession = serverSession;
        }
        return serverSession;
    }
    get loadBalanced() {
        return this.client.topology?.description.type === common_1.TopologyType.LoadBalanced;
    }
    /** @internal */ pin(conn) {
        if (this.pinnedConnection) {
            throw TypeError('Cannot pin multiple connections to the same session');
        }
        this.pinnedConnection = conn;
        conn.emit(constants_1.PINNED, this.inTransaction() ? metrics_1.ConnectionPoolMetrics.TXN : metrics_1.ConnectionPoolMetrics.CURSOR);
    }
    /** @internal */ unpin(options) {
        if (this.loadBalanced) {
            return maybeClearPinnedConnection(this, options);
        }
        this.transaction.unpinServer();
    }
    get isPinned() {
        return this.loadBalanced ? !!this.pinnedConnection : this.transaction.isPinned;
    }
    /**
     * Frees any client-side resources held by the current session.  If a session is in a transaction,
     * the transaction is aborted.
     *
     * Does not end the session on the server.
     *
     * @param options - Optional settings. Currently reserved for future use
     */ async endSession(options) {
        try {
            if (this.inTransaction()) {
                await this.abortTransaction({
                    ...options,
                    throwTimeout: true
                });
            }
        } catch (error) {
            // spec indicates that we should ignore all errors for `endSessions`
            if (error.name === 'MongoOperationTimeoutError') throw error;
            (0, utils_1.squashError)(error);
        } finally{
            if (!this.hasEnded) {
                const serverSession = this.serverSession;
                if (serverSession != null) {
                    // release the server session back to the pool
                    this.sessionPool.release(serverSession);
                    // Store a clone of the server session for reference (debugging)
                    this._serverSession = new ServerSession(serverSession);
                }
                // mark the session as ended, and emit a signal
                this.hasEnded = true;
                this.emit('ended', this);
            }
            maybeClearPinnedConnection(this, {
                force: true,
                ...options
            });
        }
    }
    /**
     * @experimental
     * An alias for {@link ClientSession.endSession|ClientSession.endSession()}.
     */ async [Symbol.asyncDispose]() {
        await this.endSession({
            force: true
        });
    }
    /**
     * Advances the operationTime for a ClientSession.
     *
     * @param operationTime - the `BSON.Timestamp` of the operation type it is desired to advance to
     */ advanceOperationTime(operationTime) {
        if (this.operationTime == null) {
            this.operationTime = operationTime;
            return;
        }
        if (operationTime.greaterThan(this.operationTime)) {
            this.operationTime = operationTime;
        }
    }
    /**
     * Advances the clusterTime for a ClientSession to the provided clusterTime of another ClientSession
     *
     * @param clusterTime - the $clusterTime returned by the server from another session in the form of a document containing the `BSON.Timestamp` clusterTime and signature
     */ advanceClusterTime(clusterTime) {
        if (!clusterTime || typeof clusterTime !== 'object') {
            throw new error_1.MongoInvalidArgumentError('input cluster time must be an object');
        }
        if (!clusterTime.clusterTime || clusterTime.clusterTime._bsontype !== 'Timestamp') {
            throw new error_1.MongoInvalidArgumentError('input cluster time "clusterTime" property must be a valid BSON Timestamp');
        }
        if (!clusterTime.signature || clusterTime.signature.hash?._bsontype !== 'Binary' || typeof clusterTime.signature.keyId !== 'bigint' && typeof clusterTime.signature.keyId !== 'number' && clusterTime.signature.keyId?._bsontype !== 'Long' // apparently we decode the key to number?
        ) {
            throw new error_1.MongoInvalidArgumentError('input cluster time must have a valid "signature" property with BSON Binary hash and BSON Long keyId');
        }
        (0, common_1._advanceClusterTime)(this, clusterTime);
    }
    /**
     * Used to determine if this session equals another
     *
     * @param session - The session to compare to
     */ equals(session) {
        if (!(session instanceof ClientSession)) {
            return false;
        }
        if (this.id == null || session.id == null) {
            return false;
        }
        return utils_1.ByteUtils.equals(this.id.id.buffer, session.id.id.buffer);
    }
    /**
     * Increment the transaction number on the internal ServerSession
     *
     * @privateRemarks
     * This helper increments a value stored on the client session that will be
     * added to the serverSession's txnNumber upon applying it to a command.
     * This is because the serverSession is lazily acquired after a connection is obtained
     */ incrementTransactionNumber() {
        this.txnNumberIncrement += 1;
    }
    /** @returns whether this session is currently in a transaction or not */ inTransaction() {
        return this.transaction.isActive;
    }
    /**
     * Starts a new transaction with the given options.
     *
     * @remarks
     * **IMPORTANT**: Running operations in parallel is not supported during a transaction. The use of `Promise.all`,
     * `Promise.allSettled`, `Promise.race`, etc to parallelize operations inside a transaction is
     * undefined behaviour.
     *
     * @param options - Options for the transaction
     */ startTransaction(options) {
        if (this.snapshotEnabled) {
            throw new error_1.MongoCompatibilityError('Transactions are not supported in snapshot sessions');
        }
        if (this.inTransaction()) {
            throw new error_1.MongoTransactionError('Transaction already in progress');
        }
        if (this.isPinned && this.transaction.isCommitted) {
            this.unpin();
        }
        this.commitAttempted = false;
        // increment txnNumber
        this.incrementTransactionNumber();
        // create transaction state
        this.transaction = new transactions_1.Transaction({
            readConcern: options?.readConcern ?? this.defaultTransactionOptions.readConcern ?? this.clientOptions?.readConcern,
            writeConcern: options?.writeConcern ?? this.defaultTransactionOptions.writeConcern ?? this.clientOptions?.writeConcern,
            readPreference: options?.readPreference ?? this.defaultTransactionOptions.readPreference ?? this.clientOptions?.readPreference,
            maxCommitTimeMS: options?.maxCommitTimeMS ?? this.defaultTransactionOptions.maxCommitTimeMS
        });
        this.transaction.transition(transactions_1.TxnState.STARTING_TRANSACTION);
    }
    /**
     * Commits the currently active transaction in this session.
     *
     * @param options - Optional options, can be used to override `defaultTimeoutMS`.
     */ async commitTransaction(options) {
        if (this.transaction.state === transactions_1.TxnState.NO_TRANSACTION) {
            throw new error_1.MongoTransactionError('No transaction started');
        }
        if (this.transaction.state === transactions_1.TxnState.STARTING_TRANSACTION || this.transaction.state === transactions_1.TxnState.TRANSACTION_COMMITTED_EMPTY) {
            // the transaction was never started, we can safely exit here
            this.transaction.transition(transactions_1.TxnState.TRANSACTION_COMMITTED_EMPTY);
            return;
        }
        if (this.transaction.state === transactions_1.TxnState.TRANSACTION_ABORTED) {
            throw new error_1.MongoTransactionError('Cannot call commitTransaction after calling abortTransaction');
        }
        const command = {
            commitTransaction: 1
        };
        const timeoutMS = typeof options?.timeoutMS === 'number' ? options.timeoutMS : typeof this.timeoutMS === 'number' ? this.timeoutMS : null;
        const wc = this.transaction.options.writeConcern ?? this.clientOptions?.writeConcern;
        if (wc != null) {
            if (timeoutMS == null && this.timeoutContext == null) {
                write_concern_1.WriteConcern.apply(command, {
                    wtimeoutMS: 10000,
                    w: 'majority',
                    ...wc
                });
            } else {
                const wcKeys = Object.keys(wc);
                if (wcKeys.length > 2 || !wcKeys.includes('wtimeoutMS') && !wcKeys.includes('wTimeoutMS')) // if the write concern was specified with wTimeoutMS, then we set both wtimeoutMS and wTimeoutMS, guaranteeing at least two keys, so if we have more than two keys, then we can automatically assume that we should add the write concern to the command. If it has 2 or fewer keys, we need to check that those keys aren't the wtimeoutMS or wTimeoutMS options before we add the write concern to the command
                write_concern_1.WriteConcern.apply(command, {
                    ...wc,
                    wtimeoutMS: undefined
                });
            }
        }
        if (this.transaction.state === transactions_1.TxnState.TRANSACTION_COMMITTED || this.commitAttempted) {
            if (timeoutMS == null && this.timeoutContext == null) {
                write_concern_1.WriteConcern.apply(command, {
                    wtimeoutMS: 10000,
                    ...wc,
                    w: 'majority'
                });
            } else {
                write_concern_1.WriteConcern.apply(command, {
                    w: 'majority',
                    ...wc,
                    wtimeoutMS: undefined
                });
            }
        }
        if (typeof this.transaction.options.maxTimeMS === 'number') {
            command.maxTimeMS = this.transaction.options.maxTimeMS;
        }
        if (this.transaction.recoveryToken) {
            command.recoveryToken = this.transaction.recoveryToken;
        }
        const operation = new run_command_1.RunCommandOperation(new utils_1.MongoDBNamespace('admin'), command, {
            session: this,
            readPreference: read_preference_1.ReadPreference.primary,
            bypassPinningCheck: true
        });
        const timeoutContext = this.timeoutContext ?? (typeof timeoutMS === 'number' ? timeout_1.TimeoutContext.create({
            serverSelectionTimeoutMS: this.clientOptions.serverSelectionTimeoutMS,
            socketTimeoutMS: this.clientOptions.socketTimeoutMS,
            timeoutMS
        }) : null);
        try {
            await (0, execute_operation_1.executeOperation)(this.client, operation, timeoutContext);
            this.commitAttempted = undefined;
            return;
        } catch (firstCommitError) {
            this.commitAttempted = true;
            if (firstCommitError instanceof error_1.MongoError && (0, error_1.isRetryableWriteError)(firstCommitError)) {
                // SPEC-1185: apply majority write concern when retrying commitTransaction
                write_concern_1.WriteConcern.apply(command, {
                    wtimeoutMS: 10000,
                    ...wc,
                    w: 'majority'
                });
                // per txns spec, must unpin session in this case
                this.unpin({
                    force: true
                });
                try {
                    await (0, execute_operation_1.executeOperation)(this.client, new run_command_1.RunCommandOperation(new utils_1.MongoDBNamespace('admin'), command, {
                        session: this,
                        readPreference: read_preference_1.ReadPreference.primary,
                        bypassPinningCheck: true
                    }), timeoutContext);
                    return;
                } catch (retryCommitError) {
                    // If the retry failed, we process that error instead of the original
                    if (shouldAddUnknownTransactionCommitResultLabel(retryCommitError)) {
                        retryCommitError.addErrorLabel(error_1.MongoErrorLabel.UnknownTransactionCommitResult);
                    }
                    if (shouldUnpinAfterCommitError(retryCommitError)) {
                        this.unpin({
                            error: retryCommitError
                        });
                    }
                    throw retryCommitError;
                }
            }
            if (shouldAddUnknownTransactionCommitResultLabel(firstCommitError)) {
                firstCommitError.addErrorLabel(error_1.MongoErrorLabel.UnknownTransactionCommitResult);
            }
            if (shouldUnpinAfterCommitError(firstCommitError)) {
                this.unpin({
                    error: firstCommitError
                });
            }
            throw firstCommitError;
        } finally{
            this.transaction.transition(transactions_1.TxnState.TRANSACTION_COMMITTED);
        }
    }
    async abortTransaction(options) {
        if (this.transaction.state === transactions_1.TxnState.NO_TRANSACTION) {
            throw new error_1.MongoTransactionError('No transaction started');
        }
        if (this.transaction.state === transactions_1.TxnState.STARTING_TRANSACTION) {
            // the transaction was never started, we can safely exit here
            this.transaction.transition(transactions_1.TxnState.TRANSACTION_ABORTED);
            return;
        }
        if (this.transaction.state === transactions_1.TxnState.TRANSACTION_ABORTED) {
            throw new error_1.MongoTransactionError('Cannot call abortTransaction twice');
        }
        if (this.transaction.state === transactions_1.TxnState.TRANSACTION_COMMITTED || this.transaction.state === transactions_1.TxnState.TRANSACTION_COMMITTED_EMPTY) {
            throw new error_1.MongoTransactionError('Cannot call abortTransaction after calling commitTransaction');
        }
        const command = {
            abortTransaction: 1
        };
        const timeoutMS = typeof options?.timeoutMS === 'number' ? options.timeoutMS : this.timeoutContext?.csotEnabled() ? this.timeoutContext.timeoutMS // refresh timeoutMS for abort operation
         : typeof this.timeoutMS === 'number' ? this.timeoutMS : null;
        const timeoutContext = timeoutMS != null ? timeout_1.TimeoutContext.create({
            timeoutMS,
            serverSelectionTimeoutMS: this.clientOptions.serverSelectionTimeoutMS,
            socketTimeoutMS: this.clientOptions.socketTimeoutMS
        }) : null;
        const wc = this.transaction.options.writeConcern ?? this.clientOptions?.writeConcern;
        if (wc != null && timeoutMS == null) {
            write_concern_1.WriteConcern.apply(command, {
                wtimeoutMS: 10000,
                w: 'majority',
                ...wc
            });
        }
        if (this.transaction.recoveryToken) {
            command.recoveryToken = this.transaction.recoveryToken;
        }
        const operation = new run_command_1.RunCommandOperation(new utils_1.MongoDBNamespace('admin'), command, {
            session: this,
            readPreference: read_preference_1.ReadPreference.primary,
            bypassPinningCheck: true
        });
        try {
            await (0, execute_operation_1.executeOperation)(this.client, operation, timeoutContext);
            this.unpin();
            return;
        } catch (firstAbortError) {
            this.unpin();
            if (firstAbortError.name === 'MongoRuntimeError') throw firstAbortError;
            if (options?.throwTimeout && firstAbortError.name === 'MongoOperationTimeoutError') {
                throw firstAbortError;
            }
            if (firstAbortError instanceof error_1.MongoError && (0, error_1.isRetryableWriteError)(firstAbortError)) {
                try {
                    await (0, execute_operation_1.executeOperation)(this.client, operation, timeoutContext);
                    return;
                } catch (secondAbortError) {
                    if (secondAbortError.name === 'MongoRuntimeError') throw secondAbortError;
                    if (options?.throwTimeout && secondAbortError.name === 'MongoOperationTimeoutError') {
                        throw secondAbortError;
                    }
                // we do not retry the retry
                }
            }
        // The spec indicates that if the operation times out or fails with a non-retryable error, we should ignore all errors on `abortTransaction`
        } finally{
            this.transaction.transition(transactions_1.TxnState.TRANSACTION_ABORTED);
            if (this.loadBalanced) {
                maybeClearPinnedConnection(this, {
                    force: false
                });
            }
        }
    }
    /**
     * This is here to ensure that ClientSession is never serialized to BSON.
     */ toBSON() {
        throw new error_1.MongoRuntimeError('ClientSession cannot be serialized to BSON.');
    }
    /**
     * Starts a transaction and runs a provided function, ensuring the commitTransaction is always attempted when all operations run in the function have completed.
     *
     * **IMPORTANT:** This method requires the function passed in to return a Promise. That promise must be made by `await`-ing all operations in such a way that rejections are propagated to the returned promise.
     *
     * **IMPORTANT:** Running operations in parallel is not supported during a transaction. The use of `Promise.all`,
     * `Promise.allSettled`, `Promise.race`, etc to parallelize operations inside a transaction is
     * undefined behaviour.
     *
     * **IMPORTANT:** When running an operation inside a `withTransaction` callback, if it is not
     * provided the explicit session in its options, it will not be part of the transaction and it will not respect timeoutMS.
     *
     *
     * @remarks
     * - If all operations successfully complete and the `commitTransaction` operation is successful, then the provided function will return the result of the provided function.
     * - If the transaction is unable to complete or an error is thrown from within the provided function, then the provided function will throw an error.
     *   - If the transaction is manually aborted within the provided function it will not throw.
     * - If the driver needs to attempt to retry the operations, the provided function may be called multiple times.
     *
     * Checkout a descriptive example here:
     * @see https://www.mongodb.com/blog/post/quick-start-nodejs--mongodb--how-to-implement-transactions
     *
     * If a command inside withTransaction fails:
     * - It may cause the transaction on the server to be aborted.
     * - This situation is normally handled transparently by the driver.
     * - However, if the application catches such an error and does not rethrow it, the driver will not be able to determine whether the transaction was aborted or not.
     * - The driver will then retry the transaction indefinitely.
     *
     * To avoid this situation, the application must not silently handle errors within the provided function.
     * If the application needs to handle errors within, it must await all operations such that if an operation is rejected it becomes the rejection of the callback function passed into withTransaction.
     *
     * @param fn - callback to run within a transaction
     * @param options - optional settings for the transaction
     * @returns A raw command response or undefined
     */ async withTransaction(fn, options) {
        const MAX_TIMEOUT = 120000;
        const timeoutMS = options?.timeoutMS ?? this.timeoutMS ?? null;
        this.timeoutContext = timeoutMS != null ? timeout_1.TimeoutContext.create({
            timeoutMS,
            serverSelectionTimeoutMS: this.clientOptions.serverSelectionTimeoutMS,
            socketTimeoutMS: this.clientOptions.socketTimeoutMS
        }) : null;
        const startTime = this.timeoutContext?.csotEnabled() ? this.timeoutContext.start : (0, utils_1.now)();
        let committed = false;
        let result;
        try {
            while(!committed){
                this.startTransaction(options); // may throw on error
                try {
                    const promise = fn(this);
                    if (!(0, utils_1.isPromiseLike)(promise)) {
                        throw new error_1.MongoInvalidArgumentError('Function provided to `withTransaction` must return a Promise');
                    }
                    result = await promise;
                    if (this.transaction.state === transactions_1.TxnState.NO_TRANSACTION || this.transaction.state === transactions_1.TxnState.TRANSACTION_COMMITTED || this.transaction.state === transactions_1.TxnState.TRANSACTION_ABORTED) {
                        // Assume callback intentionally ended the transaction
                        return result;
                    }
                } catch (fnError) {
                    if (!(fnError instanceof error_1.MongoError) || fnError instanceof error_1.MongoInvalidArgumentError) {
                        await this.abortTransaction();
                        throw fnError;
                    }
                    if (this.transaction.state === transactions_1.TxnState.STARTING_TRANSACTION || this.transaction.state === transactions_1.TxnState.TRANSACTION_IN_PROGRESS) {
                        await this.abortTransaction();
                    }
                    if (fnError.hasErrorLabel(error_1.MongoErrorLabel.TransientTransactionError) && (this.timeoutContext != null || (0, utils_1.now)() - startTime < MAX_TIMEOUT)) {
                        continue;
                    }
                    throw fnError;
                }
                while(!committed){
                    try {
                        /*
                         * We will rely on ClientSession.commitTransaction() to
                         * apply a majority write concern if commitTransaction is
                         * being retried (see: DRIVERS-601)
                         */ await this.commitTransaction();
                        committed = true;
                    } catch (commitError) {
                        /*
                         * Note: a maxTimeMS error will have the MaxTimeMSExpired
                         * code (50) and can be reported as a top-level error or
                         * inside writeConcernError, ex.
                         * { ok:0, code: 50, codeName: 'MaxTimeMSExpired' }
                         * { ok:1, writeConcernError: { code: 50, codeName: 'MaxTimeMSExpired' } }
                         */ if (!isMaxTimeMSExpiredError(commitError) && commitError.hasErrorLabel(error_1.MongoErrorLabel.UnknownTransactionCommitResult) && (this.timeoutContext != null || (0, utils_1.now)() - startTime < MAX_TIMEOUT)) {
                            continue;
                        }
                        if (commitError.hasErrorLabel(error_1.MongoErrorLabel.TransientTransactionError) && (this.timeoutContext != null || (0, utils_1.now)() - startTime < MAX_TIMEOUT)) {
                            break;
                        }
                        throw commitError;
                    }
                }
            }
            return result;
        } finally{
            this.timeoutContext = null;
        }
    }
}
exports.ClientSession = ClientSession;
const NON_DETERMINISTIC_WRITE_CONCERN_ERRORS = new Set([
    'CannotSatisfyWriteConcern',
    'UnknownReplWriteConcern',
    'UnsatisfiableWriteConcern'
]);
function shouldUnpinAfterCommitError(commitError) {
    if (commitError instanceof error_1.MongoError) {
        if ((0, error_1.isRetryableWriteError)(commitError) || commitError instanceof error_1.MongoWriteConcernError || isMaxTimeMSExpiredError(commitError)) {
            if (isUnknownTransactionCommitResult(commitError)) {
                // per txns spec, must unpin session in this case
                return true;
            }
        } else if (commitError.hasErrorLabel(error_1.MongoErrorLabel.TransientTransactionError)) {
            return true;
        }
    }
    return false;
}
function shouldAddUnknownTransactionCommitResultLabel(commitError) {
    let ok = (0, error_1.isRetryableWriteError)(commitError);
    ok ||= commitError instanceof error_1.MongoWriteConcernError;
    ok ||= isMaxTimeMSExpiredError(commitError);
    ok &&= isUnknownTransactionCommitResult(commitError);
    return ok;
}
function isUnknownTransactionCommitResult(err) {
    const isNonDeterministicWriteConcernError = err instanceof error_1.MongoServerError && err.codeName && NON_DETERMINISTIC_WRITE_CONCERN_ERRORS.has(err.codeName);
    return isMaxTimeMSExpiredError(err) || !isNonDeterministicWriteConcernError && err.code !== error_1.MONGODB_ERROR_CODES.UnsatisfiableWriteConcern && err.code !== error_1.MONGODB_ERROR_CODES.UnknownReplWriteConcern;
}
function maybeClearPinnedConnection(session, options) {
    // unpin a connection if it has been pinned
    const conn = session.pinnedConnection;
    const error = options?.error;
    if (session.inTransaction() && error && error instanceof error_1.MongoError && error.hasErrorLabel(error_1.MongoErrorLabel.TransientTransactionError)) {
        return;
    }
    const topology = session.client.topology;
    // NOTE: the spec talks about what to do on a network error only, but the tests seem to
    //       to validate that we don't unpin on _all_ errors?
    if (conn && topology != null) {
        const servers = Array.from(topology.s.servers.values());
        const loadBalancer = servers[0];
        if (options?.error == null || options?.force) {
            loadBalancer.pool.checkIn(conn);
            session.pinnedConnection = undefined;
            conn.emit(constants_1.UNPINNED, session.transaction.state !== transactions_1.TxnState.NO_TRANSACTION ? metrics_1.ConnectionPoolMetrics.TXN : metrics_1.ConnectionPoolMetrics.CURSOR);
            if (options?.forceClear) {
                loadBalancer.pool.clear({
                    serviceId: conn.serviceId
                });
            }
        }
    }
}
function isMaxTimeMSExpiredError(err) {
    if (err == null || !(err instanceof error_1.MongoServerError)) {
        return false;
    }
    return err.code === error_1.MONGODB_ERROR_CODES.MaxTimeMSExpired || err.writeConcernError?.code === error_1.MONGODB_ERROR_CODES.MaxTimeMSExpired;
}
/**
 * Reflects the existence of a session on the server. Can be reused by the session pool.
 * WARNING: not meant to be instantiated directly. For internal use only.
 * @public
 */ class ServerSession {
    /** @internal */ constructor(cloned){
        if (cloned != null) {
            const idBytes = __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["Buffer"].allocUnsafe(16);
            idBytes.set(cloned.id.id.buffer);
            this.id = {
                id: new bson_1.Binary(idBytes, cloned.id.id.sub_type)
            };
            this.lastUse = cloned.lastUse;
            this.txnNumber = cloned.txnNumber;
            this.isDirty = cloned.isDirty;
            return;
        }
        this.id = {
            id: new bson_1.Binary((0, utils_1.uuidV4)(), bson_1.Binary.SUBTYPE_UUID)
        };
        this.lastUse = (0, utils_1.now)();
        this.txnNumber = 0;
        this.isDirty = false;
    }
    /**
     * Determines if the server session has timed out.
     *
     * @param sessionTimeoutMinutes - The server's "logicalSessionTimeoutMinutes"
     */ hasTimedOut(sessionTimeoutMinutes) {
        // Take the difference of the lastUse timestamp and now, which will result in a value in
        // milliseconds, and then convert milliseconds to minutes to compare to `sessionTimeoutMinutes`
        const idleTimeMinutes = Math.round((0, utils_1.calculateDurationInMs)(this.lastUse) % 86400000 % 3600000 / 60000);
        return idleTimeMinutes > sessionTimeoutMinutes - 1;
    }
}
exports.ServerSession = ServerSession;
/**
 * Maintains a pool of Server Sessions.
 * For internal use only
 * @internal
 */ class ServerSessionPool {
    constructor(client){
        if (client == null) {
            throw new error_1.MongoRuntimeError('ServerSessionPool requires a MongoClient');
        }
        this.client = client;
        this.sessions = new utils_1.List();
    }
    /**
     * Acquire a Server Session from the pool.
     * Iterates through each session in the pool, removing any stale sessions
     * along the way. The first non-stale session found is removed from the
     * pool and returned. If no non-stale session is found, a new ServerSession is created.
     */ acquire() {
        const sessionTimeoutMinutes = this.client.topology?.logicalSessionTimeoutMinutes ?? 10;
        let session = null;
        // Try to obtain from session pool
        while(this.sessions.length > 0){
            const potentialSession = this.sessions.shift();
            if (potentialSession != null && (!!this.client.topology?.loadBalanced || !potentialSession.hasTimedOut(sessionTimeoutMinutes))) {
                session = potentialSession;
                break;
            }
        }
        // If nothing valid came from the pool make a new one
        if (session == null) {
            session = new ServerSession();
        }
        return session;
    }
    /**
     * Release a session to the session pool
     * Adds the session back to the session pool if the session has not timed out yet.
     * This method also removes any stale sessions from the pool.
     *
     * @param session - The session to release to the pool
     */ release(session) {
        const sessionTimeoutMinutes = this.client.topology?.logicalSessionTimeoutMinutes ?? 10;
        if (this.client.topology?.loadBalanced && !sessionTimeoutMinutes) {
            this.sessions.unshift(session);
        }
        if (!sessionTimeoutMinutes) {
            return;
        }
        this.sessions.prune((session)=>session.hasTimedOut(sessionTimeoutMinutes));
        if (!session.hasTimedOut(sessionTimeoutMinutes)) {
            if (session.isDirty) {
                return;
            }
            // otherwise, readd this session to the session pool
            this.sessions.unshift(session);
        }
    }
}
exports.ServerSessionPool = ServerSessionPool;
/**
 * Optionally decorate a command with sessions specific keys
 *
 * @param session - the session tracking transaction state
 * @param command - the command to decorate
 * @param options - Optional settings passed to calling operation
 *
 * @internal
 */ function applySession(session, command, options) {
    if (session.hasEnded) {
        return new error_1.MongoExpiredSessionError();
    }
    // May acquire serverSession here
    const serverSession = session.serverSession;
    if (serverSession == null) {
        return new error_1.MongoRuntimeError('Unable to acquire server session');
    }
    if (options.writeConcern?.w === 0) {
        if (session && session.explicit) {
            // Error if user provided an explicit session to an unacknowledged write (SPEC-1019)
            return new error_1.MongoAPIError('Cannot have explicit session with unacknowledged writes');
        }
        return;
    }
    // mark the last use of this session, and apply the `lsid`
    serverSession.lastUse = (0, utils_1.now)();
    command.lsid = serverSession.id;
    const inTxnOrTxnCommand = session.inTransaction() || (0, transactions_1.isTransactionCommand)(command);
    const isRetryableWrite = !!options.willRetryWrite;
    if (isRetryableWrite || inTxnOrTxnCommand) {
        serverSession.txnNumber += session.txnNumberIncrement;
        session.txnNumberIncrement = 0;
        // TODO(NODE-2674): Preserve int64 sent from MongoDB
        command.txnNumber = bson_1.Long.fromNumber(serverSession.txnNumber);
    }
    if (!inTxnOrTxnCommand) {
        if (session.transaction.state !== transactions_1.TxnState.NO_TRANSACTION) {
            session.transaction.transition(transactions_1.TxnState.NO_TRANSACTION);
        }
        if (session.supports.causalConsistency && session.operationTime && (0, utils_1.commandSupportsReadConcern)(command)) {
            command.readConcern = command.readConcern || {};
            Object.assign(command.readConcern, {
                afterClusterTime: session.operationTime
            });
        } else if (session.snapshotEnabled) {
            command.readConcern = command.readConcern || {
                level: read_concern_1.ReadConcernLevel.snapshot
            };
            if (session.snapshotTime != null) {
                Object.assign(command.readConcern, {
                    atClusterTime: session.snapshotTime
                });
            }
        }
        return;
    }
    // now attempt to apply transaction-specific sessions data
    // `autocommit` must always be false to differentiate from retryable writes
    command.autocommit = false;
    if (session.transaction.state === transactions_1.TxnState.STARTING_TRANSACTION) {
        session.transaction.transition(transactions_1.TxnState.TRANSACTION_IN_PROGRESS);
        command.startTransaction = true;
        const readConcern = session.transaction.options.readConcern || session?.clientOptions?.readConcern;
        if (readConcern) {
            command.readConcern = readConcern;
        }
        if (session.supports.causalConsistency && session.operationTime) {
            command.readConcern = command.readConcern || {};
            Object.assign(command.readConcern, {
                afterClusterTime: session.operationTime
            });
        }
    }
    return;
}
function updateSessionFromResponse(session, document) {
    if (document.$clusterTime) {
        (0, common_1._advanceClusterTime)(session, document.$clusterTime);
    }
    if (document.operationTime && session && session.supports.causalConsistency) {
        session.advanceOperationTime(document.operationTime);
    }
    if (document.recoveryToken && session && session.inTransaction()) {
        session.transaction._recoveryToken = document.recoveryToken;
    }
    if (session?.snapshotEnabled && session.snapshotTime == null) {
        // find and aggregate commands return atClusterTime on the cursor
        // distinct includes it in the response body
        const atClusterTime = document.atClusterTime;
        if (atClusterTime) {
            session.snapshotTime = atClusterTime;
        }
    }
} //# sourceMappingURL=sessions.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/abstract_cursor.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.CursorTimeoutContext = exports.AbstractCursor = exports.CursorTimeoutMode = exports.CURSOR_FLAGS = void 0;
const stream_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/next@16.0.6_@babel+core@7.28.5_react-dom@19.2.0_react@19.2.0__react@19.2.0/node_modules/next/dist/compiled/stream-browserify/index.js [app-client] (ecmascript)");
const bson_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bson.js [app-client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
const mongo_types_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/mongo_types.js [app-client] (ecmascript)");
const execute_operation_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/execute_operation.js [app-client] (ecmascript)");
const get_more_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/get_more.js [app-client] (ecmascript)");
const kill_cursors_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/kill_cursors.js [app-client] (ecmascript)");
const read_concern_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/read_concern.js [app-client] (ecmascript)");
const read_preference_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/read_preference.js [app-client] (ecmascript)");
const sessions_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/sessions.js [app-client] (ecmascript)");
const timeout_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/timeout.js [app-client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/utils.js [app-client] (ecmascript)");
/** @public */ exports.CURSOR_FLAGS = [
    'tailable',
    'oplogReplay',
    'noCursorTimeout',
    'awaitData',
    'exhaust',
    'partial'
];
function removeActiveCursor() {
    this.client.s.activeCursors.delete(this);
}
/**
 * @public
 * @experimental
 * Specifies how `timeoutMS` is applied to the cursor. Can be either `'cursorLifeTime'` or `'iteration'`
 * When set to `'iteration'`, the deadline specified by `timeoutMS` applies to each call of
 * `cursor.next()`.
 * When set to `'cursorLifetime'`, the deadline applies to the life of the entire cursor.
 *
 * Depending on the type of cursor being used, this option has different default values.
 * For non-tailable cursors, this value defaults to `'cursorLifetime'`
 * For tailable cursors, this value defaults to `'iteration'` since tailable cursors, by
 * definition can have an arbitrarily long lifetime.
 *
 * @example
 * ```ts
 * const cursor = collection.find({}, {timeoutMS: 100, timeoutMode: 'iteration'});
 * for await (const doc of cursor) {
 *  // process doc
 *  // This will throw a timeout error if any of the iterator's `next()` calls takes more than 100ms, but
 *  // will continue to iterate successfully otherwise, regardless of the number of batches.
 * }
 * ```
 *
 * @example
 * ```ts
 * const cursor = collection.find({}, { timeoutMS: 1000, timeoutMode: 'cursorLifetime' });
 * const docs = await cursor.toArray(); // This entire line will throw a timeout error if all batches are not fetched and returned within 1000ms.
 * ```
 */ exports.CursorTimeoutMode = Object.freeze({
    ITERATION: 'iteration',
    LIFETIME: 'cursorLifetime'
});
/** @public */ class AbstractCursor extends mongo_types_1.TypedEventEmitter {
    /** @event */ static{
        this.CLOSE = 'close';
    }
    /** @internal */ constructor(client, namespace, options = {}){
        super();
        /** @internal */ this.documents = null;
        /** @internal */ this.hasEmittedClose = false;
        this.on('error', utils_1.noop);
        if (!client.s.isMongoClient) {
            throw new error_1.MongoRuntimeError('Cursor must be constructed with MongoClient');
        }
        this.cursorClient = client;
        this.cursorNamespace = namespace;
        this.cursorId = null;
        this.initialized = false;
        this.isClosed = false;
        this.isKilled = false;
        this.cursorOptions = {
            readPreference: options.readPreference && options.readPreference instanceof read_preference_1.ReadPreference ? options.readPreference : read_preference_1.ReadPreference.primary,
            ...(0, bson_1.pluckBSONSerializeOptions)(options),
            timeoutMS: options?.timeoutContext?.csotEnabled() ? options.timeoutContext.timeoutMS : options.timeoutMS,
            tailable: options.tailable,
            awaitData: options.awaitData
        };
        if (this.cursorOptions.timeoutMS != null) {
            if (options.timeoutMode == null) {
                if (options.tailable) {
                    if (options.awaitData) {
                        if (options.maxAwaitTimeMS != null && options.maxAwaitTimeMS >= this.cursorOptions.timeoutMS) throw new error_1.MongoInvalidArgumentError('Cannot specify maxAwaitTimeMS >= timeoutMS for a tailable awaitData cursor');
                    }
                    this.cursorOptions.timeoutMode = exports.CursorTimeoutMode.ITERATION;
                } else {
                    this.cursorOptions.timeoutMode = exports.CursorTimeoutMode.LIFETIME;
                }
            } else {
                if (options.tailable && options.timeoutMode === exports.CursorTimeoutMode.LIFETIME) {
                    throw new error_1.MongoInvalidArgumentError("Cannot set tailable cursor's timeoutMode to LIFETIME");
                }
                this.cursorOptions.timeoutMode = options.timeoutMode;
            }
        } else {
            if (options.timeoutMode != null) throw new error_1.MongoInvalidArgumentError('Cannot set timeoutMode without setting timeoutMS');
        }
        // Set for initial command
        this.cursorOptions.omitMaxTimeMS = this.cursorOptions.timeoutMS != null && (this.cursorOptions.timeoutMode === exports.CursorTimeoutMode.ITERATION && !this.cursorOptions.tailable || this.cursorOptions.tailable && !this.cursorOptions.awaitData);
        const readConcern = read_concern_1.ReadConcern.fromOptions(options);
        if (readConcern) {
            this.cursorOptions.readConcern = readConcern;
        }
        if (typeof options.batchSize === 'number') {
            this.cursorOptions.batchSize = options.batchSize;
        }
        // we check for undefined specifically here to allow falsy values
        // eslint-disable-next-line no-restricted-syntax
        if (options.comment !== undefined) {
            this.cursorOptions.comment = options.comment;
        }
        if (typeof options.maxTimeMS === 'number') {
            this.cursorOptions.maxTimeMS = options.maxTimeMS;
        }
        if (typeof options.maxAwaitTimeMS === 'number') {
            this.cursorOptions.maxAwaitTimeMS = options.maxAwaitTimeMS;
        }
        this.cursorSession = options.session ?? null;
        this.deserializationOptions = {
            ...this.cursorOptions,
            validation: {
                utf8: options?.enableUtf8Validation === false ? false : true
            }
        };
        this.timeoutContext = options.timeoutContext;
        this.signal = options.signal;
        this.abortListener = (0, utils_1.addAbortListener)(this.signal, ()=>void this.close().then(undefined, utils_1.squashError));
        this.trackCursor();
    }
    /**
     * The cursor has no id until it receives a response from the initial cursor creating command.
     *
     * It is non-zero for as long as the database has an open cursor.
     *
     * The initiating command may receive a zero id if the entire result is in the `firstBatch`.
     */ get id() {
        return this.cursorId ?? undefined;
    }
    /** @internal */ get isDead() {
        return (this.cursorId?.isZero() ?? false) || this.isClosed || this.isKilled;
    }
    /** @internal */ get client() {
        return this.cursorClient;
    }
    /** @internal */ get server() {
        return this.selectedServer;
    }
    get namespace() {
        return this.cursorNamespace;
    }
    get readPreference() {
        return this.cursorOptions.readPreference;
    }
    get readConcern() {
        return this.cursorOptions.readConcern;
    }
    /** @internal */ get session() {
        return this.cursorSession;
    }
    set session(clientSession) {
        this.cursorSession = clientSession;
    }
    /**
     * The cursor is closed and all remaining locally buffered documents have been iterated.
     */ get closed() {
        return this.isClosed && (this.documents?.length ?? 0) === 0;
    }
    /**
     * A `killCursors` command was attempted on this cursor.
     * This is performed if the cursor id is non zero.
     */ get killed() {
        return this.isKilled;
    }
    get loadBalanced() {
        return !!this.cursorClient.topology?.loadBalanced;
    }
    /**
     * @experimental
     * An alias for {@link AbstractCursor.close|AbstractCursor.close()}.
     */ async [Symbol.asyncDispose]() {
        await this.close();
    }
    /** Adds cursor to client's tracking so it will be closed by MongoClient.close() */ trackCursor() {
        this.cursorClient.s.activeCursors.add(this);
        if (!this.listeners('close').includes(removeActiveCursor)) {
            this.once('close', removeActiveCursor);
        }
    }
    /** Returns current buffered documents length */ bufferedCount() {
        return this.documents?.length ?? 0;
    }
    /** Returns current buffered documents */ readBufferedDocuments(number) {
        const bufferedDocs = [];
        const documentsToRead = Math.min(number ?? this.documents?.length ?? 0, this.documents?.length ?? 0);
        for(let count = 0; count < documentsToRead; count++){
            const document = this.documents?.shift(this.deserializationOptions);
            if (document != null) {
                bufferedDocs.push(document);
            }
        }
        return bufferedDocs;
    }
    async *[Symbol.asyncIterator]() {
        this.signal?.throwIfAborted();
        if (this.closed) {
            return;
        }
        try {
            while(true){
                if (this.isKilled) {
                    return;
                }
                if (this.closed) {
                    return;
                }
                if (this.cursorId != null && this.isDead && (this.documents?.length ?? 0) === 0) {
                    return;
                }
                const document = await this.next();
                // eslint-disable-next-line no-restricted-syntax
                if (document === null) {
                    return;
                }
                yield document;
                this.signal?.throwIfAborted();
            }
        } finally{
            // Only close the cursor if it has not already been closed. This finally clause handles
            // the case when a user would break out of a for await of loop early.
            if (!this.isClosed) {
                try {
                    await this.close();
                } catch (error) {
                    (0, utils_1.squashError)(error);
                }
            }
        }
    }
    stream() {
        const readable = new ReadableCursorStream(this);
        const abortListener = (0, utils_1.addAbortListener)(this.signal, function() {
            readable.destroy(this.reason);
        });
        readable.once('end', ()=>{
            abortListener?.[utils_1.kDispose]();
        });
        return readable;
    }
    async hasNext() {
        this.signal?.throwIfAborted();
        if (this.cursorId === bson_1.Long.ZERO) {
            return false;
        }
        if (this.cursorOptions.timeoutMode === exports.CursorTimeoutMode.ITERATION && this.cursorId != null) {
            this.timeoutContext?.refresh();
        }
        try {
            do {
                if ((this.documents?.length ?? 0) !== 0) {
                    return true;
                }
                await this.fetchBatch();
            }while (!this.isDead || (this.documents?.length ?? 0) !== 0)
        } finally{
            if (this.cursorOptions.timeoutMode === exports.CursorTimeoutMode.ITERATION) {
                this.timeoutContext?.clear();
            }
        }
        return false;
    }
    /** Get the next available document from the cursor, returns null if no more documents are available. */ async next() {
        this.signal?.throwIfAborted();
        if (this.cursorId === bson_1.Long.ZERO) {
            throw new error_1.MongoCursorExhaustedError();
        }
        if (this.cursorOptions.timeoutMode === exports.CursorTimeoutMode.ITERATION && this.cursorId != null) {
            this.timeoutContext?.refresh();
        }
        try {
            do {
                const doc = this.documents?.shift(this.deserializationOptions);
                if (doc != null) {
                    if (this.transform != null) return await this.transformDocument(doc);
                    return doc;
                }
                await this.fetchBatch();
            }while (!this.isDead || (this.documents?.length ?? 0) !== 0)
        } finally{
            if (this.cursorOptions.timeoutMode === exports.CursorTimeoutMode.ITERATION) {
                this.timeoutContext?.clear();
            }
        }
        return null;
    }
    /**
     * Try to get the next available document from the cursor or `null` if an empty batch is returned
     */ async tryNext() {
        this.signal?.throwIfAborted();
        if (this.cursorId === bson_1.Long.ZERO) {
            throw new error_1.MongoCursorExhaustedError();
        }
        if (this.cursorOptions.timeoutMode === exports.CursorTimeoutMode.ITERATION && this.cursorId != null) {
            this.timeoutContext?.refresh();
        }
        try {
            let doc = this.documents?.shift(this.deserializationOptions);
            if (doc != null) {
                if (this.transform != null) return await this.transformDocument(doc);
                return doc;
            }
            await this.fetchBatch();
            doc = this.documents?.shift(this.deserializationOptions);
            if (doc != null) {
                if (this.transform != null) return await this.transformDocument(doc);
                return doc;
            }
        } finally{
            if (this.cursorOptions.timeoutMode === exports.CursorTimeoutMode.ITERATION) {
                this.timeoutContext?.clear();
            }
        }
        return null;
    }
    /**
     * Iterates over all the documents for this cursor using the iterator, callback pattern.
     *
     * If the iterator returns `false`, iteration will stop.
     *
     * @param iterator - The iteration callback.
     * @deprecated - Will be removed in a future release. Use for await...of instead.
     */ async forEach(iterator) {
        this.signal?.throwIfAborted();
        if (typeof iterator !== 'function') {
            throw new error_1.MongoInvalidArgumentError('Argument "iterator" must be a function');
        }
        for await (const document of this){
            const result = iterator(document);
            if (result === false) {
                break;
            }
        }
    }
    /**
     * Frees any client-side resources used by the cursor.
     */ async close(options) {
        await this.cleanup(options?.timeoutMS);
    }
    /**
     * Returns an array of documents. The caller is responsible for making sure that there
     * is enough memory to store the results. Note that the array only contains partial
     * results when this cursor had been previously accessed. In that case,
     * cursor.rewind() can be used to reset the cursor.
     */ async toArray() {
        this.signal?.throwIfAborted();
        const array = [];
        // at the end of the loop (since readBufferedDocuments is called) the buffer will be empty
        // then, the 'await of' syntax will run a getMore call
        for await (const document of this){
            array.push(document);
            const docs = this.readBufferedDocuments();
            if (this.transform != null) {
                for (const doc of docs){
                    array.push(await this.transformDocument(doc));
                }
            } else {
                // Note: previous versions of this logic used `array.push(...)`, which adds each item
                // to the callstack.  For large arrays, this can exceed the maximum call size.
                for (const doc of docs){
                    array.push(doc);
                }
            }
        }
        return array;
    }
    /**
     * Add a cursor flag to the cursor
     *
     * @param flag - The flag to set, must be one of following ['tailable', 'oplogReplay', 'noCursorTimeout', 'awaitData', 'partial' -.
     * @param value - The flag boolean value.
     */ addCursorFlag(flag, value) {
        this.throwIfInitialized();
        if (!exports.CURSOR_FLAGS.includes(flag)) {
            throw new error_1.MongoInvalidArgumentError(`Flag ${flag} is not one of ${exports.CURSOR_FLAGS}`);
        }
        if (typeof value !== 'boolean') {
            throw new error_1.MongoInvalidArgumentError(`Flag ${flag} must be a boolean value`);
        }
        this.cursorOptions[flag] = value;
        return this;
    }
    /**
     * Map all documents using the provided function
     * If there is a transform set on the cursor, that will be called first and the result passed to
     * this function's transform.
     *
     * @remarks
     *
     * **Note** Cursors use `null` internally to indicate that there are no more documents in the cursor. Providing a mapping
     * function that maps values to `null` will result in the cursor closing itself before it has finished iterating
     * all documents.  This will **not** result in a memory leak, just surprising behavior.  For example:
     *
     * ```typescript
     * const cursor = collection.find({});
     * cursor.map(() => null);
     *
     * const documents = await cursor.toArray();
     * // documents is always [], regardless of how many documents are in the collection.
     * ```
     *
     * Other falsey values are allowed:
     *
     * ```typescript
     * const cursor = collection.find({});
     * cursor.map(() => '');
     *
     * const documents = await cursor.toArray();
     * // documents is now an array of empty strings
     * ```
     *
     * **Note for Typescript Users:** adding a transform changes the return type of the iteration of this cursor,
     * it **does not** return a new instance of a cursor. This means when calling map,
     * you should always assign the result to a new variable in order to get a correctly typed cursor variable.
     * Take note of the following example:
     *
     * @example
     * ```typescript
     * const cursor: FindCursor<Document> = coll.find();
     * const mappedCursor: FindCursor<number> = cursor.map(doc => Object.keys(doc).length);
     * const keyCounts: number[] = await mappedCursor.toArray(); // cursor.toArray() still returns Document[]
     * ```
     * @param transform - The mapping transformation method.
     */ map(transform) {
        this.throwIfInitialized();
        const oldTransform = this.transform;
        if (oldTransform) {
            this.transform = (doc)=>{
                return transform(oldTransform(doc));
            };
        } else {
            this.transform = transform;
        }
        return this;
    }
    /**
     * Set the ReadPreference for the cursor.
     *
     * @param readPreference - The new read preference for the cursor.
     */ withReadPreference(readPreference) {
        this.throwIfInitialized();
        if (readPreference instanceof read_preference_1.ReadPreference) {
            this.cursorOptions.readPreference = readPreference;
        } else if (typeof readPreference === 'string') {
            this.cursorOptions.readPreference = read_preference_1.ReadPreference.fromString(readPreference);
        } else {
            throw new error_1.MongoInvalidArgumentError(`Invalid read preference: ${readPreference}`);
        }
        return this;
    }
    /**
     * Set the ReadPreference for the cursor.
     *
     * @param readPreference - The new read preference for the cursor.
     */ withReadConcern(readConcern) {
        this.throwIfInitialized();
        const resolvedReadConcern = read_concern_1.ReadConcern.fromOptions({
            readConcern
        });
        if (resolvedReadConcern) {
            this.cursorOptions.readConcern = resolvedReadConcern;
        }
        return this;
    }
    /**
     * Set a maxTimeMS on the cursor query, allowing for hard timeout limits on queries (Only supported on MongoDB 2.6 or higher)
     *
     * @param value - Number of milliseconds to wait before aborting the query.
     */ maxTimeMS(value) {
        this.throwIfInitialized();
        if (typeof value !== 'number') {
            throw new error_1.MongoInvalidArgumentError('Argument for maxTimeMS must be a number');
        }
        this.cursorOptions.maxTimeMS = value;
        return this;
    }
    /**
     * Set the batch size for the cursor.
     *
     * @param value - The number of documents to return per batch. See {@link https://www.mongodb.com/docs/manual/reference/command/find/|find command documentation}.
     */ batchSize(value) {
        this.throwIfInitialized();
        if (this.cursorOptions.tailable) {
            throw new error_1.MongoTailableCursorError('Tailable cursor does not support batchSize');
        }
        if (typeof value !== 'number') {
            throw new error_1.MongoInvalidArgumentError('Operation "batchSize" requires an integer');
        }
        this.cursorOptions.batchSize = value;
        return this;
    }
    /**
     * Rewind this cursor to its uninitialized state. Any options that are present on the cursor will
     * remain in effect. Iterating this cursor will cause new queries to be sent to the server, even
     * if the resultant data has already been retrieved by this cursor.
     */ rewind() {
        if (this.timeoutContext && this.timeoutContext.owner !== this) {
            throw new error_1.MongoAPIError(`Cannot rewind cursor that does not own its timeout context.`);
        }
        if (!this.initialized) {
            return;
        }
        this.cursorId = null;
        this.documents?.clear();
        this.timeoutContext?.clear();
        this.timeoutContext = undefined;
        this.isClosed = false;
        this.isKilled = false;
        this.initialized = false;
        this.hasEmittedClose = false;
        this.trackCursor();
        // We only want to end this session if we created it, and it hasn't ended yet
        if (this.cursorSession?.explicit === false) {
            if (!this.cursorSession.hasEnded) {
                this.cursorSession.endSession().then(undefined, utils_1.squashError);
            }
            this.cursorSession = null;
        }
    }
    /** @internal */ async getMore() {
        if (this.cursorId == null) {
            throw new error_1.MongoRuntimeError('Unexpected null cursor id. A cursor creating command should have set this');
        }
        if (this.selectedServer == null) {
            throw new error_1.MongoRuntimeError('Unexpected null selectedServer. A cursor creating command should have set this');
        }
        if (this.cursorSession == null) {
            throw new error_1.MongoRuntimeError('Unexpected null session. A cursor creating command should have set this');
        }
        const getMoreOptions = {
            ...this.cursorOptions,
            session: this.cursorSession,
            batchSize: this.cursorOptions.batchSize
        };
        const getMoreOperation = new get_more_1.GetMoreOperation(this.cursorNamespace, this.cursorId, this.selectedServer, getMoreOptions);
        return await (0, execute_operation_1.executeOperation)(this.cursorClient, getMoreOperation, this.timeoutContext);
    }
    /**
     * @internal
     *
     * This function is exposed for the unified test runner's createChangeStream
     * operation.  We cannot refactor to use the abstract _initialize method without
     * a significant refactor.
     */ async cursorInit() {
        if (this.cursorOptions.timeoutMS != null) {
            this.timeoutContext ??= new CursorTimeoutContext(timeout_1.TimeoutContext.create({
                serverSelectionTimeoutMS: this.client.s.options.serverSelectionTimeoutMS,
                timeoutMS: this.cursorOptions.timeoutMS
            }), this);
        }
        try {
            this.cursorSession ??= this.cursorClient.startSession({
                owner: this,
                explicit: false
            });
            const state = await this._initialize(this.cursorSession);
            // Set omitMaxTimeMS to the value needed for subsequent getMore calls
            this.cursorOptions.omitMaxTimeMS = this.cursorOptions.timeoutMS != null;
            const response = state.response;
            this.selectedServer = state.server;
            this.cursorId = response.id;
            this.cursorNamespace = response.ns ?? this.namespace;
            this.documents = response;
            this.initialized = true; // the cursor is now initialized, even if it is dead
        } catch (error) {
            // the cursor is now initialized, even if an error occurred
            this.initialized = true;
            await this.cleanup(undefined, error);
            throw error;
        }
        if (this.isDead) {
            await this.cleanup();
        }
        return;
    }
    /** @internal Attempt to obtain more documents */ async fetchBatch() {
        if (this.isClosed) {
            return;
        }
        if (this.isDead) {
            // if the cursor is dead, we clean it up
            // cleanupCursor should never throw, but if it does it indicates a bug in the driver
            // and we should surface the error
            await this.cleanup();
            return;
        }
        if (this.cursorId == null) {
            await this.cursorInit();
            // If the cursor died or returned documents, return
            if ((this.documents?.length ?? 0) !== 0 || this.isDead) return;
        }
        // Otherwise, run a getMore
        try {
            const response = await this.getMore();
            this.cursorId = response.id;
            this.documents = response;
        } catch (error) {
            try {
                await this.cleanup(undefined, error);
            } catch (cleanupError) {
                // `cleanupCursor` should never throw, squash and throw the original error
                (0, utils_1.squashError)(cleanupError);
            }
            throw error;
        }
        if (this.isDead) {
            // If we successfully received a response from a cursor BUT the cursor indicates that it is exhausted,
            // we intentionally clean up the cursor to release its session back into the pool before the cursor
            // is iterated.  This prevents a cursor that is exhausted on the server from holding
            // onto a session indefinitely until the AbstractCursor is iterated.
            //
            // cleanupCursorAsync should never throw, but if it does it indicates a bug in the driver
            // and we should surface the error
            await this.cleanup();
        }
    }
    /** @internal */ async cleanup(timeoutMS, error) {
        this.abortListener?.[utils_1.kDispose]();
        this.isClosed = true;
        const timeoutContextForKillCursors = ()=>{
            if (timeoutMS != null) {
                this.timeoutContext?.clear();
                return new CursorTimeoutContext(timeout_1.TimeoutContext.create({
                    serverSelectionTimeoutMS: this.client.s.options.serverSelectionTimeoutMS,
                    timeoutMS
                }), this);
            } else {
                return this.timeoutContext?.refreshed();
            }
        };
        const withEmitClose = async (fn)=>{
            try {
                await fn();
            } finally{
                this.emitClose();
            }
        };
        const close = async ()=>{
            // if no session has been defined on the cursor, the cursor was never initialized
            // or the cursor was re-wound and never re-iterated.  In either case, we
            //   1. do not need to end the session (there is no session after all)
            //   2. do not need to kill the cursor server-side
            const session = this.cursorSession;
            if (!session) return;
            try {
                if (!this.isKilled && this.cursorId && !this.cursorId.isZero() && this.cursorNamespace && this.selectedServer && !session.hasEnded) {
                    this.isKilled = true;
                    const cursorId = this.cursorId;
                    this.cursorId = bson_1.Long.ZERO;
                    await (0, execute_operation_1.executeOperation)(this.cursorClient, new kill_cursors_1.KillCursorsOperation(cursorId, this.cursorNamespace, this.selectedServer, {
                        session
                    }), timeoutContextForKillCursors());
                }
            } catch (error) {
                (0, utils_1.squashError)(error);
            } finally{
                if (session.owner === this) {
                    await session.endSession({
                        error
                    });
                }
                if (!session.inTransaction()) {
                    (0, sessions_1.maybeClearPinnedConnection)(session, {
                        error
                    });
                }
            }
        };
        await withEmitClose(close);
    }
    /** @internal */ emitClose() {
        try {
            if (!this.hasEmittedClose && ((this.documents?.length ?? 0) === 0 || this.isClosed)) {
                // @ts-expect-error: CursorEvents is generic so Parameters<CursorEvents["close"]> may not be assignable to `[]`. Not sure how to require extenders do not add parameters.
                this.emit('close');
            }
        } finally{
            this.hasEmittedClose = true;
        }
    }
    /** @internal */ async transformDocument(document) {
        if (this.transform == null) return document;
        try {
            const transformedDocument = this.transform(document);
            // eslint-disable-next-line no-restricted-syntax
            if (transformedDocument === null) {
                const TRANSFORM_TO_NULL_ERROR = 'Cursor returned a `null` document, but the cursor is not exhausted.  Mapping documents to `null` is not supported in the cursor transform.';
                throw new error_1.MongoAPIError(TRANSFORM_TO_NULL_ERROR);
            }
            return transformedDocument;
        } catch (transformError) {
            try {
                await this.close();
            } catch (closeError) {
                (0, utils_1.squashError)(closeError);
            }
            throw transformError;
        }
    }
    /** @internal */ throwIfInitialized() {
        if (this.initialized) throw new error_1.MongoCursorInUseError();
    }
}
exports.AbstractCursor = AbstractCursor;
class ReadableCursorStream extends stream_1.Readable {
    constructor(cursor){
        super({
            objectMode: true,
            autoDestroy: false,
            highWaterMark: 1
        });
        this._readInProgress = false;
        this._cursor = cursor;
    }
    // eslint-disable-next-line @typescript-eslint/no-unused-vars
    _read(size) {
        if (!this._readInProgress) {
            this._readInProgress = true;
            this._readNext();
        }
    }
    _destroy(error, callback) {
        this._cursor.close().then(()=>callback(error), (closeError)=>callback(closeError));
    }
    _readNext() {
        if (this._cursor.id === bson_1.Long.ZERO) {
            this.push(null);
            return;
        }
        this._cursor.next().then(// result from next()
        (result)=>{
            if (result == null) {
                this.push(null);
            } else if (this.destroyed) {
                this._cursor.close().then(undefined, utils_1.squashError);
            } else {
                if (this.push(result)) {
                    return this._readNext();
                }
                this._readInProgress = false;
            }
        }, // error from next()
        (err)=>{
            // NOTE: This is questionable, but we have a test backing the behavior. It seems the
            //       desired behavior is that a stream ends cleanly when a user explicitly closes
            //       a client during iteration. Alternatively, we could do the "right" thing and
            //       propagate the error message by removing this special case.
            if (err.message.match(/server is closed/)) {
                this._cursor.close().then(undefined, utils_1.squashError);
                return this.push(null);
            }
            // NOTE: This is also perhaps questionable. The rationale here is that these errors tend
            //       to be "operation was interrupted", where a cursor has been closed but there is an
            //       active getMore in-flight. This used to check if the cursor was killed but once
            //       that changed to happen in cleanup legitimate errors would not destroy the
            //       stream. There are change streams test specifically test these cases.
            if (err.message.match(/operation was interrupted/)) {
                return this.push(null);
            }
            // NOTE: The two above checks on the message of the error will cause a null to be pushed
            //       to the stream, thus closing the stream before the destroy call happens. This means
            //       that either of those error messages on a change stream will not get a proper
            //       'error' event to be emitted (the error passed to destroy). Change stream resumability
            //       relies on that error event to be emitted to create its new cursor and thus was not
            //       working on 4.4 servers because the error emitted on failover was "interrupted at
            //       shutdown" while on 5.0+ it is "The server is in quiesce mode and will shut down".
            //       See NODE-4475.
            return this.destroy(err);
        })// if either of the above handlers throw
        .catch((error)=>{
            this._readInProgress = false;
            this.destroy(error);
        });
    }
}
/**
 * @internal
 * The cursor timeout context is a wrapper around a timeout context
 * that keeps track of the "owner" of the cursor.  For timeout contexts
 * instantiated inside a cursor, the owner will be the cursor.
 *
 * All timeout behavior is exactly the same as the wrapped timeout context's.
 */ class CursorTimeoutContext extends timeout_1.TimeoutContext {
    constructor(timeoutContext, owner){
        super();
        this.timeoutContext = timeoutContext;
        this.owner = owner;
    }
    get serverSelectionTimeout() {
        return this.timeoutContext.serverSelectionTimeout;
    }
    get connectionCheckoutTimeout() {
        return this.timeoutContext.connectionCheckoutTimeout;
    }
    get clearServerSelectionTimeout() {
        return this.timeoutContext.clearServerSelectionTimeout;
    }
    get timeoutForSocketWrite() {
        return this.timeoutContext.timeoutForSocketWrite;
    }
    get timeoutForSocketRead() {
        return this.timeoutContext.timeoutForSocketRead;
    }
    csotEnabled() {
        return this.timeoutContext.csotEnabled();
    }
    refresh() {
        if (typeof this.owner !== 'symbol') return this.timeoutContext.refresh();
    }
    clear() {
        if (typeof this.owner !== 'symbol') return this.timeoutContext.clear();
    }
    get maxTimeMS() {
        return this.timeoutContext.maxTimeMS;
    }
    get timeoutMS() {
        return this.timeoutContext.csotEnabled() ? this.timeoutContext.timeoutMS : null;
    }
    refreshed() {
        return new CursorTimeoutContext(this.timeoutContext.refreshed(), this.owner);
    }
    addMaxTimeMSToCommand(command, options) {
        this.timeoutContext.addMaxTimeMSToCommand(command, options);
    }
    getSocketTimeoutMS() {
        return this.timeoutContext.getSocketTimeoutMS();
    }
}
exports.CursorTimeoutContext = CursorTimeoutContext; //# sourceMappingURL=abstract_cursor.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/explainable_cursor.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ExplainableCursor = void 0;
const abstract_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/abstract_cursor.js [app-client] (ecmascript)");
/**
 * @public
 *
 * A base class for any cursors that have `explain()` methods.
 */ class ExplainableCursor extends abstract_cursor_1.AbstractCursor {
    resolveExplainTimeoutOptions(verbosity, options) {
        let explain;
        let timeout;
        if (verbosity == null && options == null) {
            explain = undefined;
            timeout = undefined;
        } else if (verbosity != null && options == null) {
            explain = typeof verbosity !== 'object' ? verbosity : 'verbosity' in verbosity ? verbosity : undefined;
            timeout = typeof verbosity === 'object' && 'timeoutMS' in verbosity ? verbosity : undefined;
        } else {
            // @ts-expect-error TS isn't smart enough to determine that if both options are provided, the first is explain options
            explain = verbosity;
            timeout = options;
        }
        return {
            timeout,
            explain
        };
    }
}
exports.ExplainableCursor = ExplainableCursor; //# sourceMappingURL=explainable_cursor.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/aggregation_cursor.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.AggregationCursor = void 0;
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
const explain_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/explain.js [app-client] (ecmascript)");
const aggregate_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/aggregate.js [app-client] (ecmascript)");
const execute_operation_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/execute_operation.js [app-client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/utils.js [app-client] (ecmascript)");
const abstract_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/abstract_cursor.js [app-client] (ecmascript)");
const explainable_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/explainable_cursor.js [app-client] (ecmascript)");
/**
 * The **AggregationCursor** class is an internal class that embodies an aggregation cursor on MongoDB
 * allowing for iteration over the results returned from the underlying query. It supports
 * one by one document iteration, conversion to an array or can be iterated as a Node 4.X
 * or higher stream
 * @public
 */ class AggregationCursor extends explainable_cursor_1.ExplainableCursor {
    /** @internal */ constructor(client, namespace, pipeline = [], options = {}){
        super(client, namespace, options);
        this.pipeline = pipeline;
        this.aggregateOptions = options;
        const lastStage = this.pipeline[this.pipeline.length - 1];
        if (this.cursorOptions.timeoutMS != null && this.cursorOptions.timeoutMode === abstract_cursor_1.CursorTimeoutMode.ITERATION && (lastStage?.$merge != null || lastStage?.$out != null)) throw new error_1.MongoAPIError('Cannot use $out or $merge stage with ITERATION timeoutMode');
    }
    clone() {
        const clonedOptions = (0, utils_1.mergeOptions)({}, this.aggregateOptions);
        delete clonedOptions.session;
        return new AggregationCursor(this.client, this.namespace, this.pipeline, {
            ...clonedOptions
        });
    }
    map(transform) {
        return super.map(transform);
    }
    /** @internal */ async _initialize(session) {
        const options = {
            ...this.aggregateOptions,
            ...this.cursorOptions,
            session,
            signal: this.signal
        };
        if (options.explain) {
            try {
                (0, explain_1.validateExplainTimeoutOptions)(options, explain_1.Explain.fromOptions(options));
            } catch  {
                throw new error_1.MongoAPIError('timeoutMS cannot be used with explain when explain is specified in aggregateOptions');
            }
        }
        const aggregateOperation = new aggregate_1.AggregateOperation(this.namespace, this.pipeline, options);
        const response = await (0, execute_operation_1.executeOperation)(this.client, aggregateOperation, this.timeoutContext);
        return {
            server: aggregateOperation.server,
            session,
            response
        };
    }
    async explain(verbosity, options) {
        const { explain, timeout } = this.resolveExplainTimeoutOptions(verbosity, options);
        return (await (0, execute_operation_1.executeOperation)(this.client, new aggregate_1.AggregateOperation(this.namespace, this.pipeline, {
            ...this.aggregateOptions,
            ...this.cursorOptions,
            ...timeout,
            explain: explain ?? true
        }))).shift(this.deserializationOptions);
    }
    addStage(stage) {
        this.throwIfInitialized();
        if (this.cursorOptions.timeoutMS != null && this.cursorOptions.timeoutMode === abstract_cursor_1.CursorTimeoutMode.ITERATION && (stage.$out != null || stage.$merge != null)) {
            throw new error_1.MongoAPIError('Cannot use $out or $merge stage with ITERATION timeoutMode');
        }
        this.pipeline.push(stage);
        return this;
    }
    group($group) {
        return this.addStage({
            $group
        });
    }
    /** Add a limit stage to the aggregation pipeline */ limit($limit) {
        return this.addStage({
            $limit
        });
    }
    /** Add a match stage to the aggregation pipeline */ match($match) {
        return this.addStage({
            $match
        });
    }
    /** Add an out stage to the aggregation pipeline */ out($out) {
        return this.addStage({
            $out
        });
    }
    /**
     * Add a project stage to the aggregation pipeline
     *
     * @remarks
     * In order to strictly type this function you must provide an interface
     * that represents the effect of your projection on the result documents.
     *
     * By default chaining a projection to your cursor changes the returned type to the generic {@link Document} type.
     * You should specify a parameterized type to have assertions on your final results.
     *
     * @example
     * ```typescript
     * // Best way
     * const docs: AggregationCursor<{ a: number }> = cursor.project<{ a: number }>({ _id: 0, a: true });
     * // Flexible way
     * const docs: AggregationCursor<Document> = cursor.project({ _id: 0, a: true });
     * ```
     *
     * @remarks
     * In order to strictly type this function you must provide an interface
     * that represents the effect of your projection on the result documents.
     *
     * **Note for Typescript Users:** adding a transform changes the return type of the iteration of this cursor,
     * it **does not** return a new instance of a cursor. This means when calling project,
     * you should always assign the result to a new variable in order to get a correctly typed cursor variable.
     * Take note of the following example:
     *
     * @example
     * ```typescript
     * const cursor: AggregationCursor<{ a: number; b: string }> = coll.aggregate([]);
     * const projectCursor = cursor.project<{ a: number }>({ _id: 0, a: true });
     * const aPropOnlyArray: {a: number}[] = await projectCursor.toArray();
     *
     * // or always use chaining and save the final cursor
     *
     * const cursor = coll.aggregate().project<{ a: string }>({
     *   _id: 0,
     *   a: { $convert: { input: '$a', to: 'string' }
     * }});
     * ```
     */ project($project) {
        return this.addStage({
            $project
        });
    }
    /** Add a lookup stage to the aggregation pipeline */ lookup($lookup) {
        return this.addStage({
            $lookup
        });
    }
    /** Add a redact stage to the aggregation pipeline */ redact($redact) {
        return this.addStage({
            $redact
        });
    }
    /** Add a skip stage to the aggregation pipeline */ skip($skip) {
        return this.addStage({
            $skip
        });
    }
    /** Add a sort stage to the aggregation pipeline */ sort($sort) {
        return this.addStage({
            $sort
        });
    }
    /** Add a unwind stage to the aggregation pipeline */ unwind($unwind) {
        return this.addStage({
            $unwind
        });
    }
    /** Add a geoNear stage to the aggregation pipeline */ geoNear($geoNear) {
        return this.addStage({
            $geoNear
        });
    }
}
exports.AggregationCursor = AggregationCursor; //# sourceMappingURL=aggregation_cursor.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/find_cursor.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.FindCursor = exports.FLAGS = void 0;
const responses_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [app-client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
const explain_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/explain.js [app-client] (ecmascript)");
const count_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/count.js [app-client] (ecmascript)");
const execute_operation_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/execute_operation.js [app-client] (ecmascript)");
const find_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/find.js [app-client] (ecmascript)");
const sort_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/sort.js [app-client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/utils.js [app-client] (ecmascript)");
const explainable_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/explainable_cursor.js [app-client] (ecmascript)");
/** @public Flags allowed for cursor */ exports.FLAGS = [
    'tailable',
    'oplogReplay',
    'noCursorTimeout',
    'awaitData',
    'exhaust',
    'partial'
];
/** @public */ class FindCursor extends explainable_cursor_1.ExplainableCursor {
    /** @internal */ constructor(client, namespace, filter = {}, options = {}){
        super(client, namespace, options);
        /** @internal */ this.numReturned = 0;
        this.cursorFilter = filter;
        this.findOptions = options;
        if (options.sort != null) {
            this.findOptions.sort = (0, sort_1.formatSort)(options.sort);
        }
    }
    clone() {
        const clonedOptions = (0, utils_1.mergeOptions)({}, this.findOptions);
        delete clonedOptions.session;
        return new FindCursor(this.client, this.namespace, this.cursorFilter, {
            ...clonedOptions
        });
    }
    map(transform) {
        return super.map(transform);
    }
    /** @internal */ async _initialize(session) {
        const options = {
            ...this.findOptions,
            ...this.cursorOptions,
            session,
            signal: this.signal
        };
        if (options.explain) {
            try {
                (0, explain_1.validateExplainTimeoutOptions)(options, explain_1.Explain.fromOptions(options));
            } catch  {
                throw new error_1.MongoAPIError('timeoutMS cannot be used with explain when explain is specified in findOptions');
            }
        }
        const findOperation = new find_1.FindOperation(this.namespace, this.cursorFilter, options);
        const response = await (0, execute_operation_1.executeOperation)(this.client, findOperation, this.timeoutContext);
        // the response is not a cursor when `explain` is enabled
        this.numReturned = response.batchSize;
        return {
            server: findOperation.server,
            session,
            response
        };
    }
    /** @internal */ async getMore() {
        const numReturned = this.numReturned;
        const limit = this.findOptions.limit ?? Infinity;
        const remaining = limit - numReturned;
        if (numReturned === limit && !this.id?.isZero()) {
            // this is an optimization for the special case of a limit for a find command to avoid an
            // extra getMore when the limit has been reached and the limit is a multiple of the batchSize.
            // This is a consequence of the new query engine in 5.0 having no knowledge of the limit as it
            // produces results for the find command.  Once a batch is filled up, it is returned and only
            // on the subsequent getMore will the query framework consider the limit, determine the cursor
            // is exhausted and return a cursorId of zero.
            // instead, if we determine there are no more documents to request from the server, we preemptively
            // close the cursor
            try {
                await this.close();
            } catch (error) {
                (0, utils_1.squashError)(error);
            }
            return responses_1.CursorResponse.emptyGetMore;
        }
        // TODO(DRIVERS-1448): Remove logic to enforce `limit` in the driver
        let cleanup = utils_1.noop;
        const { batchSize } = this.cursorOptions;
        if (batchSize != null && batchSize > remaining) {
            this.cursorOptions.batchSize = remaining;
            // After executing the final getMore, re-assign the batchSize back to its original value so that
            // if the cursor is rewound and executed, the batchSize is still correct.
            cleanup = ()=>{
                this.cursorOptions.batchSize = batchSize;
            };
        }
        try {
            const response = await super.getMore();
            this.numReturned = this.numReturned + response.batchSize;
            return response;
        } finally{
            cleanup?.();
        }
    }
    /**
     * Get the count of documents for this cursor
     * @deprecated Use `collection.estimatedDocumentCount` or `collection.countDocuments` instead
     */ async count(options) {
        (0, utils_1.emitWarningOnce)('cursor.count is deprecated and will be removed in the next major version, please use `collection.estimatedDocumentCount` or `collection.countDocuments` instead ');
        if (typeof options === 'boolean') {
            throw new error_1.MongoInvalidArgumentError('Invalid first parameter to count');
        }
        return await (0, execute_operation_1.executeOperation)(this.client, new count_1.CountOperation(this.namespace, this.cursorFilter, {
            ...this.findOptions,
            ...this.cursorOptions,
            ...options
        }));
    }
    async explain(verbosity, options) {
        const { explain, timeout } = this.resolveExplainTimeoutOptions(verbosity, options);
        return (await (0, execute_operation_1.executeOperation)(this.client, new find_1.FindOperation(this.namespace, this.cursorFilter, {
            ...this.findOptions,
            ...this.cursorOptions,
            ...timeout,
            explain: explain ?? true
        }))).shift(this.deserializationOptions);
    }
    /** Set the cursor query */ filter(filter) {
        this.throwIfInitialized();
        this.cursorFilter = filter;
        return this;
    }
    /**
     * Set the cursor hint
     *
     * @param hint - If specified, then the query system will only consider plans using the hinted index.
     */ hint(hint) {
        this.throwIfInitialized();
        this.findOptions.hint = hint;
        return this;
    }
    /**
     * Set the cursor min
     *
     * @param min - Specify a $min value to specify the inclusive lower bound for a specific index in order to constrain the results of find(). The $min specifies the lower bound for all keys of a specific index in order.
     */ min(min) {
        this.throwIfInitialized();
        this.findOptions.min = min;
        return this;
    }
    /**
     * Set the cursor max
     *
     * @param max - Specify a $max value to specify the exclusive upper bound for a specific index in order to constrain the results of find(). The $max specifies the upper bound for all keys of a specific index in order.
     */ max(max) {
        this.throwIfInitialized();
        this.findOptions.max = max;
        return this;
    }
    /**
     * Set the cursor returnKey.
     * If set to true, modifies the cursor to only return the index field or fields for the results of the query, rather than documents.
     * If set to true and the query does not use an index to perform the read operation, the returned documents will not contain any fields.
     *
     * @param value - the returnKey value.
     */ returnKey(value) {
        this.throwIfInitialized();
        this.findOptions.returnKey = value;
        return this;
    }
    /**
     * Modifies the output of a query by adding a field $recordId to matching documents. $recordId is the internal key which uniquely identifies a document in a collection.
     *
     * @param value - The $showDiskLoc option has now been deprecated and replaced with the showRecordId field. $showDiskLoc will still be accepted for OP_QUERY stye find.
     */ showRecordId(value) {
        this.throwIfInitialized();
        this.findOptions.showRecordId = value;
        return this;
    }
    /**
     * Add a query modifier to the cursor query
     *
     * @param name - The query modifier (must start with $, such as $orderby etc)
     * @param value - The modifier value.
     */ addQueryModifier(name, value) {
        this.throwIfInitialized();
        if (name[0] !== '$') {
            throw new error_1.MongoInvalidArgumentError(`${name} is not a valid query modifier`);
        }
        // Strip of the $
        const field = name.substr(1);
        // NOTE: consider some TS magic for this
        switch(field){
            case 'comment':
                this.findOptions.comment = value;
                break;
            case 'explain':
                this.findOptions.explain = value;
                break;
            case 'hint':
                this.findOptions.hint = value;
                break;
            case 'max':
                this.findOptions.max = value;
                break;
            case 'maxTimeMS':
                this.findOptions.maxTimeMS = value;
                break;
            case 'min':
                this.findOptions.min = value;
                break;
            case 'orderby':
                this.findOptions.sort = (0, sort_1.formatSort)(value);
                break;
            case 'query':
                this.cursorFilter = value;
                break;
            case 'returnKey':
                this.findOptions.returnKey = value;
                break;
            case 'showDiskLoc':
                this.findOptions.showRecordId = value;
                break;
            default:
                throw new error_1.MongoInvalidArgumentError(`Invalid query modifier: ${name}`);
        }
        return this;
    }
    /**
     * Add a comment to the cursor query allowing for tracking the comment in the log.
     *
     * @param value - The comment attached to this query.
     */ comment(value) {
        this.throwIfInitialized();
        this.findOptions.comment = value;
        return this;
    }
    /**
     * Set a maxAwaitTimeMS on a tailing cursor query to allow to customize the timeout value for the option awaitData (Only supported on MongoDB 3.2 or higher, ignored otherwise)
     *
     * @param value - Number of milliseconds to wait before aborting the tailed query.
     */ maxAwaitTimeMS(value) {
        this.throwIfInitialized();
        if (typeof value !== 'number') {
            throw new error_1.MongoInvalidArgumentError('Argument for maxAwaitTimeMS must be a number');
        }
        this.findOptions.maxAwaitTimeMS = value;
        return this;
    }
    /**
     * Set a maxTimeMS on the cursor query, allowing for hard timeout limits on queries (Only supported on MongoDB 2.6 or higher)
     *
     * @param value - Number of milliseconds to wait before aborting the query.
     */ maxTimeMS(value) {
        this.throwIfInitialized();
        if (typeof value !== 'number') {
            throw new error_1.MongoInvalidArgumentError('Argument for maxTimeMS must be a number');
        }
        this.findOptions.maxTimeMS = value;
        return this;
    }
    /**
     * Add a project stage to the aggregation pipeline
     *
     * @remarks
     * In order to strictly type this function you must provide an interface
     * that represents the effect of your projection on the result documents.
     *
     * By default chaining a projection to your cursor changes the returned type to the generic
     * {@link Document} type.
     * You should specify a parameterized type to have assertions on your final results.
     *
     * @example
     * ```typescript
     * // Best way
     * const docs: FindCursor<{ a: number }> = cursor.project<{ a: number }>({ _id: 0, a: true });
     * // Flexible way
     * const docs: FindCursor<Document> = cursor.project({ _id: 0, a: true });
     * ```
     *
     * @remarks
     *
     * **Note for Typescript Users:** adding a transform changes the return type of the iteration of this cursor,
     * it **does not** return a new instance of a cursor. This means when calling project,
     * you should always assign the result to a new variable in order to get a correctly typed cursor variable.
     * Take note of the following example:
     *
     * @example
     * ```typescript
     * const cursor: FindCursor<{ a: number; b: string }> = coll.find();
     * const projectCursor = cursor.project<{ a: number }>({ _id: 0, a: true });
     * const aPropOnlyArray: {a: number}[] = await projectCursor.toArray();
     *
     * // or always use chaining and save the final cursor
     *
     * const cursor = coll.find().project<{ a: string }>({
     *   _id: 0,
     *   a: { $convert: { input: '$a', to: 'string' }
     * }});
     * ```
     */ project(value) {
        this.throwIfInitialized();
        this.findOptions.projection = value;
        return this;
    }
    /**
     * Sets the sort order of the cursor query.
     *
     * @param sort - The key or keys set for the sort.
     * @param direction - The direction of the sorting (1 or -1).
     */ sort(sort, direction) {
        this.throwIfInitialized();
        if (this.findOptions.tailable) {
            throw new error_1.MongoTailableCursorError('Tailable cursor does not support sorting');
        }
        this.findOptions.sort = (0, sort_1.formatSort)(sort, direction);
        return this;
    }
    /**
     * Allows disk use for blocking sort operations exceeding 100MB memory. (MongoDB 3.2 or higher)
     *
     * @remarks
     * {@link https://www.mongodb.com/docs/manual/reference/command/find/#find-cmd-allowdiskuse | find command allowDiskUse documentation}
     */ allowDiskUse(allow = true) {
        this.throwIfInitialized();
        if (!this.findOptions.sort) {
            throw new error_1.MongoInvalidArgumentError('Option "allowDiskUse" requires a sort specification');
        }
        // As of 6.0 the default is true. This allows users to get back to the old behavior.
        if (!allow) {
            this.findOptions.allowDiskUse = false;
            return this;
        }
        this.findOptions.allowDiskUse = true;
        return this;
    }
    /**
     * Set the collation options for the cursor.
     *
     * @param value - The cursor collation options (MongoDB 3.4 or higher) settings for update operation (see 3.4 documentation for available fields).
     */ collation(value) {
        this.throwIfInitialized();
        this.findOptions.collation = value;
        return this;
    }
    /**
     * Set the limit for the cursor.
     *
     * @param value - The limit for the cursor query.
     */ limit(value) {
        this.throwIfInitialized();
        if (this.findOptions.tailable) {
            throw new error_1.MongoTailableCursorError('Tailable cursor does not support limit');
        }
        if (typeof value !== 'number') {
            throw new error_1.MongoInvalidArgumentError('Operation "limit" requires an integer');
        }
        this.findOptions.limit = value;
        return this;
    }
    /**
     * Set the skip for the cursor.
     *
     * @param value - The skip for the cursor query.
     */ skip(value) {
        this.throwIfInitialized();
        if (this.findOptions.tailable) {
            throw new error_1.MongoTailableCursorError('Tailable cursor does not support skip');
        }
        if (typeof value !== 'number') {
            throw new error_1.MongoInvalidArgumentError('Operation "skip" requires an integer');
        }
        this.findOptions.skip = value;
        return this;
    }
}
exports.FindCursor = FindCursor; //# sourceMappingURL=find_cursor.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/list_indexes_cursor.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ListIndexesCursor = void 0;
const execute_operation_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/execute_operation.js [app-client] (ecmascript)");
const indexes_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/indexes.js [app-client] (ecmascript)");
const abstract_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/abstract_cursor.js [app-client] (ecmascript)");
/** @public */ class ListIndexesCursor extends abstract_cursor_1.AbstractCursor {
    constructor(collection, options){
        super(collection.client, collection.s.namespace, options);
        this.parent = collection;
        this.options = options;
    }
    clone() {
        return new ListIndexesCursor(this.parent, {
            ...this.options,
            ...this.cursorOptions
        });
    }
    /** @internal */ async _initialize(session) {
        const operation = new indexes_1.ListIndexesOperation(this.parent, {
            ...this.cursorOptions,
            ...this.options,
            session
        });
        const response = await (0, execute_operation_1.executeOperation)(this.parent.client, operation, this.timeoutContext);
        return {
            server: operation.server,
            session,
            response
        };
    }
}
exports.ListIndexesCursor = ListIndexesCursor; //# sourceMappingURL=list_indexes_cursor.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/list_search_indexes_cursor.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ListSearchIndexesCursor = void 0;
const aggregation_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/aggregation_cursor.js [app-client] (ecmascript)");
/** @public */ class ListSearchIndexesCursor extends aggregation_cursor_1.AggregationCursor {
    /** @internal */ constructor({ fullNamespace: ns, client }, name, options = {}){
        const pipeline = name == null ? [
            {
                $listSearchIndexes: {}
            }
        ] : [
            {
                $listSearchIndexes: {
                    name
                }
            }
        ];
        super(client, ns, pipeline, options);
    }
}
exports.ListSearchIndexesCursor = ListSearchIndexesCursor; //# sourceMappingURL=list_search_indexes_cursor.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/change_stream_cursor.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ChangeStreamCursor = void 0;
const change_stream_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/change_stream.js [app-client] (ecmascript)");
const constants_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/constants.js [app-client] (ecmascript)");
const aggregate_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/aggregate.js [app-client] (ecmascript)");
const execute_operation_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/execute_operation.js [app-client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/utils.js [app-client] (ecmascript)");
const abstract_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/abstract_cursor.js [app-client] (ecmascript)");
/** @internal */ class ChangeStreamCursor extends abstract_cursor_1.AbstractCursor {
    constructor(client, namespace, pipeline = [], options = {}){
        super(client, namespace, {
            ...options,
            tailable: true,
            awaitData: true
        });
        this.pipeline = pipeline;
        this.changeStreamCursorOptions = options;
        this._resumeToken = null;
        this.startAtOperationTime = options.startAtOperationTime ?? null;
        if (options.startAfter) {
            this.resumeToken = options.startAfter;
        } else if (options.resumeAfter) {
            this.resumeToken = options.resumeAfter;
        }
    }
    set resumeToken(token) {
        this._resumeToken = token;
        this.emit(change_stream_1.ChangeStream.RESUME_TOKEN_CHANGED, token);
    }
    get resumeToken() {
        return this._resumeToken;
    }
    get resumeOptions() {
        const options = {
            ...this.changeStreamCursorOptions
        };
        for (const key of [
            'resumeAfter',
            'startAfter',
            'startAtOperationTime'
        ]){
            delete options[key];
        }
        if (this.resumeToken != null) {
            if (this.changeStreamCursorOptions.startAfter && !this.hasReceived) {
                options.startAfter = this.resumeToken;
            } else {
                options.resumeAfter = this.resumeToken;
            }
        } else if (this.startAtOperationTime != null) {
            options.startAtOperationTime = this.startAtOperationTime;
        }
        return options;
    }
    cacheResumeToken(resumeToken) {
        if (this.bufferedCount() === 0 && this.postBatchResumeToken) {
            this.resumeToken = this.postBatchResumeToken;
        } else {
            this.resumeToken = resumeToken;
        }
        this.hasReceived = true;
    }
    _processBatch(response) {
        const { postBatchResumeToken } = response;
        if (postBatchResumeToken) {
            this.postBatchResumeToken = postBatchResumeToken;
            if (response.batchSize === 0) {
                this.resumeToken = postBatchResumeToken;
            }
        }
    }
    clone() {
        return new ChangeStreamCursor(this.client, this.namespace, this.pipeline, {
            ...this.cursorOptions
        });
    }
    async _initialize(session) {
        const aggregateOperation = new aggregate_1.AggregateOperation(this.namespace, this.pipeline, {
            ...this.cursorOptions,
            ...this.changeStreamCursorOptions,
            session
        });
        const response = await (0, execute_operation_1.executeOperation)(session.client, aggregateOperation, this.timeoutContext);
        const server = aggregateOperation.server;
        this.maxWireVersion = (0, utils_1.maxWireVersion)(server);
        if (this.startAtOperationTime == null && this.changeStreamCursorOptions.resumeAfter == null && this.changeStreamCursorOptions.startAfter == null) {
            this.startAtOperationTime = response.operationTime;
        }
        this._processBatch(response);
        this.emit(constants_1.INIT, response);
        this.emit(constants_1.RESPONSE);
        return {
            server,
            session,
            response
        };
    }
    async getMore() {
        const response = await super.getMore();
        this.maxWireVersion = (0, utils_1.maxWireVersion)(this.server);
        this._processBatch(response);
        this.emit(change_stream_1.ChangeStream.MORE, response);
        this.emit(change_stream_1.ChangeStream.RESPONSE);
        return response;
    }
}
exports.ChangeStreamCursor = ChangeStreamCursor; //# sourceMappingURL=change_stream_cursor.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/list_collections_cursor.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ListCollectionsCursor = void 0;
const execute_operation_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/execute_operation.js [app-client] (ecmascript)");
const list_collections_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/list_collections.js [app-client] (ecmascript)");
const abstract_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/abstract_cursor.js [app-client] (ecmascript)");
/** @public */ class ListCollectionsCursor extends abstract_cursor_1.AbstractCursor {
    constructor(db, filter, options){
        super(db.client, db.s.namespace, options);
        this.parent = db;
        this.filter = filter;
        this.options = options;
    }
    clone() {
        return new ListCollectionsCursor(this.parent, this.filter, {
            ...this.options,
            ...this.cursorOptions
        });
    }
    /** @internal */ async _initialize(session) {
        const operation = new list_collections_1.ListCollectionsOperation(this.parent, this.filter, {
            ...this.cursorOptions,
            ...this.options,
            session,
            signal: this.signal
        });
        const response = await (0, execute_operation_1.executeOperation)(this.parent.client, operation, this.timeoutContext);
        return {
            server: operation.server,
            session,
            response
        };
    }
}
exports.ListCollectionsCursor = ListCollectionsCursor; //# sourceMappingURL=list_collections_cursor.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/run_command_cursor.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.RunCommandCursor = void 0;
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
const execute_operation_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/execute_operation.js [app-client] (ecmascript)");
const get_more_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/get_more.js [app-client] (ecmascript)");
const run_command_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/run_command.js [app-client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/utils.js [app-client] (ecmascript)");
const abstract_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/abstract_cursor.js [app-client] (ecmascript)");
/** @public */ class RunCommandCursor extends abstract_cursor_1.AbstractCursor {
    /**
     * Controls the `getMore.comment` field
     * @param comment - any BSON value
     */ setComment(comment) {
        this.getMoreOptions.comment = comment;
        return this;
    }
    /**
     * Controls the `getMore.maxTimeMS` field. Only valid when cursor is tailable await
     * @param maxTimeMS - the number of milliseconds to wait for new data
     */ setMaxTimeMS(maxTimeMS) {
        this.getMoreOptions.maxAwaitTimeMS = maxTimeMS;
        return this;
    }
    /**
     * Controls the `getMore.batchSize` field
     * @param batchSize - the number documents to return in the `nextBatch`
     */ setBatchSize(batchSize) {
        this.getMoreOptions.batchSize = batchSize;
        return this;
    }
    /** Unsupported for RunCommandCursor */ clone() {
        throw new error_1.MongoAPIError('Clone not supported, create a new cursor with db.runCursorCommand');
    }
    /** Unsupported for RunCommandCursor: readConcern must be configured directly on command document */ withReadConcern(_) {
        throw new error_1.MongoAPIError('RunCommandCursor does not support readConcern it must be attached to the command being run');
    }
    /** Unsupported for RunCommandCursor: various cursor flags must be configured directly on command document */ addCursorFlag(_, __) {
        throw new error_1.MongoAPIError('RunCommandCursor does not support cursor flags, they must be attached to the command being run');
    }
    /**
     * Unsupported for RunCommandCursor: maxTimeMS must be configured directly on command document
     */ maxTimeMS(_) {
        throw new error_1.MongoAPIError('maxTimeMS must be configured on the command document directly, to configure getMore.maxTimeMS use cursor.setMaxTimeMS()');
    }
    /** Unsupported for RunCommandCursor: batchSize must be configured directly on command document */ batchSize(_) {
        throw new error_1.MongoAPIError('batchSize must be configured on the command document directly, to configure getMore.batchSize use cursor.setBatchSize()');
    }
    /** @internal */ constructor(db, command, options = {}){
        super(db.client, (0, utils_1.ns)(db.namespace), options);
        this.getMoreOptions = {};
        this.db = db;
        this.command = Object.freeze({
            ...command
        });
    }
    /** @internal */ async _initialize(session) {
        const operation = new run_command_1.RunCursorCommandOperation(this.db.s.namespace, this.command, {
            ...this.cursorOptions,
            session: session,
            readPreference: this.cursorOptions.readPreference
        });
        const response = await (0, execute_operation_1.executeOperation)(this.client, operation, this.timeoutContext);
        return {
            server: operation.server,
            session,
            response
        };
    }
    /** @internal */ async getMore() {
        if (!this.session) {
            throw new error_1.MongoRuntimeError('Unexpected null session. A cursor creating command should have set this');
        }
        // eslint-disable-next-line @typescript-eslint/no-non-null-assertion
        const getMoreOperation = new get_more_1.GetMoreOperation(this.namespace, this.id, this.server, {
            ...this.cursorOptions,
            session: this.session,
            ...this.getMoreOptions
        });
        return await (0, execute_operation_1.executeOperation)(this.client, getMoreOperation, this.timeoutContext);
    }
}
exports.RunCommandCursor = RunCommandCursor; //# sourceMappingURL=run_command_cursor.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/client_bulk_write_cursor.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ClientBulkWriteCursor = void 0;
const client_bulk_write_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/client_bulk_write/client_bulk_write.js [app-client] (ecmascript)");
const execute_operation_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/execute_operation.js [app-client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/utils.js [app-client] (ecmascript)");
const abstract_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/abstract_cursor.js [app-client] (ecmascript)");
/**
 * This is the cursor that handles client bulk write operations. Note this is never
 * exposed directly to the user and is always immediately exhausted.
 * @internal
 */ class ClientBulkWriteCursor extends abstract_cursor_1.AbstractCursor {
    /** @internal */ constructor(client, commandBuilder, options = {}){
        super(client, new utils_1.MongoDBNamespace('admin', '$cmd'), options);
        this.commandBuilder = commandBuilder;
        this.clientBulkWriteOptions = options;
    }
    /**
     * We need a way to get the top level cursor response fields for
     * generating the bulk write result, so we expose this here.
     */ get response() {
        if (this.cursorResponse) return this.cursorResponse;
        return null;
    }
    get operations() {
        return this.commandBuilder.lastOperations;
    }
    clone() {
        const clonedOptions = (0, utils_1.mergeOptions)({}, this.clientBulkWriteOptions);
        delete clonedOptions.session;
        return new ClientBulkWriteCursor(this.client, this.commandBuilder, {
            ...clonedOptions
        });
    }
    /** @internal */ async _initialize(session) {
        const clientBulkWriteOperation = new client_bulk_write_1.ClientBulkWriteOperation(this.commandBuilder, {
            ...this.clientBulkWriteOptions,
            ...this.cursorOptions,
            session
        });
        const response = await (0, execute_operation_1.executeOperation)(this.client, clientBulkWriteOperation, this.timeoutContext);
        this.cursorResponse = response;
        return {
            server: clientBulkWriteOperation.server,
            session,
            response
        };
    }
}
exports.ClientBulkWriteCursor = ClientBulkWriteCursor; //# sourceMappingURL=client_bulk_write_cursor.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/collection.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.Collection = void 0;
const bson_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bson.js [app-client] (ecmascript)");
const ordered_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bulk/ordered.js [app-client] (ecmascript)");
const unordered_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bulk/unordered.js [app-client] (ecmascript)");
const change_stream_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/change_stream.js [app-client] (ecmascript)");
const aggregation_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/aggregation_cursor.js [app-client] (ecmascript)");
const find_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/find_cursor.js [app-client] (ecmascript)");
const list_indexes_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/list_indexes_cursor.js [app-client] (ecmascript)");
const list_search_indexes_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/list_search_indexes_cursor.js [app-client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
const count_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/count.js [app-client] (ecmascript)");
const delete_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/delete.js [app-client] (ecmascript)");
const distinct_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/distinct.js [app-client] (ecmascript)");
const estimated_document_count_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/estimated_document_count.js [app-client] (ecmascript)");
const execute_operation_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/execute_operation.js [app-client] (ecmascript)");
const find_and_modify_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/find_and_modify.js [app-client] (ecmascript)");
const indexes_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/indexes.js [app-client] (ecmascript)");
const insert_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/insert.js [app-client] (ecmascript)");
const rename_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/rename.js [app-client] (ecmascript)");
const create_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/search_indexes/create.js [app-client] (ecmascript)");
const drop_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/search_indexes/drop.js [app-client] (ecmascript)");
const update_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/search_indexes/update.js [app-client] (ecmascript)");
const update_2 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/update.js [app-client] (ecmascript)");
const read_concern_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/read_concern.js [app-client] (ecmascript)");
const read_preference_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/read_preference.js [app-client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/utils.js [app-client] (ecmascript)");
const write_concern_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/write_concern.js [app-client] (ecmascript)");
/**
 * The **Collection** class is an internal class that embodies a MongoDB collection
 * allowing for insert/find/update/delete and other command operation on that MongoDB collection.
 *
 * **COLLECTION Cannot directly be instantiated**
 * @public
 *
 * @example
 * ```ts
 * import { MongoClient } from 'mongodb';
 *
 * interface Pet {
 *   name: string;
 *   kind: 'dog' | 'cat' | 'fish';
 * }
 *
 * const client = new MongoClient('mongodb://localhost:27017');
 * const pets = client.db().collection<Pet>('pets');
 *
 * const petCursor = pets.find();
 *
 * for await (const pet of petCursor) {
 *   console.log(`${pet.name} is a ${pet.kind}!`);
 * }
 * ```
 */ class Collection {
    /**
     * Create a new Collection instance
     * @internal
     */ constructor(db, name, options){
        this.db = db;
        // Internal state
        this.s = {
            db,
            options,
            namespace: new utils_1.MongoDBCollectionNamespace(db.databaseName, name),
            pkFactory: db.options?.pkFactory ?? utils_1.DEFAULT_PK_FACTORY,
            readPreference: read_preference_1.ReadPreference.fromOptions(options),
            bsonOptions: (0, bson_1.resolveBSONOptions)(options, db),
            readConcern: read_concern_1.ReadConcern.fromOptions(options),
            writeConcern: write_concern_1.WriteConcern.fromOptions(options)
        };
        this.client = db.client;
    }
    /**
     * The name of the database this collection belongs to
     */ get dbName() {
        return this.s.namespace.db;
    }
    /**
     * The name of this collection
     */ get collectionName() {
        return this.s.namespace.collection;
    }
    /**
     * The namespace of this collection, in the format `${this.dbName}.${this.collectionName}`
     */ get namespace() {
        return this.fullNamespace.toString();
    }
    /**
     *  @internal
     *
     * The `MongoDBNamespace` for the collection.
     */ get fullNamespace() {
        return this.s.namespace;
    }
    /**
     * The current readConcern of the collection. If not explicitly defined for
     * this collection, will be inherited from the parent DB
     */ get readConcern() {
        if (this.s.readConcern == null) {
            return this.db.readConcern;
        }
        return this.s.readConcern;
    }
    /**
     * The current readPreference of the collection. If not explicitly defined for
     * this collection, will be inherited from the parent DB
     */ get readPreference() {
        if (this.s.readPreference == null) {
            return this.db.readPreference;
        }
        return this.s.readPreference;
    }
    get bsonOptions() {
        return this.s.bsonOptions;
    }
    /**
     * The current writeConcern of the collection. If not explicitly defined for
     * this collection, will be inherited from the parent DB
     */ get writeConcern() {
        if (this.s.writeConcern == null) {
            return this.db.writeConcern;
        }
        return this.s.writeConcern;
    }
    /** The current index hint for the collection */ get hint() {
        return this.s.collectionHint;
    }
    set hint(v) {
        this.s.collectionHint = (0, utils_1.normalizeHintField)(v);
    }
    get timeoutMS() {
        return this.s.options.timeoutMS;
    }
    /**
     * Inserts a single document into MongoDB. If documents passed in do not contain the **_id** field,
     * one will be added to each of the documents missing it by the driver, mutating the document. This behavior
     * can be overridden by setting the **forceServerObjectId** flag.
     *
     * @param doc - The document to insert
     * @param options - Optional settings for the command
     */ async insertOne(doc, options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new insert_1.InsertOneOperation(this, doc, (0, utils_1.resolveOptions)(this, options)));
    }
    /**
     * Inserts an array of documents into MongoDB. If documents passed in do not contain the **_id** field,
     * one will be added to each of the documents missing it by the driver, mutating the document. This behavior
     * can be overridden by setting the **forceServerObjectId** flag.
     *
     * @param docs - The documents to insert
     * @param options - Optional settings for the command
     */ async insertMany(docs, options) {
        if (!Array.isArray(docs)) {
            throw new error_1.MongoInvalidArgumentError('Argument "docs" must be an array of documents');
        }
        options = (0, utils_1.resolveOptions)(this, options ?? {});
        const acknowledged = write_concern_1.WriteConcern.fromOptions(options)?.w !== 0;
        try {
            const res = await this.bulkWrite(docs.map((doc)=>({
                    insertOne: {
                        document: doc
                    }
                })), options);
            return {
                acknowledged,
                insertedCount: res.insertedCount,
                insertedIds: res.insertedIds
            };
        } catch (err) {
            if (err && err.message === 'Operation must be an object with an operation key') {
                throw new error_1.MongoInvalidArgumentError('Collection.insertMany() cannot be called with an array that has null/undefined values');
            }
            throw err;
        }
    }
    /**
     * Perform a bulkWrite operation without a fluent API
     *
     * Legal operation types are
     * - `insertOne`
     * - `replaceOne`
     * - `updateOne`
     * - `updateMany`
     * - `deleteOne`
     * - `deleteMany`
     *
     * If documents passed in do not contain the **_id** field,
     * one will be added to each of the documents missing it by the driver, mutating the document. This behavior
     * can be overridden by setting the **forceServerObjectId** flag.
     *
     * @param operations - Bulk operations to perform
     * @param options - Optional settings for the command
     * @throws MongoDriverError if operations is not an array
     */ async bulkWrite(operations, options) {
        if (!Array.isArray(operations)) {
            throw new error_1.MongoInvalidArgumentError('Argument "operations" must be an array of documents');
        }
        options = (0, utils_1.resolveOptions)(this, options ?? {});
        // TODO(NODE-7071): remove once the client doesn't need to be connected to construct
        // bulk operations
        const isConnected = this.client.topology != null;
        if (!isConnected) {
            await (0, execute_operation_1.autoConnect)(this.client);
        }
        // Create the bulk operation
        const bulk = options.ordered === false ? this.initializeUnorderedBulkOp(options) : this.initializeOrderedBulkOp(options);
        // for each op go through and add to the bulk
        for (const operation of operations){
            bulk.raw(operation);
        }
        // Execute the bulk
        return await bulk.execute({
            ...options
        });
    }
    /**
     * Update a single document in a collection
     *
     * The value of `update` can be either:
     * - UpdateFilter<TSchema> - A document that contains update operator expressions,
     * - Document[] - an aggregation pipeline.
     *
     * @param filter - The filter used to select the document to update
     * @param update - The modifications to apply
     * @param options - Optional settings for the command
     */ async updateOne(filter, update, options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new update_2.UpdateOneOperation(this.s.namespace, filter, update, (0, utils_1.resolveOptions)(this, options)));
    }
    /**
     * Replace a document in a collection with another document
     *
     * @param filter - The filter used to select the document to replace
     * @param replacement - The Document that replaces the matching document
     * @param options - Optional settings for the command
     */ async replaceOne(filter, replacement, options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new update_2.ReplaceOneOperation(this.s.namespace, filter, replacement, (0, utils_1.resolveOptions)(this, options)));
    }
    /**
     * Update multiple documents in a collection
     *
     * The value of `update` can be either:
     * - UpdateFilter<TSchema> - A document that contains update operator expressions,
     * - Document[] - an aggregation pipeline.
     *
     * @param filter - The filter used to select the document to update
     * @param update - The modifications to apply
     * @param options - Optional settings for the command
     */ async updateMany(filter, update, options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new update_2.UpdateManyOperation(this.s.namespace, filter, update, (0, utils_1.resolveOptions)(this, options)));
    }
    /**
     * Delete a document from a collection
     *
     * @param filter - The filter used to select the document to remove
     * @param options - Optional settings for the command
     */ async deleteOne(filter = {}, options = {}) {
        return await (0, execute_operation_1.executeOperation)(this.client, new delete_1.DeleteOneOperation(this.s.namespace, filter, (0, utils_1.resolveOptions)(this, options)));
    }
    /**
     * Delete multiple documents from a collection
     *
     * @param filter - The filter used to select the documents to remove
     * @param options - Optional settings for the command
     */ async deleteMany(filter = {}, options = {}) {
        return await (0, execute_operation_1.executeOperation)(this.client, new delete_1.DeleteManyOperation(this.s.namespace, filter, (0, utils_1.resolveOptions)(this, options)));
    }
    /**
     * Rename the collection.
     *
     * @remarks
     * This operation does not inherit options from the Db or MongoClient.
     *
     * @param newName - New name of of the collection.
     * @param options - Optional settings for the command
     */ async rename(newName, options) {
        // Intentionally, we do not inherit options from parent for this operation.
        return await (0, execute_operation_1.executeOperation)(this.client, new rename_1.RenameOperation(this, newName, (0, utils_1.resolveOptions)(undefined, {
            ...options,
            readPreference: read_preference_1.ReadPreference.PRIMARY
        })));
    }
    /**
     * Drop the collection from the database, removing it permanently. New accesses will create a new collection.
     *
     * @param options - Optional settings for the command
     */ async drop(options) {
        return await this.db.dropCollection(this.collectionName, options);
    }
    async findOne(filter = {}, options = {}) {
        // Explicitly set the limit to 1 and singleBatch to true for all commands, per the spec.
        // noCursorTimeout must be unset as well as batchSize.
        // See: https://github.com/mongodb/specifications/blob/master/source/crud/crud.md#findone-api-details
        const { ...opts } = options;
        opts.singleBatch = true;
        const cursor = this.find(filter, opts).limit(1);
        const result = await cursor.next();
        await cursor.close();
        return result;
    }
    find(filter = {}, options = {}) {
        return new find_cursor_1.FindCursor(this.client, this.s.namespace, filter, (0, utils_1.resolveOptions)(this, options));
    }
    /**
     * Returns the options of the collection.
     *
     * @param options - Optional settings for the command
     */ async options(options) {
        options = (0, utils_1.resolveOptions)(this, options);
        const [collection] = await this.db.listCollections({
            name: this.collectionName
        }, {
            ...options,
            nameOnly: false
        }).toArray();
        if (collection == null || collection.options == null) {
            throw new error_1.MongoAPIError(`collection ${this.namespace} not found`);
        }
        return collection.options;
    }
    /**
     * Returns if the collection is a capped collection
     *
     * @param options - Optional settings for the command
     */ async isCapped(options) {
        const { capped } = await this.options(options);
        return Boolean(capped);
    }
    /**
     * Creates an index on the db and collection collection.
     *
     * @param indexSpec - The field name or index specification to create an index for
     * @param options - Optional settings for the command
     *
     * @example
     * ```ts
     * const collection = client.db('foo').collection('bar');
     *
     * await collection.createIndex({ a: 1, b: -1 });
     *
     * // Alternate syntax for { c: 1, d: -1 } that ensures order of indexes
     * await collection.createIndex([ [c, 1], [d, -1] ]);
     *
     * // Equivalent to { e: 1 }
     * await collection.createIndex('e');
     *
     * // Equivalent to { f: 1, g: 1 }
     * await collection.createIndex(['f', 'g'])
     *
     * // Equivalent to { h: 1, i: -1 }
     * await collection.createIndex([ { h: 1 }, { i: -1 } ]);
     *
     * // Equivalent to { j: 1, k: -1, l: 2d }
     * await collection.createIndex(['j', ['k', -1], { l: '2d' }])
     * ```
     */ async createIndex(indexSpec, options) {
        const indexes = await (0, execute_operation_1.executeOperation)(this.client, indexes_1.CreateIndexesOperation.fromIndexSpecification(this, this.collectionName, indexSpec, (0, utils_1.resolveOptions)(this, options)));
        return indexes[0];
    }
    /**
     * Creates multiple indexes in the collection, this method is only supported for
     * MongoDB 2.6 or higher. Earlier version of MongoDB will throw a command not supported
     * error.
     *
     * **Note**: Unlike {@link Collection#createIndex| createIndex}, this function takes in raw index specifications.
     * Index specifications are defined {@link https://www.mongodb.com/docs/manual/reference/command/createIndexes/| here}.
     *
     * @param indexSpecs - An array of index specifications to be created
     * @param options - Optional settings for the command
     *
     * @example
     * ```ts
     * const collection = client.db('foo').collection('bar');
     * await collection.createIndexes([
     *   // Simple index on field fizz
     *   {
     *     key: { fizz: 1 },
     *   }
     *   // wildcard index
     *   {
     *     key: { '$**': 1 }
     *   },
     *   // named index on darmok and jalad
     *   {
     *     key: { darmok: 1, jalad: -1 }
     *     name: 'tanagra'
     *   }
     * ]);
     * ```
     */ async createIndexes(indexSpecs, options) {
        return await (0, execute_operation_1.executeOperation)(this.client, indexes_1.CreateIndexesOperation.fromIndexDescriptionArray(this, this.collectionName, indexSpecs, (0, utils_1.resolveOptions)(this, {
            ...options,
            maxTimeMS: undefined
        })));
    }
    /**
     * Drops an index from this collection.
     *
     * @param indexName - Name of the index to drop.
     * @param options - Optional settings for the command
     */ async dropIndex(indexName, options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new indexes_1.DropIndexOperation(this, indexName, {
            ...(0, utils_1.resolveOptions)(this, options),
            readPreference: read_preference_1.ReadPreference.primary
        }));
    }
    /**
     * Drops all indexes from this collection.
     *
     * @param options - Optional settings for the command
     */ async dropIndexes(options) {
        try {
            await (0, execute_operation_1.executeOperation)(this.client, new indexes_1.DropIndexOperation(this, '*', (0, utils_1.resolveOptions)(this, options)));
            return true;
        } catch (error) {
            // TODO(NODE-6517): Driver should only filter for namespace not found error. Other errors should be thrown.
            if (error instanceof error_1.MongoOperationTimeoutError) throw error;
            return false;
        }
    }
    /**
     * Get the list of all indexes information for the collection.
     *
     * @param options - Optional settings for the command
     */ listIndexes(options) {
        return new list_indexes_cursor_1.ListIndexesCursor(this, (0, utils_1.resolveOptions)(this, options));
    }
    /**
     * Checks if one or more indexes exist on the collection, fails on first non-existing index
     *
     * @param indexes - One or more index names to check.
     * @param options - Optional settings for the command
     */ async indexExists(indexes, options) {
        const indexNames = Array.isArray(indexes) ? indexes : [
            indexes
        ];
        const allIndexes = new Set(await this.listIndexes(options).map(({ name })=>name).toArray());
        return indexNames.every((name)=>allIndexes.has(name));
    }
    async indexInformation(options) {
        return await this.indexes({
            ...options,
            full: options?.full ?? false
        });
    }
    /**
     * Gets an estimate of the count of documents in a collection using collection metadata.
     * This will always run a count command on all server versions.
     *
     * due to an oversight in versions 5.0.0-5.0.8 of MongoDB, the count command,
     * which estimatedDocumentCount uses in its implementation, was not included in v1 of
     * the Stable API, and so users of the Stable API with estimatedDocumentCount are
     * recommended to upgrade their server version to 5.0.9+ or set apiStrict: false to avoid
     * encountering errors.
     *
     * @see {@link https://www.mongodb.com/docs/manual/reference/command/count/#behavior|Count: Behavior}
     * @param options - Optional settings for the command
     */ async estimatedDocumentCount(options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new estimated_document_count_1.EstimatedDocumentCountOperation(this, (0, utils_1.resolveOptions)(this, options)));
    }
    /**
     * Gets the number of documents matching the filter.
     * For a fast count of the total documents in a collection see {@link Collection#estimatedDocumentCount| estimatedDocumentCount}.
     *
     * Due to countDocuments using the $match aggregation pipeline stage, certain query operators cannot be used in countDocuments. This includes the $where and $near query operators, among others. Details can be found in the documentation for the $match aggregation pipeline stage.
     *
     * **Note**: When migrating from {@link Collection#count| count} to {@link Collection#countDocuments| countDocuments}
     * the following query operators must be replaced:
     *
     * | Operator | Replacement |
     * | -------- | ----------- |
     * | `$where`   | [`$expr`][1] |
     * | `$near`    | [`$geoWithin`][2] with [`$center`][3] |
     * | `$nearSphere` | [`$geoWithin`][2] with [`$centerSphere`][4] |
     *
     * [1]: https://www.mongodb.com/docs/manual/reference/operator/query/expr/
     * [2]: https://www.mongodb.com/docs/manual/reference/operator/query/geoWithin/
     * [3]: https://www.mongodb.com/docs/manual/reference/operator/query/center/#op._S_center
     * [4]: https://www.mongodb.com/docs/manual/reference/operator/query/centerSphere/#op._S_centerSphere
     *
     * @param filter - The filter for the count
     * @param options - Optional settings for the command
     *
     * @see https://www.mongodb.com/docs/manual/reference/operator/query/expr/
     * @see https://www.mongodb.com/docs/manual/reference/operator/query/geoWithin/
     * @see https://www.mongodb.com/docs/manual/reference/operator/query/center/#op._S_center
     * @see https://www.mongodb.com/docs/manual/reference/operator/query/centerSphere/#op._S_centerSphere
     */ async countDocuments(filter = {}, options = {}) {
        const pipeline = [];
        pipeline.push({
            $match: filter
        });
        if (typeof options.skip === 'number') {
            pipeline.push({
                $skip: options.skip
            });
        }
        if (typeof options.limit === 'number') {
            pipeline.push({
                $limit: options.limit
            });
        }
        pipeline.push({
            $group: {
                _id: 1,
                n: {
                    $sum: 1
                }
            }
        });
        const cursor = this.aggregate(pipeline, options);
        const doc = await cursor.next();
        await cursor.close();
        return doc?.n ?? 0;
    }
    async distinct(key, filter = {}, options = {}) {
        return await (0, execute_operation_1.executeOperation)(this.client, new distinct_1.DistinctOperation(this, key, filter, (0, utils_1.resolveOptions)(this, options)));
    }
    async indexes(options) {
        const indexes = await this.listIndexes(options).toArray();
        const full = options?.full ?? true;
        if (full) {
            return indexes;
        }
        const object = Object.fromEntries(indexes.map(({ name, key })=>[
                name,
                Object.entries(key)
            ]));
        return object;
    }
    async findOneAndDelete(filter, options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new find_and_modify_1.FindOneAndDeleteOperation(this, filter, (0, utils_1.resolveOptions)(this, options)));
    }
    async findOneAndReplace(filter, replacement, options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new find_and_modify_1.FindOneAndReplaceOperation(this, filter, replacement, (0, utils_1.resolveOptions)(this, options)));
    }
    async findOneAndUpdate(filter, update, options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new find_and_modify_1.FindOneAndUpdateOperation(this, filter, update, (0, utils_1.resolveOptions)(this, options)));
    }
    /**
     * Execute an aggregation framework pipeline against the collection, needs MongoDB \>= 2.2
     *
     * @param pipeline - An array of aggregation pipelines to execute
     * @param options - Optional settings for the command
     */ aggregate(pipeline = [], options) {
        if (!Array.isArray(pipeline)) {
            throw new error_1.MongoInvalidArgumentError('Argument "pipeline" must be an array of aggregation stages');
        }
        return new aggregation_cursor_1.AggregationCursor(this.client, this.s.namespace, pipeline, (0, utils_1.resolveOptions)(this, options));
    }
    /**
     * Create a new Change Stream, watching for new changes (insertions, updates, replacements, deletions, and invalidations) in this collection.
     *
     * @remarks
     * watch() accepts two generic arguments for distinct use cases:
     * - The first is to override the schema that may be defined for this specific collection
     * - The second is to override the shape of the change stream document entirely, if it is not provided the type will default to ChangeStreamDocument of the first argument
     * @example
     * By just providing the first argument I can type the change to be `ChangeStreamDocument<{ _id: number }>`
     * ```ts
     * collection.watch<{ _id: number }>()
     *   .on('change', change => console.log(change._id.toFixed(4)));
     * ```
     *
     * @example
     * Passing a second argument provides a way to reflect the type changes caused by an advanced pipeline.
     * Here, we are using a pipeline to have MongoDB filter for insert changes only and add a comment.
     * No need start from scratch on the ChangeStreamInsertDocument type!
     * By using an intersection we can save time and ensure defaults remain the same type!
     * ```ts
     * collection
     *   .watch<Schema, ChangeStreamInsertDocument<Schema> & { comment: string }>([
     *     { $addFields: { comment: 'big changes' } },
     *     { $match: { operationType: 'insert' } }
     *   ])
     *   .on('change', change => {
     *     change.comment.startsWith('big');
     *     change.operationType === 'insert';
     *     // No need to narrow in code because the generics did that for us!
     *     expectType<Schema>(change.fullDocument);
     *   });
     * ```
     *
     * @remarks
     * When `timeoutMS` is configured for a change stream, it will have different behaviour depending
     * on whether the change stream is in iterator mode or emitter mode. In both cases, a change
     * stream will time out if it does not receive a change event within `timeoutMS` of the last change
     * event.
     *
     * Note that if a change stream is consistently timing out when watching a collection, database or
     * client that is being changed, then this may be due to the server timing out before it can finish
     * processing the existing oplog. To address this, restart the change stream with a higher
     * `timeoutMS`.
     *
     * If the change stream times out the initial aggregate operation to establish the change stream on
     * the server, then the client will close the change stream. If the getMore calls to the server
     * time out, then the change stream will be left open, but will throw a MongoOperationTimeoutError
     * when in iterator mode and emit an error event that returns a MongoOperationTimeoutError in
     * emitter mode.
     *
     * To determine whether or not the change stream is still open following a timeout, check the
     * {@link ChangeStream.closed} getter.
     *
     * @example
     * In iterator mode, if a next() call throws a timeout error, it will attempt to resume the change stream.
     * The next call can just be retried after this succeeds.
     * ```ts
     * const changeStream = collection.watch([], { timeoutMS: 100 });
     * try {
     *     await changeStream.next();
     * } catch (e) {
     *     if (e instanceof MongoOperationTimeoutError && !changeStream.closed) {
     *       await changeStream.next();
     *     }
     *     throw e;
     * }
     * ```
     *
     * @example
     * In emitter mode, if the change stream goes `timeoutMS` without emitting a change event, it will
     * emit an error event that returns a MongoOperationTimeoutError, but will not close the change
     * stream unless the resume attempt fails. There is no need to re-establish change listeners as
     * this will automatically continue emitting change events once the resume attempt completes.
     *
     * ```ts
     * const changeStream = collection.watch([], { timeoutMS: 100 });
     * changeStream.on('change', console.log);
     * changeStream.on('error', e => {
     *     if (e instanceof MongoOperationTimeoutError && !changeStream.closed) {
     *         // do nothing
     *     } else {
     *         changeStream.close();
     *     }
     * });
     * ```
     *
     * @param pipeline - An array of {@link https://www.mongodb.com/docs/manual/reference/operator/aggregation-pipeline/|aggregation pipeline stages} through which to pass change stream documents. This allows for filtering (using $match) and manipulating the change stream documents.
     * @param options - Optional settings for the command
     * @typeParam TLocal - Type of the data being detected by the change stream
     * @typeParam TChange - Type of the whole change stream document emitted
     */ watch(pipeline = [], options = {}) {
        // Allow optionally not specifying a pipeline
        if (!Array.isArray(pipeline)) {
            options = pipeline;
            pipeline = [];
        }
        return new change_stream_1.ChangeStream(this, pipeline, (0, utils_1.resolveOptions)(this, options));
    }
    /**
     * Initiate an Out of order batch write operation. All operations will be buffered into insert/update/remove commands executed out of order.
     *
     * @throws MongoNotConnectedError
     * @remarks
     * **NOTE:** MongoClient must be connected prior to calling this method due to a known limitation in this legacy implementation.
     * However, `collection.bulkWrite()` provides an equivalent API that does not require prior connecting.
     */ initializeUnorderedBulkOp(options) {
        return new unordered_1.UnorderedBulkOperation(this, (0, utils_1.resolveOptions)(this, options));
    }
    /**
     * Initiate an In order bulk write operation. Operations will be serially executed in the order they are added, creating a new operation for each switch in types.
     *
     * @throws MongoNotConnectedError
     * @remarks
     * **NOTE:** MongoClient must be connected prior to calling this method due to a known limitation in this legacy implementation.
     * However, `collection.bulkWrite()` provides an equivalent API that does not require prior connecting.
     */ initializeOrderedBulkOp(options) {
        return new ordered_1.OrderedBulkOperation(this, (0, utils_1.resolveOptions)(this, options));
    }
    /**
     * An estimated count of matching documents in the db to a filter.
     *
     * **NOTE:** This method has been deprecated, since it does not provide an accurate count of the documents
     * in a collection. To obtain an accurate count of documents in the collection, use {@link Collection#countDocuments| countDocuments}.
     * To obtain an estimated count of all documents in the collection, use {@link Collection#estimatedDocumentCount| estimatedDocumentCount}.
     *
     * @deprecated use {@link Collection#countDocuments| countDocuments} or {@link Collection#estimatedDocumentCount| estimatedDocumentCount} instead
     *
     * @param filter - The filter for the count.
     * @param options - Optional settings for the command
     */ async count(filter = {}, options = {}) {
        return await (0, execute_operation_1.executeOperation)(this.client, new count_1.CountOperation(this.fullNamespace, filter, (0, utils_1.resolveOptions)(this, options)));
    }
    listSearchIndexes(indexNameOrOptions, options) {
        options = typeof indexNameOrOptions === 'object' ? indexNameOrOptions : options == null ? {} : options;
        const indexName = indexNameOrOptions == null ? null : typeof indexNameOrOptions === 'object' ? null : indexNameOrOptions;
        return new list_search_indexes_cursor_1.ListSearchIndexesCursor(this, indexName, options);
    }
    /**
     * Creates a single search index for the collection.
     *
     * @param description - The index description for the new search index.
     * @returns A promise that resolves to the name of the new search index.
     *
     * @remarks Only available when used against a 7.0+ Atlas cluster.
     */ async createSearchIndex(description) {
        const [index] = await this.createSearchIndexes([
            description
        ]);
        return index;
    }
    /**
     * Creates multiple search indexes for the current collection.
     *
     * @param descriptions - An array of `SearchIndexDescription`s for the new search indexes.
     * @returns A promise that resolves to an array of the newly created search index names.
     *
     * @remarks Only available when used against a 7.0+ Atlas cluster.
     * @returns
     */ async createSearchIndexes(descriptions) {
        return await (0, execute_operation_1.executeOperation)(this.client, new create_1.CreateSearchIndexesOperation(this, descriptions));
    }
    /**
     * Deletes a search index by index name.
     *
     * @param name - The name of the search index to be deleted.
     *
     * @remarks Only available when used against a 7.0+ Atlas cluster.
     */ async dropSearchIndex(name) {
        return await (0, execute_operation_1.executeOperation)(this.client, new drop_1.DropSearchIndexOperation(this, name));
    }
    /**
     * Updates a search index by replacing the existing index definition with the provided definition.
     *
     * @param name - The name of the search index to update.
     * @param definition - The new search index definition.
     *
     * @remarks Only available when used against a 7.0+ Atlas cluster.
     */ async updateSearchIndex(name, definition) {
        return await (0, execute_operation_1.executeOperation)(this.client, new update_1.UpdateSearchIndexOperation(this, name, definition));
    }
}
exports.Collection = Collection; //# sourceMappingURL=collection.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/db.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.Db = void 0;
const admin_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/admin.js [app-client] (ecmascript)");
const bson_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bson.js [app-client] (ecmascript)");
const change_stream_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/change_stream.js [app-client] (ecmascript)");
const collection_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/collection.js [app-client] (ecmascript)");
const CONSTANTS = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/constants.js [app-client] (ecmascript)");
const aggregation_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/aggregation_cursor.js [app-client] (ecmascript)");
const list_collections_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/list_collections_cursor.js [app-client] (ecmascript)");
const run_command_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/run_command_cursor.js [app-client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
const create_collection_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/create_collection.js [app-client] (ecmascript)");
const drop_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/drop.js [app-client] (ecmascript)");
const execute_operation_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/execute_operation.js [app-client] (ecmascript)");
const indexes_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/indexes.js [app-client] (ecmascript)");
const profiling_level_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/profiling_level.js [app-client] (ecmascript)");
const remove_user_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/remove_user.js [app-client] (ecmascript)");
const rename_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/rename.js [app-client] (ecmascript)");
const run_command_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/run_command.js [app-client] (ecmascript)");
const set_profiling_level_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/set_profiling_level.js [app-client] (ecmascript)");
const stats_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/stats.js [app-client] (ecmascript)");
const read_concern_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/read_concern.js [app-client] (ecmascript)");
const read_preference_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/read_preference.js [app-client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/utils.js [app-client] (ecmascript)");
const write_concern_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/write_concern.js [app-client] (ecmascript)");
// Allowed parameters
const DB_OPTIONS_ALLOW_LIST = [
    'writeConcern',
    'readPreference',
    'readPreferenceTags',
    'native_parser',
    'forceServerObjectId',
    'pkFactory',
    'serializeFunctions',
    'raw',
    'authSource',
    'ignoreUndefined',
    'readConcern',
    'retryMiliSeconds',
    'numberOfRetries',
    'useBigInt64',
    'promoteBuffers',
    'promoteLongs',
    'bsonRegExp',
    'enableUtf8Validation',
    'promoteValues',
    'compression',
    'retryWrites',
    'timeoutMS'
];
/**
 * The **Db** class is a class that represents a MongoDB Database.
 * @public
 *
 * @example
 * ```ts
 * import { MongoClient } from 'mongodb';
 *
 * interface Pet {
 *   name: string;
 *   kind: 'dog' | 'cat' | 'fish';
 * }
 *
 * const client = new MongoClient('mongodb://localhost:27017');
 * const db = client.db();
 *
 * // Create a collection that validates our union
 * await db.createCollection<Pet>('pets', {
 *   validator: { $expr: { $in: ['$kind', ['dog', 'cat', 'fish']] } }
 * })
 * ```
 */ class Db {
    static{
        this.SYSTEM_NAMESPACE_COLLECTION = CONSTANTS.SYSTEM_NAMESPACE_COLLECTION;
    }
    static{
        this.SYSTEM_INDEX_COLLECTION = CONSTANTS.SYSTEM_INDEX_COLLECTION;
    }
    static{
        this.SYSTEM_PROFILE_COLLECTION = CONSTANTS.SYSTEM_PROFILE_COLLECTION;
    }
    static{
        this.SYSTEM_USER_COLLECTION = CONSTANTS.SYSTEM_USER_COLLECTION;
    }
    static{
        this.SYSTEM_COMMAND_COLLECTION = CONSTANTS.SYSTEM_COMMAND_COLLECTION;
    }
    static{
        this.SYSTEM_JS_COLLECTION = CONSTANTS.SYSTEM_JS_COLLECTION;
    }
    /**
     * Creates a new Db instance.
     *
     * Db name cannot contain a dot, the server may apply more restrictions when an operation is run.
     *
     * @param client - The MongoClient for the database.
     * @param databaseName - The name of the database this instance represents.
     * @param options - Optional settings for Db construction.
     */ constructor(client, databaseName, options){
        options = options ?? {};
        // Filter the options
        options = (0, utils_1.filterOptions)(options, DB_OPTIONS_ALLOW_LIST);
        // Ensure there are no dots in database name
        if (typeof databaseName === 'string' && databaseName.includes('.')) {
            throw new error_1.MongoInvalidArgumentError(`Database names cannot contain the character '.'`);
        }
        // Internal state of the db object
        this.s = {
            // Options
            options,
            // Unpack read preference
            readPreference: read_preference_1.ReadPreference.fromOptions(options),
            // Merge bson options
            bsonOptions: (0, bson_1.resolveBSONOptions)(options, client),
            // Set up the primary key factory or fallback to ObjectId
            pkFactory: options?.pkFactory ?? utils_1.DEFAULT_PK_FACTORY,
            // ReadConcern
            readConcern: read_concern_1.ReadConcern.fromOptions(options),
            writeConcern: write_concern_1.WriteConcern.fromOptions(options),
            // Namespace
            namespace: new utils_1.MongoDBNamespace(databaseName)
        };
        this.client = client;
    }
    get databaseName() {
        return this.s.namespace.db;
    }
    // Options
    get options() {
        return this.s.options;
    }
    /**
     * Check if a secondary can be used (because the read preference is *not* set to primary)
     */ get secondaryOk() {
        return this.s.readPreference?.preference !== 'primary' || false;
    }
    get readConcern() {
        return this.s.readConcern;
    }
    /**
     * The current readPreference of the Db. If not explicitly defined for
     * this Db, will be inherited from the parent MongoClient
     */ get readPreference() {
        if (this.s.readPreference == null) {
            return this.client.readPreference;
        }
        return this.s.readPreference;
    }
    get bsonOptions() {
        return this.s.bsonOptions;
    }
    // get the write Concern
    get writeConcern() {
        return this.s.writeConcern;
    }
    get namespace() {
        return this.s.namespace.toString();
    }
    get timeoutMS() {
        return this.s.options?.timeoutMS;
    }
    /**
     * Create a new collection on a server with the specified options. Use this to create capped collections.
     * More information about command options available at https://www.mongodb.com/docs/manual/reference/command/create/
     *
     * Collection namespace validation is performed server-side.
     *
     * @param name - The name of the collection to create
     * @param options - Optional settings for the command
     */ async createCollection(name, options) {
        options = (0, utils_1.resolveOptions)(this, options);
        return await (0, create_collection_1.createCollections)(this, name, options);
    }
    /**
     * Execute a command
     *
     * @remarks
     * This command does not inherit options from the MongoClient.
     *
     * The driver will ensure the following fields are attached to the command sent to the server:
     * - `lsid` - sourced from an implicit session or options.session
     * - `$readPreference` - defaults to primary or can be configured by options.readPreference
     * - `$db` - sourced from the name of this database
     *
     * If the client has a serverApi setting:
     * - `apiVersion`
     * - `apiStrict`
     * - `apiDeprecationErrors`
     *
     * When in a transaction:
     * - `readConcern` - sourced from readConcern set on the TransactionOptions
     * - `writeConcern` - sourced from writeConcern set on the TransactionOptions
     *
     * Attaching any of the above fields to the command will have no effect as the driver will overwrite the value.
     *
     * @param command - The command to run
     * @param options - Optional settings for the command
     */ async command(command, options) {
        // Intentionally, we do not inherit options from parent for this operation.
        return await (0, execute_operation_1.executeOperation)(this.client, new run_command_1.RunCommandOperation(this.s.namespace, command, (0, utils_1.resolveOptions)(undefined, {
            ...(0, bson_1.resolveBSONOptions)(options),
            timeoutMS: options?.timeoutMS ?? this.timeoutMS,
            session: options?.session,
            readPreference: options?.readPreference,
            signal: options?.signal
        })));
    }
    /**
     * Execute an aggregation framework pipeline against the database.
     *
     * @param pipeline - An array of aggregation stages to be executed
     * @param options - Optional settings for the command
     */ aggregate(pipeline = [], options) {
        return new aggregation_cursor_1.AggregationCursor(this.client, this.s.namespace, pipeline, (0, utils_1.resolveOptions)(this, options));
    }
    /** Return the Admin db instance */ admin() {
        return new admin_1.Admin(this);
    }
    /**
     * Returns a reference to a MongoDB Collection. If it does not exist it will be created implicitly.
     *
     * Collection namespace validation is performed server-side.
     *
     * @param name - the collection name we wish to access.
     * @returns return the new Collection instance
     */ collection(name, options = {}) {
        if (typeof options === 'function') {
            throw new error_1.MongoInvalidArgumentError('The callback form of this helper has been removed.');
        }
        return new collection_1.Collection(this, name, (0, utils_1.resolveOptions)(this, options));
    }
    /**
     * Get all the db statistics.
     *
     * @param options - Optional settings for the command
     */ async stats(options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new stats_1.DbStatsOperation(this, (0, utils_1.resolveOptions)(this, options)));
    }
    listCollections(filter = {}, options = {}) {
        return new list_collections_cursor_1.ListCollectionsCursor(this, filter, (0, utils_1.resolveOptions)(this, options));
    }
    /**
     * Rename a collection.
     *
     * @remarks
     * This operation does not inherit options from the MongoClient.
     *
     * @param fromCollection - Name of current collection to rename
     * @param toCollection - New name of of the collection
     * @param options - Optional settings for the command
     */ async renameCollection(fromCollection, toCollection, options) {
        // Intentionally, we do not inherit options from parent for this operation.
        return await (0, execute_operation_1.executeOperation)(this.client, new rename_1.RenameOperation(this.collection(fromCollection), toCollection, (0, utils_1.resolveOptions)(undefined, {
            ...options,
            new_collection: true,
            readPreference: read_preference_1.ReadPreference.primary
        })));
    }
    /**
     * Drop a collection from the database, removing it permanently. New accesses will create a new collection.
     *
     * @param name - Name of collection to drop
     * @param options - Optional settings for the command
     */ async dropCollection(name, options) {
        options = (0, utils_1.resolveOptions)(this, options);
        return await (0, drop_1.dropCollections)(this, name, options);
    }
    /**
     * Drop a database, removing it permanently from the server.
     *
     * @param options - Optional settings for the command
     */ async dropDatabase(options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new drop_1.DropDatabaseOperation(this, (0, utils_1.resolveOptions)(this, options)));
    }
    /**
     * Fetch all collections for the current db.
     *
     * @param options - Optional settings for the command
     */ async collections(options) {
        options = (0, utils_1.resolveOptions)(this, options);
        const collections = await this.listCollections({}, {
            ...options,
            nameOnly: true
        }).toArray();
        return collections.filter(// Filter collections removing any illegal ones
        ({ name })=>!name.includes('$')).map(({ name })=>new collection_1.Collection(this, name, this.s.options));
    }
    /**
     * Creates an index on the db and collection.
     *
     * @param name - Name of the collection to create the index on.
     * @param indexSpec - Specify the field to index, or an index specification
     * @param options - Optional settings for the command
     */ async createIndex(name, indexSpec, options) {
        const indexes = await (0, execute_operation_1.executeOperation)(this.client, indexes_1.CreateIndexesOperation.fromIndexSpecification(this, name, indexSpec, options));
        return indexes[0];
    }
    /**
     * Remove a user from a database
     *
     * @param username - The username to remove
     * @param options - Optional settings for the command
     */ async removeUser(username, options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new remove_user_1.RemoveUserOperation(this, username, (0, utils_1.resolveOptions)(this, options)));
    }
    /**
     * Set the current profiling level of MongoDB
     *
     * @param level - The new profiling level (off, slow_only, all).
     * @param options - Optional settings for the command
     */ async setProfilingLevel(level, options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new set_profiling_level_1.SetProfilingLevelOperation(this, level, (0, utils_1.resolveOptions)(this, options)));
    }
    /**
     * Retrieve the current profiling Level for MongoDB
     *
     * @param options - Optional settings for the command
     */ async profilingLevel(options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new profiling_level_1.ProfilingLevelOperation(this, (0, utils_1.resolveOptions)(this, options)));
    }
    async indexInformation(name, options) {
        return await this.collection(name).indexInformation((0, utils_1.resolveOptions)(this, options));
    }
    /**
     * Create a new Change Stream, watching for new changes (insertions, updates,
     * replacements, deletions, and invalidations) in this database. Will ignore all
     * changes to system collections.
     *
     * @remarks
     * watch() accepts two generic arguments for distinct use cases:
     * - The first is to provide the schema that may be defined for all the collections within this database
     * - The second is to override the shape of the change stream document entirely, if it is not provided the type will default to ChangeStreamDocument of the first argument
     *
     * @remarks
     * When `timeoutMS` is configured for a change stream, it will have different behaviour depending
     * on whether the change stream is in iterator mode or emitter mode. In both cases, a change
     * stream will time out if it does not receive a change event within `timeoutMS` of the last change
     * event.
     *
     * Note that if a change stream is consistently timing out when watching a collection, database or
     * client that is being changed, then this may be due to the server timing out before it can finish
     * processing the existing oplog. To address this, restart the change stream with a higher
     * `timeoutMS`.
     *
     * If the change stream times out the initial aggregate operation to establish the change stream on
     * the server, then the client will close the change stream. If the getMore calls to the server
     * time out, then the change stream will be left open, but will throw a MongoOperationTimeoutError
     * when in iterator mode and emit an error event that returns a MongoOperationTimeoutError in
     * emitter mode.
     *
     * To determine whether or not the change stream is still open following a timeout, check the
     * {@link ChangeStream.closed} getter.
     *
     * @example
     * In iterator mode, if a next() call throws a timeout error, it will attempt to resume the change stream.
     * The next call can just be retried after this succeeds.
     * ```ts
     * const changeStream = collection.watch([], { timeoutMS: 100 });
     * try {
     *     await changeStream.next();
     * } catch (e) {
     *     if (e instanceof MongoOperationTimeoutError && !changeStream.closed) {
     *       await changeStream.next();
     *     }
     *     throw e;
     * }
     * ```
     *
     * @example
     * In emitter mode, if the change stream goes `timeoutMS` without emitting a change event, it will
     * emit an error event that returns a MongoOperationTimeoutError, but will not close the change
     * stream unless the resume attempt fails. There is no need to re-establish change listeners as
     * this will automatically continue emitting change events once the resume attempt completes.
     *
     * ```ts
     * const changeStream = collection.watch([], { timeoutMS: 100 });
     * changeStream.on('change', console.log);
     * changeStream.on('error', e => {
     *     if (e instanceof MongoOperationTimeoutError && !changeStream.closed) {
     *         // do nothing
     *     } else {
     *         changeStream.close();
     *     }
     * });
     * ```
     * @param pipeline - An array of {@link https://www.mongodb.com/docs/manual/reference/operator/aggregation-pipeline/|aggregation pipeline stages} through which to pass change stream documents. This allows for filtering (using $match) and manipulating the change stream documents.
     * @param options - Optional settings for the command
     * @typeParam TSchema - Type of the data being detected by the change stream
     * @typeParam TChange - Type of the whole change stream document emitted
     */ watch(pipeline = [], options = {}) {
        // Allow optionally not specifying a pipeline
        if (!Array.isArray(pipeline)) {
            options = pipeline;
            pipeline = [];
        }
        return new change_stream_1.ChangeStream(this, pipeline, (0, utils_1.resolveOptions)(this, options));
    }
    /**
     * A low level cursor API providing basic driver functionality:
     * - ClientSession management
     * - ReadPreference for server selection
     * - Running getMores automatically when a local batch is exhausted
     *
     * @param command - The command that will start a cursor on the server.
     * @param options - Configurations for running the command, bson options will apply to getMores
     */ runCursorCommand(command, options) {
        return new run_command_cursor_1.RunCommandCursor(this, command, options);
    }
}
exports.Db = Db; //# sourceMappingURL=db.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/deps.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.aws4 = void 0;
exports.getKerberos = getKerberos;
exports.getZstdLibrary = getZstdLibrary;
exports.getAwsCredentialProvider = getAwsCredentialProvider;
exports.getGcpMetadata = getGcpMetadata;
exports.getSnappy = getSnappy;
exports.getSocks = getSocks;
exports.getMongoDBClientEncryption = getMongoDBClientEncryption;
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
function makeErrorModule(error) {
    const props = error ? {
        kModuleError: error
    } : {};
    return new Proxy(props, {
        get: (_, key)=>{
            if (key === 'kModuleError') {
                return error;
            }
            throw error;
        },
        set: ()=>{
            throw error;
        }
    });
}
function getKerberos() {
    let kerberos;
    try {
        // Ensure you always wrap an optional require in the try block NODE-3199
        // eslint-disable-next-line @typescript-eslint/no-require-imports
        kerberos = (()=>{
            const e = new Error("Cannot find module 'kerberos'");
            e.code = 'MODULE_NOT_FOUND';
            throw e;
        })();
    } catch (error) {
        kerberos = makeErrorModule(new error_1.MongoMissingDependencyError('Optional module `kerberos` not found. Please install it to enable kerberos authentication', {
            cause: error,
            dependencyName: 'kerberos'
        }));
    }
    return kerberos;
}
function getZstdLibrary() {
    let ZStandard;
    try {
        // eslint-disable-next-line @typescript-eslint/no-require-imports
        ZStandard = (()=>{
            const e = new Error("Cannot find module '@mongodb-js/zstd'");
            e.code = 'MODULE_NOT_FOUND';
            throw e;
        })();
    } catch (error) {
        ZStandard = makeErrorModule(new error_1.MongoMissingDependencyError('Optional module `@mongodb-js/zstd` not found. Please install it to enable zstd compression', {
            cause: error,
            dependencyName: 'zstd'
        }));
    }
    return ZStandard;
}
function getAwsCredentialProvider() {
    try {
        // Ensure you always wrap an optional require in the try block NODE-3199
        // eslint-disable-next-line @typescript-eslint/no-require-imports
        const credentialProvider = (()=>{
            const e = new Error("Cannot find module '@aws-sdk/credential-providers'");
            e.code = 'MODULE_NOT_FOUND';
            throw e;
        })();
        return credentialProvider;
    } catch (error) {
        return makeErrorModule(new error_1.MongoMissingDependencyError('Optional module `@aws-sdk/credential-providers` not found.' + ' Please install it to enable getting aws credentials via the official sdk.', {
            cause: error,
            dependencyName: '@aws-sdk/credential-providers'
        }));
    }
}
function getGcpMetadata() {
    try {
        // Ensure you always wrap an optional require in the try block NODE-3199
        // eslint-disable-next-line @typescript-eslint/no-require-imports
        const credentialProvider = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/gcp-metadata@8.1.2/node_modules/gcp-metadata/build/src/index.js [app-client] (ecmascript)");
        return credentialProvider;
    } catch (error) {
        return makeErrorModule(new error_1.MongoMissingDependencyError('Optional module `gcp-metadata` not found.' + ' Please install it to enable getting gcp credentials via the official sdk.', {
            cause: error,
            dependencyName: 'gcp-metadata'
        }));
    }
}
function getSnappy() {
    try {
        // Ensure you always wrap an optional require in the try block NODE-3199
        // eslint-disable-next-line @typescript-eslint/no-require-imports
        const value = (()=>{
            const e = new Error("Cannot find module 'snappy'");
            e.code = 'MODULE_NOT_FOUND';
            throw e;
        })();
        return value;
    } catch (error) {
        const kModuleError = new error_1.MongoMissingDependencyError('Optional module `snappy` not found. Please install it to enable snappy compression', {
            cause: error,
            dependencyName: 'snappy'
        });
        return {
            kModuleError
        };
    }
}
function getSocks() {
    try {
        // Ensure you always wrap an optional require in the try block NODE-3199
        // eslint-disable-next-line @typescript-eslint/no-require-imports
        const value = (()=>{
            const e = new Error("Cannot find module 'socks'");
            e.code = 'MODULE_NOT_FOUND';
            throw e;
        })();
        return value;
    } catch (error) {
        const kModuleError = new error_1.MongoMissingDependencyError('Optional module `socks` not found. Please install it to connections over a SOCKS5 proxy', {
            cause: error,
            dependencyName: 'socks'
        });
        return {
            kModuleError
        };
    }
}
exports.aws4 = loadAws4();
function loadAws4() {
    let aws4;
    try {
        // eslint-disable-next-line @typescript-eslint/no-require-imports
        aws4 = (()=>{
            const e = new Error("Cannot find module 'aws4'");
            e.code = 'MODULE_NOT_FOUND';
            throw e;
        })();
    } catch (error) {
        aws4 = makeErrorModule(new error_1.MongoMissingDependencyError('Optional module `aws4` not found. Please install it to enable AWS authentication', {
            cause: error,
            dependencyName: 'aws4'
        }));
    }
    return aws4;
}
/** A utility function to get the instance of mongodb-client-encryption, if it exists. */ function getMongoDBClientEncryption() {
    let mongodbClientEncryption = null;
    try {
        // NOTE(NODE-3199): Ensure you always wrap an optional require literally in the try block
        // Cannot be moved to helper utility function, bundlers search and replace the actual require call
        // in a way that makes this line throw at bundle time, not runtime, catching here will make bundling succeed
        // eslint-disable-next-line @typescript-eslint/no-require-imports
        mongodbClientEncryption = (()=>{
            const e = new Error("Cannot find module 'mongodb-client-encryption'");
            e.code = 'MODULE_NOT_FOUND';
            throw e;
        })();
    } catch (error) {
        const kModuleError = new error_1.MongoMissingDependencyError('Optional module `mongodb-client-encryption` not found. Please install it to use auto encryption or ClientEncryption.', {
            cause: error,
            dependencyName: 'mongodb-client-encryption'
        });
        return {
            kModuleError
        };
    }
    return mongodbClientEncryption;
} //# sourceMappingURL=deps.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/client-side-encryption/errors.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.MongoCryptKMSRequestNetworkTimeoutError = exports.MongoCryptAzureKMSRequestError = exports.MongoCryptCreateEncryptedCollectionError = exports.MongoCryptCreateDataKeyError = exports.MongoCryptInvalidArgumentError = exports.defaultErrorWrapper = exports.MongoCryptError = void 0;
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
/**
 * @public
 * An error indicating that something went wrong specifically with MongoDB Client Encryption
 */ class MongoCryptError extends error_1.MongoError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, options = {}){
        super(message, options);
    }
    get name() {
        return 'MongoCryptError';
    }
}
exports.MongoCryptError = MongoCryptError;
const defaultErrorWrapper = (error)=>new MongoCryptError(error.message, {
        cause: error
    });
exports.defaultErrorWrapper = defaultErrorWrapper;
/**
 * @public
 *
 * An error indicating an invalid argument was provided to an encryption API.
 */ class MongoCryptInvalidArgumentError extends MongoCryptError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoCryptInvalidArgumentError';
    }
}
exports.MongoCryptInvalidArgumentError = MongoCryptInvalidArgumentError;
/**
 * @public
 * An error indicating that `ClientEncryption.createEncryptedCollection()` failed to create data keys
 */ class MongoCryptCreateDataKeyError extends MongoCryptError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(encryptedFields, { cause }){
        super(`Unable to complete creating data keys: ${cause.message}`, {
            cause
        });
        this.encryptedFields = encryptedFields;
    }
    get name() {
        return 'MongoCryptCreateDataKeyError';
    }
}
exports.MongoCryptCreateDataKeyError = MongoCryptCreateDataKeyError;
/**
 * @public
 * An error indicating that `ClientEncryption.createEncryptedCollection()` failed to create a collection
 */ class MongoCryptCreateEncryptedCollectionError extends MongoCryptError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(encryptedFields, { cause }){
        super(`Unable to create collection: ${cause.message}`, {
            cause
        });
        this.encryptedFields = encryptedFields;
    }
    get name() {
        return 'MongoCryptCreateEncryptedCollectionError';
    }
}
exports.MongoCryptCreateEncryptedCollectionError = MongoCryptCreateEncryptedCollectionError;
/**
 * @public
 * An error indicating that mongodb-client-encryption failed to auto-refresh Azure KMS credentials.
 */ class MongoCryptAzureKMSRequestError extends MongoCryptError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, body){
        super(message);
        this.body = body;
    }
    get name() {
        return 'MongoCryptAzureKMSRequestError';
    }
}
exports.MongoCryptAzureKMSRequestError = MongoCryptAzureKMSRequestError;
/** @public */ class MongoCryptKMSRequestNetworkTimeoutError extends MongoCryptError {
    get name() {
        return 'MongoCryptKMSRequestNetworkTimeoutError';
    }
}
exports.MongoCryptKMSRequestNetworkTimeoutError = MongoCryptKMSRequestNetworkTimeoutError; //# sourceMappingURL=errors.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/client-side-encryption/providers/aws.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.loadAWSCredentials = loadAWSCredentials;
const aws_temporary_credentials_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cmap/auth/aws_temporary_credentials.js [app-client] (ecmascript)");
/**
 * @internal
 */ async function loadAWSCredentials(kmsProviders, provider) {
    const credentialProvider = new aws_temporary_credentials_1.AWSSDKCredentialProvider(provider);
    // We shouldn't ever receive a response from the AWS SDK that doesn't have a `SecretAccessKey`
    // or `AccessKeyId`.  However, TS says these fields are optional.  We provide empty strings
    // and let libmongocrypt error if we're unable to fetch the required keys.
    const { SecretAccessKey = '', AccessKeyId = '', Token } = await credentialProvider.getCredentials();
    const aws = {
        secretAccessKey: SecretAccessKey,
        accessKeyId: AccessKeyId
    };
    // the AWS session token is only required for temporary credentials so only attach it to the
    // result if it's present in the response from the aws sdk
    Token != null && (aws.sessionToken = Token);
    return {
        ...kmsProviders,
        aws
    };
} //# sourceMappingURL=aws.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/client-side-encryption/providers/azure.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.tokenCache = exports.AzureCredentialCache = exports.AZURE_BASE_URL = void 0;
exports.addAzureParams = addAzureParams;
exports.prepareRequest = prepareRequest;
exports.fetchAzureKMSToken = fetchAzureKMSToken;
exports.loadAzureCredentials = loadAzureCredentials;
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/utils.js [app-client] (ecmascript)");
const errors_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/client-side-encryption/errors.js [app-client] (ecmascript)");
const MINIMUM_TOKEN_REFRESH_IN_MILLISECONDS = 6000;
/** Base URL for getting Azure tokens. */ exports.AZURE_BASE_URL = 'http://169.254.169.254/metadata/identity/oauth2/token?';
/**
 * @internal
 */ class AzureCredentialCache {
    constructor(){
        this.cachedToken = null;
    }
    async getToken() {
        if (this.cachedToken == null || this.needsRefresh(this.cachedToken)) {
            this.cachedToken = await this._getToken();
        }
        return {
            accessToken: this.cachedToken.accessToken
        };
    }
    needsRefresh(token) {
        const timeUntilExpirationMS = token.expiresOnTimestamp - Date.now();
        return timeUntilExpirationMS <= MINIMUM_TOKEN_REFRESH_IN_MILLISECONDS;
    }
    /**
     * exposed for testing
     */ resetCache() {
        this.cachedToken = null;
    }
    /**
     * exposed for testing
     */ _getToken() {
        return fetchAzureKMSToken();
    }
}
exports.AzureCredentialCache = AzureCredentialCache;
/** @internal */ exports.tokenCache = new AzureCredentialCache();
/** @internal */ async function parseResponse(response) {
    const { status, body: rawBody } = response;
    const body = (()=>{
        try {
            return JSON.parse(rawBody);
        } catch  {
            throw new errors_1.MongoCryptAzureKMSRequestError('Malformed JSON body in GET request.');
        }
    })();
    if (status !== 200) {
        throw new errors_1.MongoCryptAzureKMSRequestError('Unable to complete request.', body);
    }
    if (!body.access_token) {
        throw new errors_1.MongoCryptAzureKMSRequestError('Malformed response body - missing field `access_token`.');
    }
    if (!body.expires_in) {
        throw new errors_1.MongoCryptAzureKMSRequestError('Malformed response body - missing field `expires_in`.');
    }
    const expiresInMS = Number(body.expires_in) * 1000;
    if (Number.isNaN(expiresInMS)) {
        throw new errors_1.MongoCryptAzureKMSRequestError('Malformed response body - unable to parse int from `expires_in` field.');
    }
    return {
        accessToken: body.access_token,
        expiresOnTimestamp: Date.now() + expiresInMS
    };
}
/**
 * @internal
 * Get the Azure endpoint URL.
 */ function addAzureParams(url, resource, username) {
    url.searchParams.append('api-version', '2018-02-01');
    url.searchParams.append('resource', resource);
    if (username) {
        url.searchParams.append('client_id', username);
    }
    return url;
}
/**
 * @internal
 *
 * parses any options provided by prose tests to `fetchAzureKMSToken` and merges them with
 * the default values for headers and the request url.
 */ function prepareRequest(options) {
    const url = new URL(options.url?.toString() ?? exports.AZURE_BASE_URL);
    addAzureParams(url, 'https://vault.azure.net');
    const headers = {
        ...options.headers,
        'Content-Type': 'application/json',
        Metadata: true
    };
    return {
        headers,
        url
    };
}
/**
 * @internal
 *
 * `AzureKMSRequestOptions` allows prose tests to modify the http request sent to the idms
 * servers.  This is required to simulate different server conditions.  No options are expected to
 * be set outside of tests.
 *
 * exposed for CSFLE
 * [prose test 18](https://github.com/mongodb/specifications/tree/master/source/client-side-encryption/tests#azure-imds-credentials)
 */ async function fetchAzureKMSToken(options = {}) {
    const { headers, url } = prepareRequest(options);
    try {
        const response = await (0, utils_1.get)(url, {
            headers
        });
        return await parseResponse(response);
    } catch (error) {
        if (error instanceof error_1.MongoNetworkTimeoutError) {
            throw new errors_1.MongoCryptAzureKMSRequestError(`[Azure KMS] ${error.message}`);
        }
        throw error;
    }
}
/**
 * @internal
 *
 * @throws Will reject with a `MongoCryptError` if the http request fails or the http response is malformed.
 */ async function loadAzureCredentials(kmsProviders) {
    const azure = await exports.tokenCache.getToken();
    return {
        ...kmsProviders,
        azure
    };
} //# sourceMappingURL=azure.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/client-side-encryption/providers/gcp.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.loadGCPCredentials = loadGCPCredentials;
const deps_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/deps.js [app-client] (ecmascript)");
/** @internal */ async function loadGCPCredentials(kmsProviders) {
    const gcpMetadata = (0, deps_1.getGcpMetadata)();
    if ('kModuleError' in gcpMetadata) {
        return kmsProviders;
    }
    const { access_token: accessToken } = await gcpMetadata.instance({
        property: 'service-accounts/default/token'
    });
    return {
        ...kmsProviders,
        gcp: {
            accessToken
        }
    };
} //# sourceMappingURL=gcp.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/client-side-encryption/providers/index.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.isEmptyCredentials = isEmptyCredentials;
exports.refreshKMSCredentials = refreshKMSCredentials;
const aws_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/client-side-encryption/providers/aws.js [app-client] (ecmascript)");
const azure_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/client-side-encryption/providers/azure.js [app-client] (ecmascript)");
const gcp_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/client-side-encryption/providers/gcp.js [app-client] (ecmascript)");
/**
 * Auto credential fetching should only occur when the provider is defined on the kmsProviders map
 * and the settings are an empty object.
 *
 * This is distinct from a nullish provider key.
 *
 * @internal - exposed for testing purposes only
 */ function isEmptyCredentials(providerName, kmsProviders) {
    const provider = kmsProviders[providerName];
    if (provider == null) {
        return false;
    }
    return typeof provider === 'object' && Object.keys(provider).length === 0;
}
/**
 * Load cloud provider credentials for the user provided KMS providers.
 * Credentials will only attempt to get loaded if they do not exist
 * and no existing credentials will get overwritten.
 *
 * @internal
 */ async function refreshKMSCredentials(kmsProviders, credentialProviders) {
    let finalKMSProviders = kmsProviders;
    if (isEmptyCredentials('aws', kmsProviders)) {
        finalKMSProviders = await (0, aws_1.loadAWSCredentials)(finalKMSProviders, credentialProviders?.aws);
    }
    if (isEmptyCredentials('gcp', kmsProviders)) {
        finalKMSProviders = await (0, gcp_1.loadGCPCredentials)(finalKMSProviders);
    }
    if (isEmptyCredentials('azure', kmsProviders)) {
        finalKMSProviders = await (0, azure_1.loadAzureCredentials)(finalKMSProviders);
    }
    return finalKMSProviders;
} //# sourceMappingURL=index.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/client-side-encryption/state_machine.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/Pathology/patho-client/node_modules/.pnpm/next@16.0.6_@babel+core@7.28.5_react-dom@19.2.0_react@19.2.0__react@19.2.0/node_modules/next/dist/build/polyfills/process.js [app-client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.StateMachine = void 0;
const fs = (()=>{
    const e = new Error("Cannot find module 'fs/promises'");
    e.code = 'MODULE_NOT_FOUND';
    throw e;
})();
const net = (()=>{
    const e = new Error("Cannot find module 'net'");
    e.code = 'MODULE_NOT_FOUND';
    throw e;
})();
const tls = (()=>{
    const e = new Error("Cannot find module 'tls'");
    e.code = 'MODULE_NOT_FOUND';
    throw e;
})();
const bson_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bson.js [app-client] (ecmascript)");
const abstract_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/abstract_cursor.js [app-client] (ecmascript)");
const deps_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/deps.js [app-client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
const timeout_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/timeout.js [app-client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/utils.js [app-client] (ecmascript)");
const client_encryption_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/client-side-encryption/client_encryption.js [app-client] (ecmascript)");
const errors_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/client-side-encryption/errors.js [app-client] (ecmascript)");
let socks = null;
function loadSocks() {
    if (socks == null) {
        const socksImport = (0, deps_1.getSocks)();
        if ('kModuleError' in socksImport) {
            throw socksImport.kModuleError;
        }
        socks = socksImport;
    }
    return socks;
}
// libmongocrypt states
const MONGOCRYPT_CTX_ERROR = 0;
const MONGOCRYPT_CTX_NEED_MONGO_COLLINFO = 1;
const MONGOCRYPT_CTX_NEED_MONGO_MARKINGS = 2;
const MONGOCRYPT_CTX_NEED_MONGO_KEYS = 3;
const MONGOCRYPT_CTX_NEED_KMS_CREDENTIALS = 7;
const MONGOCRYPT_CTX_NEED_KMS = 4;
const MONGOCRYPT_CTX_READY = 5;
const MONGOCRYPT_CTX_DONE = 6;
const HTTPS_PORT = 443;
const stateToString = new Map([
    [
        MONGOCRYPT_CTX_ERROR,
        'MONGOCRYPT_CTX_ERROR'
    ],
    [
        MONGOCRYPT_CTX_NEED_MONGO_COLLINFO,
        'MONGOCRYPT_CTX_NEED_MONGO_COLLINFO'
    ],
    [
        MONGOCRYPT_CTX_NEED_MONGO_MARKINGS,
        'MONGOCRYPT_CTX_NEED_MONGO_MARKINGS'
    ],
    [
        MONGOCRYPT_CTX_NEED_MONGO_KEYS,
        'MONGOCRYPT_CTX_NEED_MONGO_KEYS'
    ],
    [
        MONGOCRYPT_CTX_NEED_KMS_CREDENTIALS,
        'MONGOCRYPT_CTX_NEED_KMS_CREDENTIALS'
    ],
    [
        MONGOCRYPT_CTX_NEED_KMS,
        'MONGOCRYPT_CTX_NEED_KMS'
    ],
    [
        MONGOCRYPT_CTX_READY,
        'MONGOCRYPT_CTX_READY'
    ],
    [
        MONGOCRYPT_CTX_DONE,
        'MONGOCRYPT_CTX_DONE'
    ]
]);
const INSECURE_TLS_OPTIONS = [
    'tlsInsecure',
    'tlsAllowInvalidCertificates',
    'tlsAllowInvalidHostnames'
];
/**
 * Helper function for logging. Enabled by setting the environment flag MONGODB_CRYPT_DEBUG.
 * @param msg - Anything you want to be logged.
 */ function debug(msg) {
    if (__TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].env.MONGODB_CRYPT_DEBUG) {
        // eslint-disable-next-line no-console
        console.error(msg);
    }
}
/**
 * This is kind of a hack.  For `rewrapManyDataKey`, we have tests that
 * guarantee that when there are no matching keys, `rewrapManyDataKey` returns
 * nothing.  We also have tests for auto encryption that guarantee for `encrypt`
 * we return an error when there are no matching keys.  This error is generated in
 * subsequent iterations of the state machine.
 * Some apis (`encrypt`) throw if there are no filter matches and others (`rewrapManyDataKey`)
 * do not.  We set the result manually here, and let the state machine continue.  `libmongocrypt`
 * will inform us if we need to error by setting the state to `MONGOCRYPT_CTX_ERROR` but
 * otherwise we'll return `{ v: [] }`.
 */ let EMPTY_V;
/**
 * @internal
 * An internal class that executes across a MongoCryptContext until either
 * a finishing state or an error is reached. Do not instantiate directly.
 */ // TODO(DRIVERS-2671): clarify CSOT behavior for FLE APIs
class StateMachine {
    constructor(options, bsonOptions = (0, bson_1.pluckBSONSerializeOptions)(options)){
        this.options = options;
        this.bsonOptions = bsonOptions;
    }
    /**
     * Executes the state machine according to the specification
     */ async execute(executor, context, options) {
        const keyVaultNamespace = executor._keyVaultNamespace;
        const keyVaultClient = executor._keyVaultClient;
        const metaDataClient = executor._metaDataClient;
        const mongocryptdClient = executor._mongocryptdClient;
        const mongocryptdManager = executor._mongocryptdManager;
        let result = null;
        // Typescript treats getters just like properties: Once you've tested it for equality
        // it cannot change. Which is exactly the opposite of what we use state and status for.
        // Every call to at least `addMongoOperationResponse` and `finalize` can change the state.
        // These wrappers let us write code more naturally and not add compiler exceptions
        // to conditions checks inside the state machine.
        const getStatus = ()=>context.status;
        const getState = ()=>context.state;
        while(getState() !== MONGOCRYPT_CTX_DONE && getState() !== MONGOCRYPT_CTX_ERROR){
            options.signal?.throwIfAborted();
            debug(`[context#${context.id}] ${stateToString.get(getState()) || getState()}`);
            switch(getState()){
                case MONGOCRYPT_CTX_NEED_MONGO_COLLINFO:
                    {
                        const filter = (0, bson_1.deserialize)(context.nextMongoOperation());
                        if (!metaDataClient) {
                            throw new errors_1.MongoCryptError('unreachable state machine state: entered MONGOCRYPT_CTX_NEED_MONGO_COLLINFO but metadata client is undefined');
                        }
                        const collInfoCursor = this.fetchCollectionInfo(metaDataClient, context.ns, filter, options);
                        for await (const collInfo of collInfoCursor){
                            context.addMongoOperationResponse((0, bson_1.serialize)(collInfo));
                            if (getState() === MONGOCRYPT_CTX_ERROR) break;
                        }
                        if (getState() === MONGOCRYPT_CTX_ERROR) break;
                        context.finishMongoOperation();
                        break;
                    }
                case MONGOCRYPT_CTX_NEED_MONGO_MARKINGS:
                    {
                        const command = context.nextMongoOperation();
                        if (getState() === MONGOCRYPT_CTX_ERROR) break;
                        if (!mongocryptdClient) {
                            throw new errors_1.MongoCryptError('unreachable state machine state: entered MONGOCRYPT_CTX_NEED_MONGO_MARKINGS but mongocryptdClient is undefined');
                        }
                        // When we are using the shared library, we don't have a mongocryptd manager.
                        const markedCommand = mongocryptdManager ? await mongocryptdManager.withRespawn(this.markCommand.bind(this, mongocryptdClient, context.ns, command, options)) : await this.markCommand(mongocryptdClient, context.ns, command, options);
                        context.addMongoOperationResponse(markedCommand);
                        context.finishMongoOperation();
                        break;
                    }
                case MONGOCRYPT_CTX_NEED_MONGO_KEYS:
                    {
                        const filter = context.nextMongoOperation();
                        const keys = await this.fetchKeys(keyVaultClient, keyVaultNamespace, filter, options);
                        if (keys.length === 0) {
                            // See docs on EMPTY_V
                            result = EMPTY_V ??= (0, bson_1.serialize)({
                                v: []
                            });
                        }
                        for (const key of keys){
                            context.addMongoOperationResponse((0, bson_1.serialize)(key));
                        }
                        context.finishMongoOperation();
                        break;
                    }
                case MONGOCRYPT_CTX_NEED_KMS_CREDENTIALS:
                    {
                        const kmsProviders = await executor.askForKMSCredentials();
                        context.provideKMSProviders((0, bson_1.serialize)(kmsProviders));
                        break;
                    }
                case MONGOCRYPT_CTX_NEED_KMS:
                    {
                        await Promise.all(this.requests(context, options));
                        context.finishKMSRequests();
                        break;
                    }
                case MONGOCRYPT_CTX_READY:
                    {
                        const finalizedContext = context.finalize();
                        if (getState() === MONGOCRYPT_CTX_ERROR) {
                            const message = getStatus().message || 'Finalization error';
                            throw new errors_1.MongoCryptError(message);
                        }
                        result = finalizedContext;
                        break;
                    }
                default:
                    throw new errors_1.MongoCryptError(`Unknown state: ${getState()}`);
            }
        }
        if (getState() === MONGOCRYPT_CTX_ERROR || result == null) {
            const message = getStatus().message;
            if (!message) {
                debug(`unidentifiable error in MongoCrypt - received an error status from \`libmongocrypt\` but received no error message.`);
            }
            throw new errors_1.MongoCryptError(message ?? 'unidentifiable error in MongoCrypt - received an error status from `libmongocrypt` but received no error message.');
        }
        return result;
    }
    /**
     * Handles the request to the KMS service. Exposed for testing purposes. Do not directly invoke.
     * @param kmsContext - A C++ KMS context returned from the bindings
     * @returns A promise that resolves when the KMS reply has be fully parsed
     */ async kmsRequest(request, options) {
        const parsedUrl = request.endpoint.split(':');
        const port = parsedUrl[1] != null ? Number.parseInt(parsedUrl[1], 10) : HTTPS_PORT;
        const socketOptions = {
            host: parsedUrl[0],
            servername: parsedUrl[0],
            port,
            ...(0, client_encryption_1.autoSelectSocketOptions)(this.options.socketOptions || {})
        };
        const message = request.message;
        const buffer = new utils_1.BufferPool();
        let netSocket;
        let socket;
        function destroySockets() {
            for (const sock of [
                socket,
                netSocket
            ]){
                if (sock) {
                    sock.destroy();
                }
            }
        }
        function onerror(cause) {
            return new errors_1.MongoCryptError('KMS request failed', {
                cause
            });
        }
        function onclose() {
            return new errors_1.MongoCryptError('KMS request closed');
        }
        const tlsOptions = this.options.tlsOptions;
        if (tlsOptions) {
            const kmsProvider = request.kmsProvider;
            const providerTlsOptions = tlsOptions[kmsProvider];
            if (providerTlsOptions) {
                const error = this.validateTlsOptions(kmsProvider, providerTlsOptions);
                if (error) {
                    throw error;
                }
                try {
                    await this.setTlsOptions(providerTlsOptions, socketOptions);
                } catch (err) {
                    throw onerror(err);
                }
            }
        }
        let abortListener;
        try {
            if (this.options.proxyOptions && this.options.proxyOptions.proxyHost) {
                netSocket = new net.Socket();
                const { promise: willConnect, reject: rejectOnNetSocketError, resolve: resolveOnNetSocketConnect } = (0, utils_1.promiseWithResolvers)();
                netSocket.once('error', (err)=>rejectOnNetSocketError(onerror(err))).once('close', ()=>rejectOnNetSocketError(onclose())).once('connect', ()=>resolveOnNetSocketConnect());
                const netSocketOptions = {
                    ...socketOptions,
                    host: this.options.proxyOptions.proxyHost,
                    port: this.options.proxyOptions.proxyPort || 1080
                };
                netSocket.connect(netSocketOptions);
                await willConnect;
                try {
                    socks ??= loadSocks();
                    socketOptions.socket = (await socks.SocksClient.createConnection({
                        existing_socket: netSocket,
                        command: 'connect',
                        destination: {
                            host: socketOptions.host,
                            port: socketOptions.port
                        },
                        proxy: {
                            // host and port are ignored because we pass existing_socket
                            host: 'iLoveJavaScript',
                            port: 0,
                            type: 5,
                            userId: this.options.proxyOptions.proxyUsername,
                            password: this.options.proxyOptions.proxyPassword
                        }
                    })).socket;
                } catch (err) {
                    throw onerror(err);
                }
            }
            socket = tls.connect(socketOptions, ()=>{
                socket.write(message);
            });
            const { promise: willResolveKmsRequest, reject: rejectOnTlsSocketError, resolve } = (0, utils_1.promiseWithResolvers)();
            abortListener = (0, utils_1.addAbortListener)(options?.signal, function() {
                destroySockets();
                rejectOnTlsSocketError(this.reason);
            });
            socket.once('error', (err)=>rejectOnTlsSocketError(onerror(err))).once('close', ()=>rejectOnTlsSocketError(onclose())).on('data', (data)=>{
                buffer.append(data);
                while(request.bytesNeeded > 0 && buffer.length){
                    const bytesNeeded = Math.min(request.bytesNeeded, buffer.length);
                    request.addResponse(buffer.read(bytesNeeded));
                }
                if (request.bytesNeeded <= 0) {
                    resolve();
                }
            });
            await (options?.timeoutContext?.csotEnabled() ? Promise.all([
                willResolveKmsRequest,
                timeout_1.Timeout.expires(options.timeoutContext?.remainingTimeMS)
            ]) : willResolveKmsRequest);
        } catch (error) {
            if (error instanceof timeout_1.TimeoutError) throw new error_1.MongoOperationTimeoutError('KMS request timed out');
            throw error;
        } finally{
            // There's no need for any more activity on this socket at this point.
            destroySockets();
            abortListener?.[utils_1.kDispose]();
        }
    }
    *requests(context, options) {
        for(let request = context.nextKMSRequest(); request != null; request = context.nextKMSRequest()){
            yield this.kmsRequest(request, options);
        }
    }
    /**
     * Validates the provided TLS options are secure.
     *
     * @param kmsProvider - The KMS provider name.
     * @param tlsOptions - The client TLS options for the provider.
     *
     * @returns An error if any option is invalid.
     */ validateTlsOptions(kmsProvider, tlsOptions) {
        const tlsOptionNames = Object.keys(tlsOptions);
        for (const option of INSECURE_TLS_OPTIONS){
            if (tlsOptionNames.includes(option)) {
                return new errors_1.MongoCryptError(`Insecure TLS options prohibited for ${kmsProvider}: ${option}`);
            }
        }
    }
    /**
     * Sets only the valid secure TLS options.
     *
     * @param tlsOptions - The client TLS options for the provider.
     * @param options - The existing connection options.
     */ async setTlsOptions(tlsOptions, options) {
        // If a secureContext is provided, ensure it is set.
        if (tlsOptions.secureContext) {
            options.secureContext = tlsOptions.secureContext;
        }
        if (tlsOptions.tlsCertificateKeyFile) {
            const cert = await fs.readFile(tlsOptions.tlsCertificateKeyFile);
            options.cert = options.key = cert;
        }
        if (tlsOptions.tlsCAFile) {
            options.ca = await fs.readFile(tlsOptions.tlsCAFile);
        }
        if (tlsOptions.tlsCertificateKeyFilePassword) {
            options.passphrase = tlsOptions.tlsCertificateKeyFilePassword;
        }
    }
    /**
     * Fetches collection info for a provided namespace, when libmongocrypt
     * enters the `MONGOCRYPT_CTX_NEED_MONGO_COLLINFO` state. The result is
     * used to inform libmongocrypt of the schema associated with this
     * namespace. Exposed for testing purposes. Do not directly invoke.
     *
     * @param client - A MongoClient connected to the topology
     * @param ns - The namespace to list collections from
     * @param filter - A filter for the listCollections command
     * @param callback - Invoked with the info of the requested collection, or with an error
     */ fetchCollectionInfo(client, ns, filter, options) {
        const { db } = utils_1.MongoDBCollectionNamespace.fromString(ns);
        const cursor = client.db(db).listCollections(filter, {
            promoteLongs: false,
            promoteValues: false,
            timeoutContext: options?.timeoutContext && new abstract_cursor_1.CursorTimeoutContext(options?.timeoutContext, Symbol()),
            signal: options?.signal,
            nameOnly: false
        });
        return cursor;
    }
    /**
     * Calls to the mongocryptd to provide markings for a command.
     * Exposed for testing purposes. Do not directly invoke.
     * @param client - A MongoClient connected to a mongocryptd
     * @param ns - The namespace (database.collection) the command is being executed on
     * @param command - The command to execute.
     * @param callback - Invoked with the serialized and marked bson command, or with an error
     */ async markCommand(client, ns, command, options) {
        const { db } = utils_1.MongoDBCollectionNamespace.fromString(ns);
        const bsonOptions = {
            promoteLongs: false,
            promoteValues: false
        };
        const rawCommand = (0, bson_1.deserialize)(command, bsonOptions);
        const commandOptions = {
            timeoutMS: undefined,
            signal: undefined
        };
        if (options?.timeoutContext?.csotEnabled()) {
            commandOptions.timeoutMS = options.timeoutContext.remainingTimeMS;
        }
        if (options?.signal) {
            commandOptions.signal = options.signal;
        }
        const response = await client.db(db).command(rawCommand, {
            ...bsonOptions,
            ...commandOptions
        });
        return (0, bson_1.serialize)(response, this.bsonOptions);
    }
    /**
     * Requests keys from the keyVault collection on the topology.
     * Exposed for testing purposes. Do not directly invoke.
     * @param client - A MongoClient connected to the topology
     * @param keyVaultNamespace - The namespace (database.collection) of the keyVault Collection
     * @param filter - The filter for the find query against the keyVault Collection
     * @param callback - Invoked with the found keys, or with an error
     */ fetchKeys(client, keyVaultNamespace, filter, options) {
        const { db: dbName, collection: collectionName } = utils_1.MongoDBCollectionNamespace.fromString(keyVaultNamespace);
        const commandOptions = {
            timeoutContext: undefined,
            signal: undefined
        };
        if (options?.timeoutContext != null) {
            commandOptions.timeoutContext = new abstract_cursor_1.CursorTimeoutContext(options.timeoutContext, Symbol());
        }
        if (options?.signal != null) {
            commandOptions.signal = options.signal;
        }
        return client.db(dbName).collection(collectionName, {
            readConcern: {
                level: 'majority'
            }
        }).find((0, bson_1.deserialize)(filter), commandOptions).toArray();
    }
}
exports.StateMachine = StateMachine; //# sourceMappingURL=state_machine.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/client-side-encryption/client_encryption.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/Pathology/patho-client/node_modules/.pnpm/next@16.0.6_@babel+core@7.28.5_react-dom@19.2.0_react@19.2.0__react@19.2.0/node_modules/next/dist/compiled/buffer/index.js [app-client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ClientEncryption = void 0;
exports.autoSelectSocketOptions = autoSelectSocketOptions;
const bson_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bson.js [app-client] (ecmascript)");
const deps_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/deps.js [app-client] (ecmascript)");
const timeout_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/timeout.js [app-client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/utils.js [app-client] (ecmascript)");
const errors_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/client-side-encryption/errors.js [app-client] (ecmascript)");
const index_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/client-side-encryption/providers/index.js [app-client] (ecmascript)");
const state_machine_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/client-side-encryption/state_machine.js [app-client] (ecmascript)");
/**
 * @public
 * The public interface for explicit in-use encryption
 */ class ClientEncryption {
    /** @internal */ static getMongoCrypt() {
        const encryption = (0, deps_1.getMongoDBClientEncryption)();
        if ('kModuleError' in encryption) {
            throw encryption.kModuleError;
        }
        return encryption.MongoCrypt;
    }
    /**
     * Create a new encryption instance
     *
     * @example
     * ```ts
     * new ClientEncryption(mongoClient, {
     *   keyVaultNamespace: 'client.encryption',
     *   kmsProviders: {
     *     local: {
     *       key: masterKey // The master key used for encryption/decryption. A 96-byte long Buffer
     *     }
     *   }
     * });
     * ```
     *
     * @example
     * ```ts
     * new ClientEncryption(mongoClient, {
     *   keyVaultNamespace: 'client.encryption',
     *   kmsProviders: {
     *     aws: {
     *       accessKeyId: AWS_ACCESS_KEY,
     *       secretAccessKey: AWS_SECRET_KEY
     *     }
     *   }
     * });
     * ```
     */ constructor(client, options){
        this._client = client;
        this._proxyOptions = options.proxyOptions ?? {};
        this._tlsOptions = options.tlsOptions ?? {};
        this._kmsProviders = options.kmsProviders || {};
        const { timeoutMS } = (0, utils_1.resolveTimeoutOptions)(client, options);
        this._timeoutMS = timeoutMS;
        this._credentialProviders = options.credentialProviders;
        if (options.credentialProviders?.aws && !(0, index_1.isEmptyCredentials)('aws', this._kmsProviders)) {
            throw new errors_1.MongoCryptInvalidArgumentError('Can only provide a custom AWS credential provider when the state machine is configured for automatic AWS credential fetching');
        }
        if (options.keyVaultNamespace == null) {
            throw new errors_1.MongoCryptInvalidArgumentError('Missing required option `keyVaultNamespace`');
        }
        const mongoCryptOptions = {
            ...options,
            kmsProviders: !__TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["Buffer"].isBuffer(this._kmsProviders) ? (0, bson_1.serialize)(this._kmsProviders) : this._kmsProviders,
            errorWrapper: errors_1.defaultErrorWrapper
        };
        this._keyVaultNamespace = options.keyVaultNamespace;
        this._keyVaultClient = options.keyVaultClient || client;
        const MongoCrypt = ClientEncryption.getMongoCrypt();
        this._mongoCrypt = new MongoCrypt(mongoCryptOptions);
    }
    /**
     * Creates a data key used for explicit encryption and inserts it into the key vault namespace
     *
     * @example
     * ```ts
     * // Using async/await to create a local key
     * const dataKeyId = await clientEncryption.createDataKey('local');
     * ```
     *
     * @example
     * ```ts
     * // Using async/await to create an aws key
     * const dataKeyId = await clientEncryption.createDataKey('aws', {
     *   masterKey: {
     *     region: 'us-east-1',
     *     key: 'xxxxxxxxxxxxxx' // CMK ARN here
     *   }
     * });
     * ```
     *
     * @example
     * ```ts
     * // Using async/await to create an aws key with a keyAltName
     * const dataKeyId = await clientEncryption.createDataKey('aws', {
     *   masterKey: {
     *     region: 'us-east-1',
     *     key: 'xxxxxxxxxxxxxx' // CMK ARN here
     *   },
     *   keyAltNames: [ 'mySpecialKey' ]
     * });
     * ```
     */ async createDataKey(provider, options = {}) {
        if (options.keyAltNames && !Array.isArray(options.keyAltNames)) {
            throw new errors_1.MongoCryptInvalidArgumentError(`Option "keyAltNames" must be an array of strings, but was of type ${typeof options.keyAltNames}.`);
        }
        let keyAltNames = undefined;
        if (options.keyAltNames && options.keyAltNames.length > 0) {
            keyAltNames = options.keyAltNames.map((keyAltName, i)=>{
                if (typeof keyAltName !== 'string') {
                    throw new errors_1.MongoCryptInvalidArgumentError(`Option "keyAltNames" must be an array of strings, but item at index ${i} was of type ${typeof keyAltName}`);
                }
                return (0, bson_1.serialize)({
                    keyAltName
                });
            });
        }
        let keyMaterial = undefined;
        if (options.keyMaterial) {
            keyMaterial = (0, bson_1.serialize)({
                keyMaterial: options.keyMaterial
            });
        }
        const dataKeyBson = (0, bson_1.serialize)({
            provider,
            ...options.masterKey
        });
        const context = this._mongoCrypt.makeDataKeyContext(dataKeyBson, {
            keyAltNames,
            keyMaterial
        });
        const stateMachine = new state_machine_1.StateMachine({
            proxyOptions: this._proxyOptions,
            tlsOptions: this._tlsOptions,
            socketOptions: autoSelectSocketOptions(this._client.s.options)
        });
        const timeoutContext = options?.timeoutContext ?? timeout_1.TimeoutContext.create((0, utils_1.resolveTimeoutOptions)(this._client, {
            timeoutMS: this._timeoutMS
        }));
        const dataKey = (0, bson_1.deserialize)(await stateMachine.execute(this, context, {
            timeoutContext
        }));
        const { db: dbName, collection: collectionName } = utils_1.MongoDBCollectionNamespace.fromString(this._keyVaultNamespace);
        const { insertedId } = await this._keyVaultClient.db(dbName).collection(collectionName).insertOne(dataKey, {
            writeConcern: {
                w: 'majority'
            },
            timeoutMS: timeoutContext?.csotEnabled() ? timeoutContext?.getRemainingTimeMSOrThrow() : undefined
        });
        return insertedId;
    }
    /**
     * Searches the keyvault for any data keys matching the provided filter.  If there are matches, rewrapManyDataKey then attempts to re-wrap the data keys using the provided options.
     *
     * If no matches are found, then no bulk write is performed.
     *
     * @example
     * ```ts
     * // rewrapping all data data keys (using a filter that matches all documents)
     * const filter = {};
     *
     * const result = await clientEncryption.rewrapManyDataKey(filter);
     * if (result.bulkWriteResult != null) {
     *  // keys were re-wrapped, results will be available in the bulkWrite object.
     * }
     * ```
     *
     * @example
     * ```ts
     * // attempting to rewrap all data keys with no matches
     * const filter = { _id: new Binary() } // assume _id matches no documents in the database
     * const result = await clientEncryption.rewrapManyDataKey(filter);
     *
     * if (result.bulkWriteResult == null) {
     *  // no keys matched, `bulkWriteResult` does not exist on the result object
     * }
     * ```
     */ async rewrapManyDataKey(filter, options) {
        let keyEncryptionKeyBson = undefined;
        if (options) {
            const keyEncryptionKey = Object.assign({
                provider: options.provider
            }, options.masterKey);
            keyEncryptionKeyBson = (0, bson_1.serialize)(keyEncryptionKey);
        }
        const filterBson = (0, bson_1.serialize)(filter);
        const context = this._mongoCrypt.makeRewrapManyDataKeyContext(filterBson, keyEncryptionKeyBson);
        const stateMachine = new state_machine_1.StateMachine({
            proxyOptions: this._proxyOptions,
            tlsOptions: this._tlsOptions,
            socketOptions: autoSelectSocketOptions(this._client.s.options)
        });
        const timeoutContext = timeout_1.TimeoutContext.create((0, utils_1.resolveTimeoutOptions)(this._client, {
            timeoutMS: this._timeoutMS
        }));
        const { v: dataKeys } = (0, bson_1.deserialize)(await stateMachine.execute(this, context, {
            timeoutContext
        }));
        if (dataKeys.length === 0) {
            return {};
        }
        const { db: dbName, collection: collectionName } = utils_1.MongoDBCollectionNamespace.fromString(this._keyVaultNamespace);
        const replacements = dataKeys.map((key)=>({
                updateOne: {
                    filter: {
                        _id: key._id
                    },
                    update: {
                        $set: {
                            masterKey: key.masterKey,
                            keyMaterial: key.keyMaterial
                        },
                        $currentDate: {
                            updateDate: true
                        }
                    }
                }
            }));
        const result = await this._keyVaultClient.db(dbName).collection(collectionName).bulkWrite(replacements, {
            writeConcern: {
                w: 'majority'
            },
            timeoutMS: timeoutContext.csotEnabled() ? timeoutContext?.remainingTimeMS : undefined
        });
        return {
            bulkWriteResult: result
        };
    }
    /**
     * Deletes the key with the provided id from the keyvault, if it exists.
     *
     * @example
     * ```ts
     * // delete a key by _id
     * const id = new Binary(); // id is a bson binary subtype 4 object
     * const { deletedCount } = await clientEncryption.deleteKey(id);
     *
     * if (deletedCount != null && deletedCount > 0) {
     *   // successful deletion
     * }
     * ```
     *
     */ async deleteKey(_id) {
        const { db: dbName, collection: collectionName } = utils_1.MongoDBCollectionNamespace.fromString(this._keyVaultNamespace);
        return await this._keyVaultClient.db(dbName).collection(collectionName).deleteOne({
            _id
        }, {
            writeConcern: {
                w: 'majority'
            },
            timeoutMS: this._timeoutMS
        });
    }
    /**
     * Finds all the keys currently stored in the keyvault.
     *
     * This method will not throw.
     *
     * @returns a FindCursor over all keys in the keyvault.
     * @example
     * ```ts
     * // fetching all keys
     * const keys = await clientEncryption.getKeys().toArray();
     * ```
     */ getKeys() {
        const { db: dbName, collection: collectionName } = utils_1.MongoDBCollectionNamespace.fromString(this._keyVaultNamespace);
        return this._keyVaultClient.db(dbName).collection(collectionName).find({}, {
            readConcern: {
                level: 'majority'
            },
            timeoutMS: this._timeoutMS
        });
    }
    /**
     * Finds a key in the keyvault with the specified _id.
     *
     * Returns a promise that either resolves to a {@link DataKey} if a document matches the key or null if no documents
     * match the id.  The promise rejects with an error if an error is thrown.
     * @example
     * ```ts
     * // getting a key by id
     * const id = new Binary(); // id is a bson binary subtype 4 object
     * const key = await clientEncryption.getKey(id);
     * if (!key) {
     *  // key is null if there was no matching key
     * }
     * ```
     */ async getKey(_id) {
        const { db: dbName, collection: collectionName } = utils_1.MongoDBCollectionNamespace.fromString(this._keyVaultNamespace);
        return await this._keyVaultClient.db(dbName).collection(collectionName).findOne({
            _id
        }, {
            readConcern: {
                level: 'majority'
            },
            timeoutMS: this._timeoutMS
        });
    }
    /**
     * Finds a key in the keyvault which has the specified keyAltName.
     *
     * @param keyAltName - a keyAltName to search for a key
     * @returns Returns a promise that either resolves to a {@link DataKey} if a document matches the key or null if no documents
     * match the keyAltName.  The promise rejects with an error if an error is thrown.
     * @example
     * ```ts
     * // get a key by alt name
     * const keyAltName = 'keyAltName';
     * const key = await clientEncryption.getKeyByAltName(keyAltName);
     * if (!key) {
     *  // key is null if there is no matching key
     * }
     * ```
     */ async getKeyByAltName(keyAltName) {
        const { db: dbName, collection: collectionName } = utils_1.MongoDBCollectionNamespace.fromString(this._keyVaultNamespace);
        return await this._keyVaultClient.db(dbName).collection(collectionName).findOne({
            keyAltNames: keyAltName
        }, {
            readConcern: {
                level: 'majority'
            },
            timeoutMS: this._timeoutMS
        });
    }
    /**
     * Adds a keyAltName to a key identified by the provided _id.
     *
     * This method resolves to/returns the *old* key value (prior to adding the new altKeyName).
     *
     * @param _id - The id of the document to update.
     * @param keyAltName - a keyAltName to search for a key
     * @returns Returns a promise that either resolves to a {@link DataKey} if a document matches the key or null if no documents
     * match the id.  The promise rejects with an error if an error is thrown.
     * @example
     * ```ts
     * // adding an keyAltName to a data key
     * const id = new Binary();  // id is a bson binary subtype 4 object
     * const keyAltName = 'keyAltName';
     * const oldKey = await clientEncryption.addKeyAltName(id, keyAltName);
     * if (!oldKey) {
     *  // null is returned if there is no matching document with an id matching the supplied id
     * }
     * ```
     */ async addKeyAltName(_id, keyAltName) {
        const { db: dbName, collection: collectionName } = utils_1.MongoDBCollectionNamespace.fromString(this._keyVaultNamespace);
        const value = await this._keyVaultClient.db(dbName).collection(collectionName).findOneAndUpdate({
            _id
        }, {
            $addToSet: {
                keyAltNames: keyAltName
            }
        }, {
            writeConcern: {
                w: 'majority'
            },
            returnDocument: 'before',
            timeoutMS: this._timeoutMS
        });
        return value;
    }
    /**
     * Adds a keyAltName to a key identified by the provided _id.
     *
     * This method resolves to/returns the *old* key value (prior to removing the new altKeyName).
     *
     * If the removed keyAltName is the last keyAltName for that key, the `altKeyNames` property is unset from the document.
     *
     * @param _id - The id of the document to update.
     * @param keyAltName - a keyAltName to search for a key
     * @returns Returns a promise that either resolves to a {@link DataKey} if a document matches the key or null if no documents
     * match the id.  The promise rejects with an error if an error is thrown.
     * @example
     * ```ts
     * // removing a key alt name from a data key
     * const id = new Binary();  // id is a bson binary subtype 4 object
     * const keyAltName = 'keyAltName';
     * const oldKey = await clientEncryption.removeKeyAltName(id, keyAltName);
     *
     * if (!oldKey) {
     *  // null is returned if there is no matching document with an id matching the supplied id
     * }
     * ```
     */ async removeKeyAltName(_id, keyAltName) {
        const { db: dbName, collection: collectionName } = utils_1.MongoDBCollectionNamespace.fromString(this._keyVaultNamespace);
        const pipeline = [
            {
                $set: {
                    keyAltNames: {
                        $cond: [
                            {
                                $eq: [
                                    '$keyAltNames',
                                    [
                                        keyAltName
                                    ]
                                ]
                            },
                            '$$REMOVE',
                            {
                                $filter: {
                                    input: '$keyAltNames',
                                    cond: {
                                        $ne: [
                                            '$$this',
                                            keyAltName
                                        ]
                                    }
                                }
                            }
                        ]
                    }
                }
            }
        ];
        const value = await this._keyVaultClient.db(dbName).collection(collectionName).findOneAndUpdate({
            _id
        }, pipeline, {
            writeConcern: {
                w: 'majority'
            },
            returnDocument: 'before',
            timeoutMS: this._timeoutMS
        });
        return value;
    }
    /**
     * A convenience method for creating an encrypted collection.
     * This method will create data keys for any encryptedFields that do not have a `keyId` defined
     * and then create a new collection with the full set of encryptedFields.
     *
     * @param db - A Node.js driver Db object with which to create the collection
     * @param name - The name of the collection to be created
     * @param options - Options for createDataKey and for createCollection
     * @returns created collection and generated encryptedFields
     * @throws MongoCryptCreateDataKeyError - If part way through the process a createDataKey invocation fails, an error will be rejected that has the partial `encryptedFields` that were created.
     * @throws MongoCryptCreateEncryptedCollectionError - If creating the collection fails, an error will be rejected that has the entire `encryptedFields` that were created.
     */ async createEncryptedCollection(db, name, options) {
        const { provider, masterKey, createCollectionOptions: { encryptedFields: { ...encryptedFields }, ...createCollectionOptions } } = options;
        const timeoutContext = this._timeoutMS != null ? timeout_1.TimeoutContext.create((0, utils_1.resolveTimeoutOptions)(this._client, {
            timeoutMS: this._timeoutMS
        })) : undefined;
        if (Array.isArray(encryptedFields.fields)) {
            const createDataKeyPromises = encryptedFields.fields.map(async (field)=>field == null || typeof field !== 'object' || field.keyId != null ? field : {
                    ...field,
                    keyId: await this.createDataKey(provider, {
                        masterKey,
                        // clone the timeoutContext
                        // in order to avoid sharing the same timeout for server selection and connection checkout across different concurrent operations
                        timeoutContext: timeoutContext?.csotEnabled() ? timeoutContext?.clone() : undefined
                    })
                });
            const createDataKeyResolutions = await Promise.allSettled(createDataKeyPromises);
            encryptedFields.fields = createDataKeyResolutions.map((resolution, index)=>resolution.status === 'fulfilled' ? resolution.value : encryptedFields.fields[index]);
            const rejection = createDataKeyResolutions.find((result)=>result.status === 'rejected');
            if (rejection != null) {
                throw new errors_1.MongoCryptCreateDataKeyError(encryptedFields, {
                    cause: rejection.reason
                });
            }
        }
        try {
            const collection = await db.createCollection(name, {
                ...createCollectionOptions,
                encryptedFields,
                timeoutMS: timeoutContext?.csotEnabled() ? timeoutContext?.getRemainingTimeMSOrThrow() : undefined
            });
            return {
                collection,
                encryptedFields
            };
        } catch (cause) {
            throw new errors_1.MongoCryptCreateEncryptedCollectionError(encryptedFields, {
                cause
            });
        }
    }
    /**
     * Explicitly encrypt a provided value. Note that either `options.keyId` or `options.keyAltName` must
     * be specified. Specifying both `options.keyId` and `options.keyAltName` is considered an error.
     *
     * @param value - The value that you wish to serialize. Must be of a type that can be serialized into BSON
     * @param options -
     * @returns a Promise that either resolves with the encrypted value, or rejects with an error.
     *
     * @example
     * ```ts
     * // Encryption with async/await api
     * async function encryptMyData(value) {
     *   const keyId = await clientEncryption.createDataKey('local');
     *   return clientEncryption.encrypt(value, { keyId, algorithm: 'AEAD_AES_256_CBC_HMAC_SHA_512-Deterministic' });
     * }
     * ```
     *
     * @example
     * ```ts
     * // Encryption using a keyAltName
     * async function encryptMyData(value) {
     *   await clientEncryption.createDataKey('local', { keyAltNames: 'mySpecialKey' });
     *   return clientEncryption.encrypt(value, { keyAltName: 'mySpecialKey', algorithm: 'AEAD_AES_256_CBC_HMAC_SHA_512-Deterministic' });
     * }
     * ```
     */ async encrypt(value, options) {
        return await this._encrypt(value, false, options);
    }
    /**
     * Encrypts a Match Expression or Aggregate Expression to query a range index.
     *
     * Only supported when queryType is "range" and algorithm is "Range".
     *
     * @param expression - a BSON document of one of the following forms:
     *  1. A Match Expression of this form:
     *      `{$and: [{<field>: {$gt: <value1>}}, {<field>: {$lt: <value2> }}]}`
     *  2. An Aggregate Expression of this form:
     *      `{$and: [{$gt: [<fieldpath>, <value1>]}, {$lt: [<fieldpath>, <value2>]}]}`
     *
     *    `$gt` may also be `$gte`. `$lt` may also be `$lte`.
     *
     * @param options -
     * @returns Returns a Promise that either resolves with the encrypted value or rejects with an error.
     */ async encryptExpression(expression, options) {
        return await this._encrypt(expression, true, options);
    }
    /**
     * Explicitly decrypt a provided encrypted value
     *
     * @param value - An encrypted value
     * @returns a Promise that either resolves with the decrypted value, or rejects with an error
     *
     * @example
     * ```ts
     * // Decrypting value with async/await API
     * async function decryptMyValue(value) {
     *   return clientEncryption.decrypt(value);
     * }
     * ```
     */ async decrypt(value) {
        const valueBuffer = (0, bson_1.serialize)({
            v: value
        });
        const context = this._mongoCrypt.makeExplicitDecryptionContext(valueBuffer);
        const stateMachine = new state_machine_1.StateMachine({
            proxyOptions: this._proxyOptions,
            tlsOptions: this._tlsOptions,
            socketOptions: autoSelectSocketOptions(this._client.s.options)
        });
        const timeoutContext = this._timeoutMS != null ? timeout_1.TimeoutContext.create((0, utils_1.resolveTimeoutOptions)(this._client, {
            timeoutMS: this._timeoutMS
        })) : undefined;
        const { v } = (0, bson_1.deserialize)(await stateMachine.execute(this, context, {
            timeoutContext
        }));
        return v;
    }
    /**
     * @internal
     * Ask the user for KMS credentials.
     *
     * This returns anything that looks like the kmsProviders original input
     * option. It can be empty, and any provider specified here will override
     * the original ones.
     */ async askForKMSCredentials() {
        return await (0, index_1.refreshKMSCredentials)(this._kmsProviders, this._credentialProviders);
    }
    static get libmongocryptVersion() {
        return ClientEncryption.getMongoCrypt().libmongocryptVersion;
    }
    /**
     * @internal
     * A helper that perform explicit encryption of values and expressions.
     * Explicitly encrypt a provided value. Note that either `options.keyId` or `options.keyAltName` must
     * be specified. Specifying both `options.keyId` and `options.keyAltName` is considered an error.
     *
     * @param value - The value that you wish to encrypt. Must be of a type that can be serialized into BSON
     * @param expressionMode - a boolean that indicates whether or not to encrypt the value as an expression
     * @param options - options to pass to encrypt
     * @returns the raw result of the call to stateMachine.execute().  When expressionMode is set to true, the return
     *          value will be a bson document.  When false, the value will be a BSON Binary.
     *
     */ async _encrypt(value, expressionMode, options) {
        const { algorithm, keyId, keyAltName, contentionFactor, queryType, rangeOptions, textOptions } = options;
        const contextOptions = {
            expressionMode,
            algorithm
        };
        if (keyId) {
            contextOptions.keyId = keyId.buffer;
        }
        if (keyAltName) {
            if (keyId) {
                throw new errors_1.MongoCryptInvalidArgumentError(`"options" cannot contain both "keyId" and "keyAltName"`);
            }
            if (typeof keyAltName !== 'string') {
                throw new errors_1.MongoCryptInvalidArgumentError(`"options.keyAltName" must be of type string, but was of type ${typeof keyAltName}`);
            }
            contextOptions.keyAltName = (0, bson_1.serialize)({
                keyAltName
            });
        }
        if (typeof contentionFactor === 'number' || typeof contentionFactor === 'bigint') {
            contextOptions.contentionFactor = contentionFactor;
        }
        if (typeof queryType === 'string') {
            contextOptions.queryType = queryType;
        }
        if (typeof rangeOptions === 'object') {
            contextOptions.rangeOptions = (0, bson_1.serialize)(rangeOptions);
        }
        if (typeof textOptions === 'object') {
            contextOptions.textOptions = (0, bson_1.serialize)(textOptions);
        }
        const valueBuffer = (0, bson_1.serialize)({
            v: value
        });
        const stateMachine = new state_machine_1.StateMachine({
            proxyOptions: this._proxyOptions,
            tlsOptions: this._tlsOptions,
            socketOptions: autoSelectSocketOptions(this._client.s.options)
        });
        const context = this._mongoCrypt.makeExplicitEncryptionContext(valueBuffer, contextOptions);
        const timeoutContext = this._timeoutMS != null ? timeout_1.TimeoutContext.create((0, utils_1.resolveTimeoutOptions)(this._client, {
            timeoutMS: this._timeoutMS
        })) : undefined;
        const { v } = (0, bson_1.deserialize)(await stateMachine.execute(this, context, {
            timeoutContext
        }));
        return v;
    }
}
exports.ClientEncryption = ClientEncryption;
/**
 * Get the socket options from the client.
 * @param baseOptions - The mongo client options.
 * @returns ClientEncryptionSocketOptions
 */ function autoSelectSocketOptions(baseOptions) {
    const options = {
        autoSelectFamily: true
    };
    if ('autoSelectFamily' in baseOptions) {
        options.autoSelectFamily = baseOptions.autoSelectFamily;
    }
    if ('autoSelectFamilyAttemptTimeout' in baseOptions) {
        options.autoSelectFamilyAttemptTimeout = baseOptions.autoSelectFamilyAttemptTimeout;
    }
    return options;
} //# sourceMappingURL=client_encryption.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/client-side-encryption/mongocryptd_manager.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.MongocryptdManager = void 0;
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
/**
 * @internal
 * An internal class that handles spawning a mongocryptd.
 */ class MongocryptdManager {
    static{
        this.DEFAULT_MONGOCRYPTD_URI = 'mongodb://localhost:27020';
    }
    constructor(extraOptions = {}){
        this.spawnPath = '';
        this.spawnArgs = [];
        this.uri = typeof extraOptions.mongocryptdURI === 'string' && extraOptions.mongocryptdURI.length > 0 ? extraOptions.mongocryptdURI : MongocryptdManager.DEFAULT_MONGOCRYPTD_URI;
        this.bypassSpawn = !!extraOptions.mongocryptdBypassSpawn;
        if (Object.hasOwn(extraOptions, 'mongocryptdSpawnPath') && extraOptions.mongocryptdSpawnPath) {
            this.spawnPath = extraOptions.mongocryptdSpawnPath;
        }
        if (Object.hasOwn(extraOptions, 'mongocryptdSpawnArgs') && Array.isArray(extraOptions.mongocryptdSpawnArgs)) {
            this.spawnArgs = this.spawnArgs.concat(extraOptions.mongocryptdSpawnArgs);
        }
        if (this.spawnArgs.filter((arg)=>typeof arg === 'string').every((arg)=>arg.indexOf('--idleShutdownTimeoutSecs') < 0)) {
            this.spawnArgs.push('--idleShutdownTimeoutSecs', '60');
        }
    }
    /**
     * Will check to see if a mongocryptd is up. If it is not up, it will attempt
     * to spawn a mongocryptd in a detached process, and then wait for it to be up.
     */ async spawn() {
        const cmdName = this.spawnPath || 'mongocryptd';
        // eslint-disable-next-line @typescript-eslint/no-require-imports
        const { spawn } = (()=>{
            const e = new Error("Cannot find module 'child_process'");
            e.code = 'MODULE_NOT_FOUND';
            throw e;
        })();
        // Spawned with stdio: ignore and detached: true
        // to ensure child can outlive parent.
        this._child = spawn(cmdName, this.spawnArgs, {
            stdio: 'ignore',
            detached: true
        });
        this._child.on('error', ()=>{
        // From the FLE spec:
        // "The stdout and stderr of the spawned process MUST not be exposed in the driver
        // (e.g. redirect to /dev/null). Users can pass the argument --logpath to
        // extraOptions.mongocryptdSpawnArgs if they need to inspect mongocryptd logs.
        // If spawning is necessary, the driver MUST spawn mongocryptd whenever server
        // selection on the MongoClient to mongocryptd fails. If the MongoClient fails to
        // connect after spawning, the server selection error is propagated to the user."
        // The AutoEncrypter and MongoCryptdManager should work together to spawn
        // mongocryptd whenever necessary.  Additionally, the `mongocryptd` intentionally
        // shuts down after 60s and gets respawned when necessary.  We rely on server
        // selection timeouts when connecting to the `mongocryptd` to inform users that something
        // has been configured incorrectly.  For those reasons, we suppress stderr from
        // the `mongocryptd` process and immediately unref the process.
        });
        // unref child to remove handle from event loop
        this._child.unref();
    }
    /**
     * @returns the result of `fn` or rejects with an error.
     */ async withRespawn(fn) {
        try {
            const result = await fn();
            return result;
        } catch (err) {
            // If we are not bypassing spawning, then we should retry once on a MongoTimeoutError (server selection error)
            const shouldSpawn = err instanceof error_1.MongoNetworkTimeoutError && !this.bypassSpawn;
            if (!shouldSpawn) {
                throw err;
            }
        }
        await this.spawn();
        const result = await fn();
        return result;
    }
}
exports.MongocryptdManager = MongocryptdManager; //# sourceMappingURL=mongocryptd_manager.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/client-side-encryption/auto_encrypter.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/Pathology/patho-client/node_modules/.pnpm/next@16.0.6_@babel+core@7.28.5_react-dom@19.2.0_react@19.2.0__react@19.2.0/node_modules/next/dist/compiled/buffer/index.js [app-client] (ecmascript)");
"use strict";
var _a;
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.AutoEncrypter = exports.AutoEncryptionLoggerLevel = void 0;
const net = (()=>{
    const e = new Error("Cannot find module 'net'");
    e.code = 'MODULE_NOT_FOUND';
    throw e;
})();
const bson_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bson.js [app-client] (ecmascript)");
const constants_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/constants.js [app-client] (ecmascript)");
const deps_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/deps.js [app-client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
const mongo_client_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/mongo_client.js [app-client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/utils.js [app-client] (ecmascript)");
const client_encryption_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/client-side-encryption/client_encryption.js [app-client] (ecmascript)");
const errors_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/client-side-encryption/errors.js [app-client] (ecmascript)");
const mongocryptd_manager_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/client-side-encryption/mongocryptd_manager.js [app-client] (ecmascript)");
const providers_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/client-side-encryption/providers/index.js [app-client] (ecmascript)");
const state_machine_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/client-side-encryption/state_machine.js [app-client] (ecmascript)");
/** @public */ exports.AutoEncryptionLoggerLevel = Object.freeze({
    FatalError: 0,
    Error: 1,
    Warning: 2,
    Info: 3,
    Trace: 4
});
/**
 * @internal An internal class to be used by the driver for auto encryption
 * **NOTE**: Not meant to be instantiated directly, this is for internal use only.
 */ class AutoEncrypter {
    static{
        _a = constants_1.kDecorateResult;
    }
    /** @internal */ static getMongoCrypt() {
        const encryption = (0, deps_1.getMongoDBClientEncryption)();
        if ('kModuleError' in encryption) {
            throw encryption.kModuleError;
        }
        return encryption.MongoCrypt;
    }
    /**
     * Create an AutoEncrypter
     *
     * **Note**: Do not instantiate this class directly. Rather, supply the relevant options to a MongoClient
     *
     * **Note**: Supplying `options.schemaMap` provides more security than relying on JSON Schemas obtained from the server.
     * It protects against a malicious server advertising a false JSON Schema, which could trick the client into sending unencrypted data that should be encrypted.
     * Schemas supplied in the schemaMap only apply to configuring automatic encryption for Client-Side Field Level Encryption.
     * Other validation rules in the JSON schema will not be enforced by the driver and will result in an error.
     *
     * @example <caption>Create an AutoEncrypter that makes use of mongocryptd</caption>
     * ```ts
     * // Enabling autoEncryption via a MongoClient using mongocryptd
     * const { MongoClient } = require('mongodb');
     * const client = new MongoClient(URL, {
     *   autoEncryption: {
     *     kmsProviders: {
     *       aws: {
     *         accessKeyId: AWS_ACCESS_KEY,
     *         secretAccessKey: AWS_SECRET_KEY
     *       }
     *     }
     *   }
     * });
     * ```
     *
     * await client.connect();
     * // From here on, the client will be encrypting / decrypting automatically
     * @example <caption>Create an AutoEncrypter that makes use of libmongocrypt's CSFLE shared library</caption>
     * ```ts
     * // Enabling autoEncryption via a MongoClient using CSFLE shared library
     * const { MongoClient } = require('mongodb');
     * const client = new MongoClient(URL, {
     *   autoEncryption: {
     *     kmsProviders: {
     *       aws: {}
     *     },
     *     extraOptions: {
     *       cryptSharedLibPath: '/path/to/local/crypt/shared/lib',
     *       cryptSharedLibRequired: true
     *     }
     *   }
     * });
     * ```
     *
     * await client.connect();
     * // From here on, the client will be encrypting / decrypting automatically
     */ constructor(client, options){
        /**
         * Used by devtools to enable decorating decryption results.
         *
         * When set and enabled, `decrypt` will automatically recursively
         * traverse a decrypted document and if a field has been decrypted,
         * it will mark it as decrypted.  Compass uses this to determine which
         * fields were decrypted.
         */ this[_a] = false;
        this._client = client;
        this._bypassEncryption = options.bypassAutoEncryption === true;
        this._keyVaultNamespace = options.keyVaultNamespace || 'admin.datakeys';
        this._keyVaultClient = options.keyVaultClient || client;
        this._metaDataClient = options.metadataClient || client;
        this._proxyOptions = options.proxyOptions || {};
        this._tlsOptions = options.tlsOptions || {};
        this._kmsProviders = options.kmsProviders || {};
        this._credentialProviders = options.credentialProviders;
        if (options.credentialProviders?.aws && !(0, providers_1.isEmptyCredentials)('aws', this._kmsProviders)) {
            throw new errors_1.MongoCryptInvalidArgumentError('Can only provide a custom AWS credential provider when the state machine is configured for automatic AWS credential fetching');
        }
        const mongoCryptOptions = {
            errorWrapper: errors_1.defaultErrorWrapper
        };
        if (options.schemaMap) {
            mongoCryptOptions.schemaMap = __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["Buffer"].isBuffer(options.schemaMap) ? options.schemaMap : (0, bson_1.serialize)(options.schemaMap);
        }
        if (options.encryptedFieldsMap) {
            mongoCryptOptions.encryptedFieldsMap = __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["Buffer"].isBuffer(options.encryptedFieldsMap) ? options.encryptedFieldsMap : (0, bson_1.serialize)(options.encryptedFieldsMap);
        }
        mongoCryptOptions.kmsProviders = !__TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["Buffer"].isBuffer(this._kmsProviders) ? (0, bson_1.serialize)(this._kmsProviders) : this._kmsProviders;
        if (options.options?.logger) {
            mongoCryptOptions.logger = options.options.logger;
        }
        if (options.extraOptions && options.extraOptions.cryptSharedLibPath) {
            mongoCryptOptions.cryptSharedLibPath = options.extraOptions.cryptSharedLibPath;
        }
        if (options.bypassQueryAnalysis) {
            mongoCryptOptions.bypassQueryAnalysis = options.bypassQueryAnalysis;
        }
        if (options.keyExpirationMS != null) {
            mongoCryptOptions.keyExpirationMS = options.keyExpirationMS;
        }
        this._bypassMongocryptdAndCryptShared = this._bypassEncryption || !!options.bypassQueryAnalysis;
        if (options.extraOptions && options.extraOptions.cryptSharedLibSearchPaths) {
            // Only for driver testing
            mongoCryptOptions.cryptSharedLibSearchPaths = options.extraOptions.cryptSharedLibSearchPaths;
        } else if (!this._bypassMongocryptdAndCryptShared) {
            mongoCryptOptions.cryptSharedLibSearchPaths = [
                '$SYSTEM'
            ];
        }
        const MongoCrypt = AutoEncrypter.getMongoCrypt();
        this._mongocrypt = new MongoCrypt(mongoCryptOptions);
        this._contextCounter = 0;
        if (options.extraOptions && options.extraOptions.cryptSharedLibRequired && !this.cryptSharedLibVersionInfo) {
            throw new errors_1.MongoCryptInvalidArgumentError('`cryptSharedLibRequired` set but no crypt_shared library loaded');
        }
        // Only instantiate mongocryptd manager/client once we know for sure
        // that we are not using the CSFLE shared library.
        if (!this._bypassMongocryptdAndCryptShared && !this.cryptSharedLibVersionInfo) {
            this._mongocryptdManager = new mongocryptd_manager_1.MongocryptdManager(options.extraOptions);
            const clientOptions = {
                serverSelectionTimeoutMS: 10000
            };
            if ((options.extraOptions == null || typeof options.extraOptions.mongocryptdURI !== 'string') && !net.getDefaultAutoSelectFamily) {
                // Only set family if autoSelectFamily options are not supported.
                clientOptions.family = 4;
            }
            // eslint-disable-next-line @typescript-eslint/ban-ts-comment
            // @ts-ignore: TS complains as this always returns true on versions where it is present.
            if (net.getDefaultAutoSelectFamily) {
                // AutoEncrypter is made inside of MongoClient constructor while options are being parsed,
                // we do not have access to the options that are in progress.
                // TODO(NODE-6449): AutoEncrypter does not use client options for autoSelectFamily
                Object.assign(clientOptions, (0, client_encryption_1.autoSelectSocketOptions)(this._client.s?.options ?? {}));
            }
            this._mongocryptdClient = new mongo_client_1.MongoClient(this._mongocryptdManager.uri, clientOptions);
        }
    }
    /**
     * Initializes the auto encrypter by spawning a mongocryptd and connecting to it.
     *
     * This function is a no-op when bypassSpawn is set or the crypt shared library is used.
     */ async init() {
        if (this._bypassMongocryptdAndCryptShared || this.cryptSharedLibVersionInfo) {
            return;
        }
        if (!this._mongocryptdManager) {
            throw new error_1.MongoRuntimeError('Reached impossible state: mongocryptdManager is undefined when neither bypassSpawn nor the shared lib are specified.');
        }
        if (!this._mongocryptdClient) {
            throw new error_1.MongoRuntimeError('Reached impossible state: mongocryptdClient is undefined when neither bypassSpawn nor the shared lib are specified.');
        }
        if (!this._mongocryptdManager.bypassSpawn) {
            await this._mongocryptdManager.spawn();
        }
        try {
            const client = await this._mongocryptdClient.connect();
            return client;
        } catch (error) {
            throw new error_1.MongoRuntimeError('Unable to connect to `mongocryptd`, please make sure it is running or in your PATH for auto-spawn', {
                cause: error
            });
        }
    }
    /**
     * Cleans up the `_mongocryptdClient`, if present.
     */ async close() {
        await this._mongocryptdClient?.close();
    }
    /**
     * Encrypt a command for a given namespace.
     */ async encrypt(ns, cmd, options = {}) {
        options.signal?.throwIfAborted();
        if (this._bypassEncryption) {
            // If `bypassAutoEncryption` has been specified, don't encrypt
            return cmd;
        }
        const commandBuffer = __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["Buffer"].isBuffer(cmd) ? cmd : (0, bson_1.serialize)(cmd, options);
        const context = this._mongocrypt.makeEncryptionContext(utils_1.MongoDBCollectionNamespace.fromString(ns).db, commandBuffer);
        context.id = this._contextCounter++;
        context.ns = ns;
        context.document = cmd;
        const stateMachine = new state_machine_1.StateMachine({
            promoteValues: false,
            promoteLongs: false,
            proxyOptions: this._proxyOptions,
            tlsOptions: this._tlsOptions,
            socketOptions: (0, client_encryption_1.autoSelectSocketOptions)(this._client.s.options)
        });
        return (0, bson_1.deserialize)(await stateMachine.execute(this, context, options), {
            promoteValues: false,
            promoteLongs: false
        });
    }
    /**
     * Decrypt a command response
     */ async decrypt(response, options = {}) {
        options.signal?.throwIfAborted();
        const context = this._mongocrypt.makeDecryptionContext(response);
        context.id = this._contextCounter++;
        const stateMachine = new state_machine_1.StateMachine({
            ...options,
            proxyOptions: this._proxyOptions,
            tlsOptions: this._tlsOptions,
            socketOptions: (0, client_encryption_1.autoSelectSocketOptions)(this._client.s.options)
        });
        return await stateMachine.execute(this, context, options);
    }
    /**
     * Ask the user for KMS credentials.
     *
     * This returns anything that looks like the kmsProviders original input
     * option. It can be empty, and any provider specified here will override
     * the original ones.
     */ async askForKMSCredentials() {
        return await (0, providers_1.refreshKMSCredentials)(this._kmsProviders, this._credentialProviders);
    }
    /**
     * Return the current libmongocrypt's CSFLE shared library version
     * as `{ version: bigint, versionStr: string }`, or `null` if no CSFLE
     * shared library was loaded.
     */ get cryptSharedLibVersionInfo() {
        return this._mongocrypt.cryptSharedLibVersionInfo;
    }
    static get libmongocryptVersion() {
        return AutoEncrypter.getMongoCrypt().libmongocryptVersion;
    }
}
exports.AutoEncrypter = AutoEncrypter; //# sourceMappingURL=auto_encrypter.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/encrypter.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.Encrypter = void 0;
const auto_encrypter_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/client-side-encryption/auto_encrypter.js [app-client] (ecmascript)");
const constants_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/constants.js [app-client] (ecmascript)");
const deps_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/deps.js [app-client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
const mongo_client_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/mongo_client.js [app-client] (ecmascript)");
/** @internal */ class Encrypter {
    constructor(client, uri, options){
        if (typeof options.autoEncryption !== 'object') {
            throw new error_1.MongoInvalidArgumentError('Option "autoEncryption" must be specified');
        }
        // initialize to null, if we call getInternalClient, we may set this it is important to not overwrite those function calls.
        this.internalClient = null;
        this.bypassAutoEncryption = !!options.autoEncryption.bypassAutoEncryption;
        this.needsConnecting = false;
        if (options.maxPoolSize === 0 && options.autoEncryption.keyVaultClient == null) {
            options.autoEncryption.keyVaultClient = client;
        } else if (options.autoEncryption.keyVaultClient == null) {
            options.autoEncryption.keyVaultClient = this.getInternalClient(client, uri, options);
        }
        if (this.bypassAutoEncryption) {
            options.autoEncryption.metadataClient = undefined;
        } else if (options.maxPoolSize === 0) {
            options.autoEncryption.metadataClient = client;
        } else {
            options.autoEncryption.metadataClient = this.getInternalClient(client, uri, options);
        }
        if (options.proxyHost) {
            options.autoEncryption.proxyOptions = {
                proxyHost: options.proxyHost,
                proxyPort: options.proxyPort,
                proxyUsername: options.proxyUsername,
                proxyPassword: options.proxyPassword
            };
        }
        this.autoEncrypter = new auto_encrypter_1.AutoEncrypter(client, options.autoEncryption);
    }
    getInternalClient(client, uri, options) {
        let internalClient = this.internalClient;
        if (internalClient == null) {
            const clonedOptions = {};
            for (const key of [
                ...Object.getOwnPropertyNames(options),
                ...Object.getOwnPropertySymbols(options)
            ]){
                if ([
                    'autoEncryption',
                    'minPoolSize',
                    'servers',
                    'caseTranslate',
                    'dbName'
                ].includes(key)) continue;
                Reflect.set(clonedOptions, key, Reflect.get(options, key));
            }
            clonedOptions.minPoolSize = 0;
            internalClient = new mongo_client_1.MongoClient(uri, clonedOptions);
            this.internalClient = internalClient;
            for (const eventName of constants_1.MONGO_CLIENT_EVENTS){
                for (const listener of client.listeners(eventName)){
                    internalClient.on(eventName, listener);
                }
            }
            client.on('newListener', (eventName, listener)=>{
                internalClient?.on(eventName, listener);
            });
            this.needsConnecting = true;
        }
        return internalClient;
    }
    async connectInternalClient() {
        const internalClient = this.internalClient;
        if (this.needsConnecting && internalClient != null) {
            this.needsConnecting = false;
            await internalClient.connect();
        }
    }
    async close(client) {
        let error;
        try {
            await this.autoEncrypter.close();
        } catch (autoEncrypterError) {
            error = autoEncrypterError;
        }
        const internalClient = this.internalClient;
        if (internalClient != null && client !== internalClient) {
            return await internalClient.close();
        }
        if (error != null) {
            throw error;
        }
    }
    static checkForMongoCrypt() {
        const mongodbClientEncryption = (0, deps_1.getMongoDBClientEncryption)();
        if ('kModuleError' in mongodbClientEncryption) {
            throw new error_1.MongoMissingDependencyError('Auto-encryption requested, but the module is not installed. ' + 'Please add `mongodb-client-encryption` as a dependency of your project', {
                cause: mongodbClientEncryption['kModuleError'],
                dependencyName: 'mongodb-client-encryption'
            });
        }
    }
}
exports.Encrypter = Encrypter; //# sourceMappingURL=encrypter.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/connection_string.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/Pathology/patho-client/node_modules/.pnpm/next@16.0.6_@babel+core@7.28.5_react-dom@19.2.0_react@19.2.0__react@19.2.0/node_modules/next/dist/build/polyfills/process.js [app-client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.DEFAULT_OPTIONS = exports.OPTIONS = void 0;
exports.resolveSRVRecord = resolveSRVRecord;
exports.parseOptions = parseOptions;
const dns = (()=>{
    const e = new Error("Cannot find module 'dns'");
    e.code = 'MODULE_NOT_FOUND';
    throw e;
})();
const mongodb_connection_string_url_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb-connection-string-url@7.0.0/node_modules/mongodb-connection-string-url/lib/index.js [app-client] (ecmascript)");
const url_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/next@16.0.6_@babel+core@7.28.5_react-dom@19.2.0_react@19.2.0__react@19.2.0/node_modules/next/dist/compiled/native-url/index.js [app-client] (ecmascript)");
const mongo_credentials_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cmap/auth/mongo_credentials.js [app-client] (ecmascript)");
const providers_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cmap/auth/providers.js [app-client] (ecmascript)");
const compression_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cmap/wire_protocol/compression.js [app-client] (ecmascript)");
const encrypter_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/encrypter.js [app-client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
const mongo_client_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/mongo_client.js [app-client] (ecmascript)");
const mongo_logger_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/mongo_logger.js [app-client] (ecmascript)");
const read_concern_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/read_concern.js [app-client] (ecmascript)");
const read_preference_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/read_preference.js [app-client] (ecmascript)");
const monitor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/sdam/monitor.js [app-client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/utils.js [app-client] (ecmascript)");
const write_concern_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/write_concern.js [app-client] (ecmascript)");
const VALID_TXT_RECORDS = [
    'authSource',
    'replicaSet',
    'loadBalanced'
];
const LB_SINGLE_HOST_ERROR = 'loadBalanced option only supported with a single host in the URI';
const LB_REPLICA_SET_ERROR = 'loadBalanced option not supported with a replicaSet option';
const LB_DIRECT_CONNECTION_ERROR = 'loadBalanced option not supported when directConnection is provided';
function retryDNSTimeoutFor(api) {
    return async function dnsReqRetryTimeout(lookupAddress) {
        try {
            return await dns.promises[api](lookupAddress);
        } catch (firstDNSError) {
            if (firstDNSError.code === dns.TIMEOUT) {
                return await dns.promises[api](lookupAddress);
            } else {
                throw firstDNSError;
            }
        }
    };
}
const resolveSrv = retryDNSTimeoutFor('resolveSrv');
const resolveTxt = retryDNSTimeoutFor('resolveTxt');
/**
 * Lookup a `mongodb+srv` connection string, combine the parts and reparse it as a normal
 * connection string.
 *
 * @param uri - The connection string to parse
 * @param options - Optional user provided connection string options
 */ async function resolveSRVRecord(options) {
    if (typeof options.srvHost !== 'string') {
        throw new error_1.MongoAPIError('Option "srvHost" must not be empty');
    }
    // Asynchronously start TXT resolution so that we do not have to wait until
    // the SRV record is resolved before starting a second DNS query.
    const lookupAddress = options.srvHost;
    const txtResolutionPromise = resolveTxt(lookupAddress);
    txtResolutionPromise.then(undefined, utils_1.squashError); // rejections will be handled later
    const hostname = `_${options.srvServiceName}._tcp.${lookupAddress}`;
    // Resolve the SRV record and use the result as the list of hosts to connect to.
    const addresses = await resolveSrv(hostname);
    if (addresses.length === 0) {
        throw new error_1.MongoAPIError('No addresses found at host');
    }
    for (const { name } of addresses){
        (0, utils_1.checkParentDomainMatch)(name, lookupAddress);
    }
    const hostAddresses = addresses.map((r)=>utils_1.HostAddress.fromString(`${r.name}:${r.port ?? 27017}`));
    validateLoadBalancedOptions(hostAddresses, options, true);
    // Use the result of resolving the TXT record and add options from there if they exist.
    let record;
    try {
        record = await txtResolutionPromise;
    } catch (error) {
        if (error.code !== 'ENODATA' && error.code !== 'ENOTFOUND') {
            throw error;
        }
        return hostAddresses;
    }
    if (record.length > 1) {
        throw new error_1.MongoParseError('Multiple text records not allowed');
    }
    const txtRecordOptions = new url_1.URLSearchParams(record[0].join(''));
    const txtRecordOptionKeys = [
        ...txtRecordOptions.keys()
    ];
    if (txtRecordOptionKeys.some((key)=>!VALID_TXT_RECORDS.includes(key))) {
        throw new error_1.MongoParseError(`Text record may only set any of: ${VALID_TXT_RECORDS.join(', ')}`);
    }
    if (VALID_TXT_RECORDS.some((option)=>txtRecordOptions.get(option) === '')) {
        throw new error_1.MongoParseError('Cannot have empty URI params in DNS TXT Record');
    }
    const source = txtRecordOptions.get('authSource') ?? undefined;
    const replicaSet = txtRecordOptions.get('replicaSet') ?? undefined;
    const loadBalanced = txtRecordOptions.get('loadBalanced') ?? undefined;
    if (!options.userSpecifiedAuthSource && source && options.credentials && !providers_1.AUTH_MECHS_AUTH_SRC_EXTERNAL.has(options.credentials.mechanism)) {
        options.credentials = mongo_credentials_1.MongoCredentials.merge(options.credentials, {
            source
        });
    }
    if (!options.userSpecifiedReplicaSet && replicaSet) {
        options.replicaSet = replicaSet;
    }
    if (loadBalanced === 'true') {
        options.loadBalanced = true;
    }
    if (options.replicaSet && options.srvMaxHosts > 0) {
        throw new error_1.MongoParseError('Cannot combine replicaSet option with srvMaxHosts');
    }
    validateLoadBalancedOptions(hostAddresses, options, true);
    return hostAddresses;
}
/**
 * Checks if TLS options are valid
 *
 * @param allOptions - All options provided by user or included in default options map
 * @throws MongoAPIError if TLS options are invalid
 */ function checkTLSOptions(allOptions) {
    if (!allOptions) return;
    const check = (a, b)=>{
        if (allOptions.has(a) && allOptions.has(b)) {
            throw new error_1.MongoAPIError(`The '${a}' option cannot be used with the '${b}' option`);
        }
    };
    check('tlsInsecure', 'tlsAllowInvalidCertificates');
    check('tlsInsecure', 'tlsAllowInvalidHostnames');
}
function getBoolean(name, value) {
    if (typeof value === 'boolean') return value;
    switch(value){
        case 'true':
            return true;
        case 'false':
            return false;
        default:
            throw new error_1.MongoParseError(`${name} must be either "true" or "false"`);
    }
}
function getIntFromOptions(name, value) {
    const parsedInt = (0, utils_1.parseInteger)(value);
    if (parsedInt != null) {
        return parsedInt;
    }
    throw new error_1.MongoParseError(`Expected ${name} to be stringified int value, got: ${value}`);
}
function getUIntFromOptions(name, value) {
    const parsedValue = getIntFromOptions(name, value);
    if (parsedValue < 0) {
        throw new error_1.MongoParseError(`${name} can only be a positive int value, got: ${value}`);
    }
    return parsedValue;
}
function* entriesFromString(value) {
    if (value === '') {
        return;
    }
    const keyValuePairs = value.split(',');
    for (const keyValue of keyValuePairs){
        const [key, value] = keyValue.split(/:(.*)/);
        if (value == null) {
            throw new error_1.MongoParseError('Cannot have undefined values in key value pairs');
        }
        yield [
            key,
            value
        ];
    }
}
class CaseInsensitiveMap extends Map {
    constructor(entries = []){
        super(entries.map(([k, v])=>[
                k.toLowerCase(),
                v
            ]));
    }
    has(k) {
        return super.has(k.toLowerCase());
    }
    get(k) {
        return super.get(k.toLowerCase());
    }
    set(k, v) {
        return super.set(k.toLowerCase(), v);
    }
    delete(k) {
        return super.delete(k.toLowerCase());
    }
}
function parseOptions(uri, mongoClient = undefined, options = {}) {
    if (mongoClient != null && !(mongoClient instanceof mongo_client_1.MongoClient)) {
        options = mongoClient;
        mongoClient = undefined;
    }
    // validate BSONOptions
    if (options.useBigInt64 && typeof options.promoteLongs === 'boolean' && !options.promoteLongs) {
        throw new error_1.MongoAPIError('Must request either bigint or Long for int64 deserialization');
    }
    if (options.useBigInt64 && typeof options.promoteValues === 'boolean' && !options.promoteValues) {
        throw new error_1.MongoAPIError('Must request either bigint or Long for int64 deserialization');
    }
    const url = new mongodb_connection_string_url_1.default(uri);
    const { hosts, isSRV } = url;
    const mongoOptions = Object.create(null);
    mongoOptions.hosts = isSRV ? [] : hosts.map(utils_1.HostAddress.fromString);
    const urlOptions = new CaseInsensitiveMap();
    if (url.pathname !== '/' && url.pathname !== '') {
        const dbName = decodeURIComponent(url.pathname[0] === '/' ? url.pathname.slice(1) : url.pathname);
        if (dbName) {
            urlOptions.set('dbName', [
                dbName
            ]);
        }
    }
    if (url.username !== '') {
        const auth = {
            username: decodeURIComponent(url.username)
        };
        if (typeof url.password === 'string') {
            auth.password = decodeURIComponent(url.password);
        }
        urlOptions.set('auth', [
            auth
        ]);
    }
    for (const key of url.searchParams.keys()){
        const values = url.searchParams.getAll(key);
        const isReadPreferenceTags = /readPreferenceTags/i.test(key);
        if (!isReadPreferenceTags && values.length > 1) {
            throw new error_1.MongoInvalidArgumentError(`URI option "${key}" cannot appear more than once in the connection string`);
        }
        if (!isReadPreferenceTags && values.includes('')) {
            throw new error_1.MongoAPIError(`URI option "${key}" cannot be specified with no value`);
        }
        if (!urlOptions.has(key)) {
            urlOptions.set(key, values);
        }
    }
    const objectOptions = new CaseInsensitiveMap(Object.entries(options).filter(([, v])=>v != null));
    // Validate options that can only be provided by one of uri or object
    if (urlOptions.has('serverApi')) {
        throw new error_1.MongoParseError('URI cannot contain `serverApi`, it can only be passed to the client');
    }
    const uriMechanismProperties = urlOptions.get('authMechanismProperties');
    if (uriMechanismProperties) {
        for (const property of uriMechanismProperties){
            if (/(^|,)ALLOWED_HOSTS:/.test(property)) {
                throw new error_1.MongoParseError('Auth mechanism property ALLOWED_HOSTS is not allowed in the connection string.');
            }
        }
    }
    if (objectOptions.has('loadBalanced')) {
        throw new error_1.MongoParseError('loadBalanced is only a valid option in the URI');
    }
    // All option collection
    const allProvidedOptions = new CaseInsensitiveMap();
    const allProvidedKeys = new Set([
        ...urlOptions.keys(),
        ...objectOptions.keys()
    ]);
    for (const key of allProvidedKeys){
        const values = [];
        const objectOptionValue = objectOptions.get(key);
        if (objectOptionValue != null) {
            values.push(objectOptionValue);
        }
        const urlValues = urlOptions.get(key) ?? [];
        values.push(...urlValues);
        allProvidedOptions.set(key, values);
    }
    if (allProvidedOptions.has('tls') || allProvidedOptions.has('ssl')) {
        const tlsAndSslOpts = (allProvidedOptions.get('tls') || []).concat(allProvidedOptions.get('ssl') || []).map(getBoolean.bind(null, 'tls/ssl'));
        if (new Set(tlsAndSslOpts).size !== 1) {
            throw new error_1.MongoParseError('All values of tls/ssl must be the same.');
        }
    }
    checkTLSOptions(allProvidedOptions);
    const unsupportedOptions = (0, utils_1.setDifference)(allProvidedKeys, Array.from(Object.keys(exports.OPTIONS)).map((s)=>s.toLowerCase()));
    if (unsupportedOptions.size !== 0) {
        const optionWord = unsupportedOptions.size > 1 ? 'options' : 'option';
        const isOrAre = unsupportedOptions.size > 1 ? 'are' : 'is';
        throw new error_1.MongoParseError(`${optionWord} ${Array.from(unsupportedOptions).join(', ')} ${isOrAre} not supported`);
    }
    // Option parsing and setting
    for (const [key, descriptor] of Object.entries(exports.OPTIONS)){
        const values = allProvidedOptions.get(key);
        if (!values || values.length === 0) {
            if (exports.DEFAULT_OPTIONS.has(key)) {
                setOption(mongoOptions, key, descriptor, [
                    exports.DEFAULT_OPTIONS.get(key)
                ]);
            }
        } else {
            const { deprecated } = descriptor;
            if (deprecated) {
                const deprecatedMsg = typeof deprecated === 'string' ? `: ${deprecated}` : '';
                (0, utils_1.emitWarning)(`${key} is a deprecated option${deprecatedMsg}`);
            }
            setOption(mongoOptions, key, descriptor, values);
        }
    }
    if (mongoOptions.credentials) {
        const isGssapi = mongoOptions.credentials.mechanism === providers_1.AuthMechanism.MONGODB_GSSAPI;
        const isX509 = mongoOptions.credentials.mechanism === providers_1.AuthMechanism.MONGODB_X509;
        const isAws = mongoOptions.credentials.mechanism === providers_1.AuthMechanism.MONGODB_AWS;
        const isOidc = mongoOptions.credentials.mechanism === providers_1.AuthMechanism.MONGODB_OIDC;
        if ((isGssapi || isX509) && allProvidedOptions.has('authSource') && mongoOptions.credentials.source !== '$external') {
            // If authSource was explicitly given and its incorrect, we error
            throw new error_1.MongoParseError(`authMechanism ${mongoOptions.credentials.mechanism} requires an authSource of '$external'`);
        }
        if (!(isGssapi || isX509 || isAws || isOidc) && mongoOptions.dbName && !allProvidedOptions.has('authSource')) {
            // inherit the dbName unless GSSAPI or X509, then silently ignore dbName
            // and there was no specific authSource given
            mongoOptions.credentials = mongo_credentials_1.MongoCredentials.merge(mongoOptions.credentials, {
                source: mongoOptions.dbName
            });
        }
        if (isAws) {
            const { username, password } = mongoOptions.credentials;
            if (username || password) {
                throw new error_1.MongoAPIError('username and password cannot be provided when using MONGODB-AWS. Credentials must be provided in a manner that can be read by the AWS SDK.');
            }
            if (mongoOptions.credentials.mechanismProperties.AWS_SESSION_TOKEN) {
                throw new error_1.MongoAPIError('AWS_SESSION_TOKEN cannot be provided when using MONGODB-AWS. Credentials must be provided in a manner that can be read by the AWS SDK.');
            }
        }
        mongoOptions.credentials.validate();
        // Check if the only auth related option provided was authSource, if so we can remove credentials
        if (mongoOptions.credentials.password === '' && mongoOptions.credentials.username === '' && mongoOptions.credentials.mechanism === providers_1.AuthMechanism.MONGODB_DEFAULT && Object.keys(mongoOptions.credentials.mechanismProperties).length === 0) {
            delete mongoOptions.credentials;
        }
    }
    if (!mongoOptions.dbName) {
        // dbName default is applied here because of the credential validation above
        mongoOptions.dbName = 'test';
    }
    validateLoadBalancedOptions(hosts, mongoOptions, isSRV);
    if (mongoClient && mongoOptions.autoEncryption) {
        encrypter_1.Encrypter.checkForMongoCrypt();
        mongoOptions.encrypter = new encrypter_1.Encrypter(mongoClient, uri, options);
        mongoOptions.autoEncrypter = mongoOptions.encrypter.autoEncrypter;
    }
    // Potential SRV Overrides and SRV connection string validations
    mongoOptions.userSpecifiedAuthSource = objectOptions.has('authSource') || urlOptions.has('authSource');
    mongoOptions.userSpecifiedReplicaSet = objectOptions.has('replicaSet') || urlOptions.has('replicaSet');
    if (isSRV) {
        // SRV Record is resolved upon connecting
        mongoOptions.srvHost = hosts[0];
        if (mongoOptions.directConnection) {
            throw new error_1.MongoAPIError('SRV URI does not support directConnection');
        }
        if (mongoOptions.srvMaxHosts > 0 && typeof mongoOptions.replicaSet === 'string') {
            throw new error_1.MongoParseError('Cannot use srvMaxHosts option with replicaSet');
        }
        // SRV turns on TLS by default, but users can override and turn it off
        const noUserSpecifiedTLS = !objectOptions.has('tls') && !urlOptions.has('tls');
        const noUserSpecifiedSSL = !objectOptions.has('ssl') && !urlOptions.has('ssl');
        if (noUserSpecifiedTLS && noUserSpecifiedSSL) {
            mongoOptions.tls = true;
        }
    } else {
        const userSpecifiedSrvOptions = urlOptions.has('srvMaxHosts') || objectOptions.has('srvMaxHosts') || urlOptions.has('srvServiceName') || objectOptions.has('srvServiceName');
        if (userSpecifiedSrvOptions) {
            throw new error_1.MongoParseError('Cannot use srvMaxHosts or srvServiceName with a non-srv connection string');
        }
    }
    if (mongoOptions.directConnection && mongoOptions.hosts.length !== 1) {
        throw new error_1.MongoParseError('directConnection option requires exactly one host');
    }
    if (!mongoOptions.proxyHost && (mongoOptions.proxyPort || mongoOptions.proxyUsername || mongoOptions.proxyPassword)) {
        throw new error_1.MongoParseError('Must specify proxyHost if other proxy options are passed');
    }
    if (mongoOptions.proxyUsername && !mongoOptions.proxyPassword || !mongoOptions.proxyUsername && mongoOptions.proxyPassword) {
        throw new error_1.MongoParseError('Can only specify both of proxy username/password or neither');
    }
    const proxyOptions = [
        'proxyHost',
        'proxyPort',
        'proxyUsername',
        'proxyPassword'
    ].map((key)=>urlOptions.get(key) ?? []);
    if (proxyOptions.some((options)=>options.length > 1)) {
        throw new error_1.MongoParseError('Proxy options cannot be specified multiple times in the connection string');
    }
    mongoOptions.mongoLoggerOptions = mongo_logger_1.MongoLogger.resolveOptions({
        MONGODB_LOG_COMMAND: __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].env.MONGODB_LOG_COMMAND,
        MONGODB_LOG_TOPOLOGY: __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].env.MONGODB_LOG_TOPOLOGY,
        MONGODB_LOG_SERVER_SELECTION: __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].env.MONGODB_LOG_SERVER_SELECTION,
        MONGODB_LOG_CONNECTION: __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].env.MONGODB_LOG_CONNECTION,
        MONGODB_LOG_CLIENT: __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].env.MONGODB_LOG_CLIENT,
        MONGODB_LOG_ALL: __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].env.MONGODB_LOG_ALL,
        MONGODB_LOG_MAX_DOCUMENT_LENGTH: __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].env.MONGODB_LOG_MAX_DOCUMENT_LENGTH,
        MONGODB_LOG_PATH: __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].env.MONGODB_LOG_PATH
    }, {
        mongodbLogPath: mongoOptions.mongodbLogPath,
        mongodbLogComponentSeverities: mongoOptions.mongodbLogComponentSeverities,
        mongodbLogMaxDocumentLength: mongoOptions.mongodbLogMaxDocumentLength
    });
    return mongoOptions;
}
/**
 * #### Throws if LB mode is true:
 * - hosts contains more than one host
 * - there is a replicaSet name set
 * - directConnection is set
 * - if srvMaxHosts is used when an srv connection string is passed in
 *
 * @throws MongoParseError
 */ function validateLoadBalancedOptions(hosts, mongoOptions, isSrv) {
    if (mongoOptions.loadBalanced) {
        if (hosts.length > 1) {
            throw new error_1.MongoParseError(LB_SINGLE_HOST_ERROR);
        }
        if (mongoOptions.replicaSet) {
            throw new error_1.MongoParseError(LB_REPLICA_SET_ERROR);
        }
        if (mongoOptions.directConnection) {
            throw new error_1.MongoParseError(LB_DIRECT_CONNECTION_ERROR);
        }
        if (isSrv && mongoOptions.srvMaxHosts > 0) {
            throw new error_1.MongoParseError('Cannot limit srv hosts with loadBalanced enabled');
        }
    }
    return;
}
function setOption(mongoOptions, key, descriptor, values) {
    const { target, type, transform } = descriptor;
    const name = target ?? key;
    switch(type){
        case 'boolean':
            mongoOptions[name] = getBoolean(name, values[0]);
            break;
        case 'int':
            mongoOptions[name] = getIntFromOptions(name, values[0]);
            break;
        case 'uint':
            mongoOptions[name] = getUIntFromOptions(name, values[0]);
            break;
        case 'string':
            if (values[0] == null) {
                break;
            }
            // The value should always be a string here, but since the array is typed as unknown
            // there still needs to be an explicit cast.
            // eslint-disable-next-line @typescript-eslint/no-base-to-string
            mongoOptions[name] = String(values[0]);
            break;
        case 'record':
            if (!(0, utils_1.isRecord)(values[0])) {
                throw new error_1.MongoParseError(`${name} must be an object`);
            }
            mongoOptions[name] = values[0];
            break;
        case 'any':
            mongoOptions[name] = values[0];
            break;
        default:
            {
                if (!transform) {
                    throw new error_1.MongoParseError('Descriptors missing a type must define a transform');
                }
                const transformValue = transform({
                    name,
                    options: mongoOptions,
                    values
                });
                mongoOptions[name] = transformValue;
                break;
            }
    }
}
exports.OPTIONS = {
    appName: {
        type: 'string'
    },
    auth: {
        target: 'credentials',
        transform ({ name, options, values: [value] }) {
            if (!(0, utils_1.isRecord)(value, [
                'username',
                'password'
            ])) {
                throw new error_1.MongoParseError(`${name} must be an object with 'username' and 'password' properties`);
            }
            return mongo_credentials_1.MongoCredentials.merge(options.credentials, {
                username: value.username,
                password: value.password
            });
        }
    },
    authMechanism: {
        target: 'credentials',
        transform ({ options, values: [value] }) {
            const mechanisms = Object.values(providers_1.AuthMechanism);
            const [mechanism] = mechanisms.filter((m)=>m.match(RegExp(String.raw`\b${value}\b`, 'i')));
            if (!mechanism) {
                throw new error_1.MongoParseError(`authMechanism one of ${mechanisms}, got ${value}`);
            }
            let source = options.credentials?.source;
            if (mechanism === providers_1.AuthMechanism.MONGODB_PLAIN || providers_1.AUTH_MECHS_AUTH_SRC_EXTERNAL.has(mechanism)) {
                // some mechanisms have '$external' as the Auth Source
                source = '$external';
            }
            let password = options.credentials?.password;
            if (mechanism === providers_1.AuthMechanism.MONGODB_X509 && password === '') {
                password = undefined;
            }
            return mongo_credentials_1.MongoCredentials.merge(options.credentials, {
                mechanism,
                source,
                password
            });
        }
    },
    // Note that if the authMechanismProperties contain a TOKEN_RESOURCE that has a
    // comma in it, it MUST be supplied as a MongoClient option instead of in the
    // connection string.
    authMechanismProperties: {
        target: 'credentials',
        transform ({ options, values }) {
            // We can have a combination of options passed in the URI and options passed
            // as an object to the MongoClient. So we must transform the string options
            // as well as merge them together with a potentially provided object.
            let mechanismProperties = Object.create(null);
            for (const optionValue of values){
                if (typeof optionValue === 'string') {
                    for (const [key, value] of entriesFromString(optionValue)){
                        try {
                            mechanismProperties[key] = getBoolean(key, value);
                        } catch  {
                            mechanismProperties[key] = value;
                        }
                    }
                } else {
                    if (!(0, utils_1.isRecord)(optionValue)) {
                        throw new error_1.MongoParseError('AuthMechanismProperties must be an object');
                    }
                    mechanismProperties = {
                        ...optionValue
                    };
                }
            }
            return mongo_credentials_1.MongoCredentials.merge(options.credentials, {
                mechanismProperties
            });
        }
    },
    authSource: {
        target: 'credentials',
        transform ({ options, values: [value] }) {
            const source = String(value);
            return mongo_credentials_1.MongoCredentials.merge(options.credentials, {
                source
            });
        }
    },
    autoEncryption: {
        type: 'record'
    },
    autoSelectFamily: {
        type: 'boolean',
        default: true
    },
    autoSelectFamilyAttemptTimeout: {
        type: 'uint'
    },
    bsonRegExp: {
        type: 'boolean'
    },
    serverApi: {
        target: 'serverApi',
        transform ({ values: [version] }) {
            const serverApiToValidate = typeof version === 'string' ? {
                version
            } : version;
            const versionToValidate = serverApiToValidate && serverApiToValidate.version;
            if (!versionToValidate) {
                throw new error_1.MongoParseError(`Invalid \`serverApi\` property; must specify a version from the following enum: ["${Object.values(mongo_client_1.ServerApiVersion).join('", "')}"]`);
            }
            if (!Object.values(mongo_client_1.ServerApiVersion).some((v)=>v === versionToValidate)) {
                throw new error_1.MongoParseError(`Invalid server API version=${versionToValidate}; must be in the following enum: ["${Object.values(mongo_client_1.ServerApiVersion).join('", "')}"]`);
            }
            return serverApiToValidate;
        }
    },
    checkKeys: {
        type: 'boolean'
    },
    compressors: {
        default: 'none',
        target: 'compressors',
        transform ({ values }) {
            const compressionList = new Set();
            for (const compVal of values){
                const compValArray = typeof compVal === 'string' ? compVal.split(',') : compVal;
                if (!Array.isArray(compValArray)) {
                    throw new error_1.MongoInvalidArgumentError('compressors must be an array or a comma-delimited list of strings');
                }
                for (const c of compValArray){
                    if (Object.keys(compression_1.Compressor).includes(String(c))) {
                        compressionList.add(String(c));
                    } else {
                        throw new error_1.MongoInvalidArgumentError(`${c} is not a valid compression mechanism. Must be one of: ${Object.keys(compression_1.Compressor)}.`);
                    }
                }
            }
            return [
                ...compressionList
            ];
        }
    },
    connectTimeoutMS: {
        default: 30000,
        type: 'uint'
    },
    dbName: {
        type: 'string'
    },
    directConnection: {
        default: false,
        type: 'boolean'
    },
    driverInfo: {
        default: {},
        type: 'record'
    },
    enableUtf8Validation: {
        type: 'boolean',
        default: true
    },
    family: {
        transform ({ name, values: [value] }) {
            const transformValue = getIntFromOptions(name, value);
            if (transformValue === 4 || transformValue === 6) {
                return transformValue;
            }
            throw new error_1.MongoParseError(`Option 'family' must be 4 or 6 got ${transformValue}.`);
        }
    },
    fieldsAsRaw: {
        type: 'record'
    },
    forceServerObjectId: {
        default: false,
        type: 'boolean'
    },
    fsync: {
        deprecated: 'Please use journal instead',
        target: 'writeConcern',
        transform ({ name, options, values: [value] }) {
            const wc = write_concern_1.WriteConcern.fromOptions({
                writeConcern: {
                    ...options.writeConcern,
                    fsync: getBoolean(name, value)
                }
            });
            if (!wc) throw new error_1.MongoParseError(`Unable to make a writeConcern from fsync=${value}`);
            return wc;
        }
    },
    heartbeatFrequencyMS: {
        default: 10000,
        type: 'uint'
    },
    ignoreUndefined: {
        type: 'boolean'
    },
    j: {
        deprecated: 'Please use journal instead',
        target: 'writeConcern',
        transform ({ name, options, values: [value] }) {
            const wc = write_concern_1.WriteConcern.fromOptions({
                writeConcern: {
                    ...options.writeConcern,
                    journal: getBoolean(name, value)
                }
            });
            if (!wc) throw new error_1.MongoParseError(`Unable to make a writeConcern from journal=${value}`);
            return wc;
        }
    },
    journal: {
        target: 'writeConcern',
        transform ({ name, options, values: [value] }) {
            const wc = write_concern_1.WriteConcern.fromOptions({
                writeConcern: {
                    ...options.writeConcern,
                    journal: getBoolean(name, value)
                }
            });
            if (!wc) throw new error_1.MongoParseError(`Unable to make a writeConcern from journal=${value}`);
            return wc;
        }
    },
    loadBalanced: {
        default: false,
        type: 'boolean'
    },
    localThresholdMS: {
        default: 15,
        type: 'uint'
    },
    maxConnecting: {
        default: 2,
        transform ({ name, values: [value] }) {
            const maxConnecting = getUIntFromOptions(name, value);
            if (maxConnecting === 0) {
                throw new error_1.MongoInvalidArgumentError('maxConnecting must be > 0 if specified');
            }
            return maxConnecting;
        }
    },
    maxIdleTimeMS: {
        default: 0,
        type: 'uint'
    },
    maxPoolSize: {
        default: 100,
        type: 'uint'
    },
    maxStalenessSeconds: {
        target: 'readPreference',
        transform ({ name, options, values: [value] }) {
            const maxStalenessSeconds = getUIntFromOptions(name, value);
            if (options.readPreference) {
                return read_preference_1.ReadPreference.fromOptions({
                    readPreference: {
                        ...options.readPreference,
                        maxStalenessSeconds
                    }
                });
            } else {
                return new read_preference_1.ReadPreference('secondary', undefined, {
                    maxStalenessSeconds
                });
            }
        }
    },
    minInternalBufferSize: {
        type: 'uint'
    },
    minPoolSize: {
        default: 0,
        type: 'uint'
    },
    minHeartbeatFrequencyMS: {
        default: 500,
        type: 'uint'
    },
    monitorCommands: {
        default: false,
        type: 'boolean'
    },
    name: {
        target: 'driverInfo',
        transform ({ values: [value], options }) {
            return {
                ...options.driverInfo,
                name: String(value)
            };
        }
    },
    noDelay: {
        default: true,
        type: 'boolean'
    },
    pkFactory: {
        default: utils_1.DEFAULT_PK_FACTORY,
        transform ({ values: [value] }) {
            if ((0, utils_1.isRecord)(value, [
                'createPk'
            ]) && typeof value.createPk === 'function') {
                return value;
            }
            throw new error_1.MongoParseError(`Option pkFactory must be an object with a createPk function, got ${value}`);
        }
    },
    promoteBuffers: {
        type: 'boolean'
    },
    promoteLongs: {
        type: 'boolean'
    },
    promoteValues: {
        type: 'boolean'
    },
    useBigInt64: {
        type: 'boolean'
    },
    proxyHost: {
        type: 'string'
    },
    proxyPassword: {
        type: 'string'
    },
    proxyPort: {
        type: 'uint'
    },
    proxyUsername: {
        type: 'string'
    },
    raw: {
        default: false,
        type: 'boolean'
    },
    readConcern: {
        transform ({ values: [value], options }) {
            if (value instanceof read_concern_1.ReadConcern || (0, utils_1.isRecord)(value, [
                'level'
            ])) {
                return read_concern_1.ReadConcern.fromOptions({
                    ...options.readConcern,
                    ...value
                });
            }
            throw new error_1.MongoParseError(`ReadConcern must be an object, got ${JSON.stringify(value)}`);
        }
    },
    readConcernLevel: {
        target: 'readConcern',
        transform ({ values: [level], options }) {
            return read_concern_1.ReadConcern.fromOptions({
                ...options.readConcern,
                level: level
            });
        }
    },
    readPreference: {
        default: read_preference_1.ReadPreference.primary,
        transform ({ values: [value], options }) {
            if (value instanceof read_preference_1.ReadPreference) {
                return read_preference_1.ReadPreference.fromOptions({
                    readPreference: {
                        ...options.readPreference,
                        ...value
                    },
                    ...value
                });
            }
            if ((0, utils_1.isRecord)(value, [
                'mode'
            ])) {
                const rp = read_preference_1.ReadPreference.fromOptions({
                    readPreference: {
                        ...options.readPreference,
                        ...value
                    },
                    ...value
                });
                if (rp) return rp;
                else throw new error_1.MongoParseError(`Cannot make read preference from ${JSON.stringify(value)}`);
            }
            if (typeof value === 'string') {
                const rpOpts = {
                    hedge: options.readPreference?.hedge,
                    maxStalenessSeconds: options.readPreference?.maxStalenessSeconds
                };
                return new read_preference_1.ReadPreference(value, options.readPreference?.tags, rpOpts);
            }
            throw new error_1.MongoParseError(`Unknown ReadPreference value: ${value}`);
        }
    },
    readPreferenceTags: {
        target: 'readPreference',
        transform ({ values, options }) {
            const tags = Array.isArray(values[0]) ? values[0] : values;
            const readPreferenceTags = [];
            for (const tag of tags){
                const readPreferenceTag = Object.create(null);
                if (typeof tag === 'string') {
                    for (const [k, v] of entriesFromString(tag)){
                        readPreferenceTag[k] = v;
                    }
                }
                if ((0, utils_1.isRecord)(tag)) {
                    for (const [k, v] of Object.entries(tag)){
                        readPreferenceTag[k] = v;
                    }
                }
                readPreferenceTags.push(readPreferenceTag);
            }
            return read_preference_1.ReadPreference.fromOptions({
                readPreference: options.readPreference,
                readPreferenceTags
            });
        }
    },
    replicaSet: {
        type: 'string'
    },
    retryReads: {
        default: true,
        type: 'boolean'
    },
    retryWrites: {
        default: true,
        type: 'boolean'
    },
    serializeFunctions: {
        type: 'boolean'
    },
    serverMonitoringMode: {
        default: 'auto',
        transform ({ values: [value] }) {
            if (!Object.values(monitor_1.ServerMonitoringMode).includes(value)) {
                throw new error_1.MongoParseError('serverMonitoringMode must be one of `auto`, `poll`, or `stream`');
            }
            return value;
        }
    },
    serverSelectionTimeoutMS: {
        default: 30000,
        type: 'uint'
    },
    servername: {
        type: 'string'
    },
    socketTimeoutMS: {
        // TODO(NODE-6491): deprecated: 'Please use timeoutMS instead',
        default: 0,
        type: 'uint'
    },
    srvMaxHosts: {
        type: 'uint',
        default: 0
    },
    srvServiceName: {
        type: 'string',
        default: 'mongodb'
    },
    ssl: {
        target: 'tls',
        type: 'boolean'
    },
    timeoutMS: {
        type: 'uint'
    },
    tls: {
        type: 'boolean'
    },
    tlsAllowInvalidCertificates: {
        target: 'rejectUnauthorized',
        transform ({ name, values: [value] }) {
            // allowInvalidCertificates is the inverse of rejectUnauthorized
            return !getBoolean(name, value);
        }
    },
    tlsAllowInvalidHostnames: {
        target: 'checkServerIdentity',
        transform ({ name, values: [value] }) {
            // tlsAllowInvalidHostnames means setting the checkServerIdentity function to a noop
            return getBoolean(name, value) ? ()=>undefined : undefined;
        }
    },
    tlsCAFile: {
        type: 'string'
    },
    tlsCRLFile: {
        type: 'string'
    },
    tlsCertificateKeyFile: {
        type: 'string'
    },
    tlsCertificateKeyFilePassword: {
        target: 'passphrase',
        type: 'any'
    },
    tlsInsecure: {
        transform ({ name, options, values: [value] }) {
            const tlsInsecure = getBoolean(name, value);
            if (tlsInsecure) {
                options.checkServerIdentity = ()=>undefined;
                options.rejectUnauthorized = false;
            } else {
                options.checkServerIdentity = options.tlsAllowInvalidHostnames ? ()=>undefined : undefined;
                options.rejectUnauthorized = options.tlsAllowInvalidCertificates ? false : true;
            }
            return tlsInsecure;
        }
    },
    w: {
        target: 'writeConcern',
        transform ({ values: [value], options }) {
            return write_concern_1.WriteConcern.fromOptions({
                writeConcern: {
                    ...options.writeConcern,
                    w: value
                }
            });
        }
    },
    waitQueueTimeoutMS: {
        // TODO(NODE-6491): deprecated: 'Please use timeoutMS instead',
        default: 0,
        type: 'uint'
    },
    writeConcern: {
        target: 'writeConcern',
        transform ({ values: [value], options }) {
            if ((0, utils_1.isRecord)(value) || value instanceof write_concern_1.WriteConcern) {
                return write_concern_1.WriteConcern.fromOptions({
                    writeConcern: {
                        ...options.writeConcern,
                        ...value
                    }
                });
            } else if (value === 'majority' || typeof value === 'number') {
                return write_concern_1.WriteConcern.fromOptions({
                    writeConcern: {
                        ...options.writeConcern,
                        w: value
                    }
                });
            }
            throw new error_1.MongoParseError(`Invalid WriteConcern cannot parse: ${JSON.stringify(value)}`);
        }
    },
    wtimeout: {
        deprecated: 'Please use wtimeoutMS instead',
        target: 'writeConcern',
        transform ({ values: [value], options }) {
            const wc = write_concern_1.WriteConcern.fromOptions({
                writeConcern: {
                    ...options.writeConcern,
                    wtimeout: getUIntFromOptions('wtimeout', value)
                }
            });
            if (wc) return wc;
            throw new error_1.MongoParseError(`Cannot make WriteConcern from wtimeout`);
        }
    },
    wtimeoutMS: {
        target: 'writeConcern',
        transform ({ values: [value], options }) {
            const wc = write_concern_1.WriteConcern.fromOptions({
                writeConcern: {
                    ...options.writeConcern,
                    wtimeoutMS: getUIntFromOptions('wtimeoutMS', value)
                }
            });
            if (wc) return wc;
            throw new error_1.MongoParseError(`Cannot make WriteConcern from wtimeout`);
        }
    },
    zlibCompressionLevel: {
        default: 0,
        type: 'int'
    },
    mongodbLogPath: {
        transform ({ values: [value] }) {
            if (!(typeof value === 'string' && [
                'stderr',
                'stdout'
            ].includes(value) || value && typeof value === 'object' && 'write' in value && typeof value.write === 'function')) {
                throw new error_1.MongoAPIError(`Option 'mongodbLogPath' must be of type 'stderr' | 'stdout' | MongoDBLogWritable`);
            }
            return value;
        }
    },
    mongodbLogComponentSeverities: {
        transform ({ values: [value] }) {
            if (typeof value !== 'object' || !value) {
                throw new error_1.MongoAPIError(`Option 'mongodbLogComponentSeverities' must be a non-null object`);
            }
            for (const [k, v] of Object.entries(value)){
                if (typeof v !== 'string' || typeof k !== 'string') {
                    throw new error_1.MongoAPIError(`User input for option 'mongodbLogComponentSeverities' object cannot include a non-string key or value`);
                }
                if (!Object.values(mongo_logger_1.MongoLoggableComponent).some((val)=>val === k) && k !== 'default') {
                    throw new error_1.MongoAPIError(`User input for option 'mongodbLogComponentSeverities' contains invalid key: ${k}`);
                }
                if (!Object.values(mongo_logger_1.SeverityLevel).some((val)=>val === v)) {
                    throw new error_1.MongoAPIError(`Option 'mongodbLogComponentSeverities' does not support ${v} as a value for ${k}`);
                }
            }
            return value;
        }
    },
    mongodbLogMaxDocumentLength: {
        type: 'uint'
    },
    // Custom types for modifying core behavior
    connectionType: {
        type: 'any'
    },
    srvPoller: {
        type: 'any'
    },
    // Accepted Node.js Options
    allowPartialTrustChain: {
        type: 'any'
    },
    minDHSize: {
        type: 'any'
    },
    pskCallback: {
        type: 'any'
    },
    secureContext: {
        type: 'any'
    },
    enableTrace: {
        type: 'any'
    },
    requestCert: {
        type: 'any'
    },
    rejectUnauthorized: {
        type: 'any'
    },
    checkServerIdentity: {
        type: 'any'
    },
    keepAliveInitialDelay: {
        type: 'any'
    },
    ALPNProtocols: {
        type: 'any'
    },
    SNICallback: {
        type: 'any'
    },
    session: {
        type: 'any'
    },
    requestOCSP: {
        type: 'any'
    },
    localAddress: {
        type: 'any'
    },
    localPort: {
        type: 'any'
    },
    hints: {
        type: 'any'
    },
    lookup: {
        type: 'any'
    },
    ca: {
        type: 'any'
    },
    cert: {
        type: 'any'
    },
    ciphers: {
        type: 'any'
    },
    crl: {
        type: 'any'
    },
    ecdhCurve: {
        type: 'any'
    },
    key: {
        type: 'any'
    },
    passphrase: {
        type: 'any'
    },
    pfx: {
        type: 'any'
    },
    secureProtocol: {
        type: 'any'
    },
    index: {
        type: 'any'
    },
    // Legacy options from v3 era
    __skipPingOnConnect: {
        type: 'boolean'
    }
};
exports.DEFAULT_OPTIONS = new CaseInsensitiveMap(Object.entries(exports.OPTIONS).filter(([, descriptor])=>descriptor.default != null).map(([k, d])=>[
        k,
        d.default
    ])); //# sourceMappingURL=connection_string.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/mongo_client_auth_providers.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.MongoClientAuthProviders = void 0;
const gssapi_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cmap/auth/gssapi.js [app-client] (ecmascript)");
const mongodb_aws_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cmap/auth/mongodb_aws.js [app-client] (ecmascript)");
const mongodb_oidc_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cmap/auth/mongodb_oidc.js [app-client] (ecmascript)");
const automated_callback_workflow_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/automated_callback_workflow.js [app-client] (ecmascript)");
const human_callback_workflow_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/human_callback_workflow.js [app-client] (ecmascript)");
const token_cache_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/token_cache.js [app-client] (ecmascript)");
const plain_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cmap/auth/plain.js [app-client] (ecmascript)");
const providers_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cmap/auth/providers.js [app-client] (ecmascript)");
const scram_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cmap/auth/scram.js [app-client] (ecmascript)");
const x509_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cmap/auth/x509.js [app-client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
/** @internal */ const AUTH_PROVIDERS = new Map([
    [
        providers_1.AuthMechanism.MONGODB_AWS,
        ({ AWS_CREDENTIAL_PROVIDER })=>new mongodb_aws_1.MongoDBAWS(AWS_CREDENTIAL_PROVIDER)
    ],
    [
        providers_1.AuthMechanism.MONGODB_GSSAPI,
        ()=>new gssapi_1.GSSAPI()
    ],
    [
        providers_1.AuthMechanism.MONGODB_OIDC,
        (properties)=>new mongodb_oidc_1.MongoDBOIDC(getWorkflow(properties))
    ],
    [
        providers_1.AuthMechanism.MONGODB_PLAIN,
        ()=>new plain_1.Plain()
    ],
    [
        providers_1.AuthMechanism.MONGODB_SCRAM_SHA1,
        ()=>new scram_1.ScramSHA1()
    ],
    [
        providers_1.AuthMechanism.MONGODB_SCRAM_SHA256,
        ()=>new scram_1.ScramSHA256()
    ],
    [
        providers_1.AuthMechanism.MONGODB_X509,
        ()=>new x509_1.X509()
    ]
]);
/**
 * Create a set of providers per client
 * to avoid sharing the provider's cache between different clients.
 * @internal
 */ class MongoClientAuthProviders {
    constructor(){
        this.existingProviders = new Map();
    }
    /**
     * Get or create an authentication provider based on the provided mechanism.
     * We don't want to create all providers at once, as some providers may not be used.
     * @param name - The name of the provider to get or create.
     * @param credentials - The credentials.
     * @returns The provider.
     * @throws MongoInvalidArgumentError if the mechanism is not supported.
     * @internal
     */ getOrCreateProvider(name, authMechanismProperties) {
        const authProvider = this.existingProviders.get(name);
        if (authProvider) {
            return authProvider;
        }
        const providerFunction = AUTH_PROVIDERS.get(name);
        if (!providerFunction) {
            throw new error_1.MongoInvalidArgumentError(`authMechanism ${name} not supported`);
        }
        const provider = providerFunction(authMechanismProperties);
        this.existingProviders.set(name, provider);
        return provider;
    }
}
exports.MongoClientAuthProviders = MongoClientAuthProviders;
/**
 * Gets either a device workflow or callback workflow.
 */ function getWorkflow(authMechanismProperties) {
    if (authMechanismProperties.OIDC_HUMAN_CALLBACK) {
        return new human_callback_workflow_1.HumanCallbackWorkflow(new token_cache_1.TokenCache(), authMechanismProperties.OIDC_HUMAN_CALLBACK);
    } else if (authMechanismProperties.OIDC_CALLBACK) {
        return new automated_callback_workflow_1.AutomatedCallbackWorkflow(new token_cache_1.TokenCache(), authMechanismProperties.OIDC_CALLBACK);
    } else {
        const environment = authMechanismProperties.ENVIRONMENT;
        const workflow = mongodb_oidc_1.OIDC_WORKFLOWS.get(environment)?.();
        if (!workflow) {
            throw new error_1.MongoInvalidArgumentError(`Could not load workflow for environment ${authMechanismProperties.ENVIRONMENT}`);
        }
        return workflow;
    }
} //# sourceMappingURL=mongo_client_auth_providers.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/mongo_client.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.MongoClient = exports.ServerApiVersion = void 0;
const fs_1 = (()=>{
    const e = new Error("Cannot find module 'fs'");
    e.code = 'MODULE_NOT_FOUND';
    throw e;
})();
const _1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/index.js [app-client] (ecmascript)");
const bson_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bson.js [app-client] (ecmascript)");
const change_stream_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/change_stream.js [app-client] (ecmascript)");
const mongo_credentials_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cmap/auth/mongo_credentials.js [app-client] (ecmascript)");
const providers_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cmap/auth/providers.js [app-client] (ecmascript)");
const client_metadata_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cmap/handshake/client_metadata.js [app-client] (ecmascript)");
const connection_string_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/connection_string.js [app-client] (ecmascript)");
const constants_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/constants.js [app-client] (ecmascript)");
const db_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/db.js [app-client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
const mongo_client_auth_providers_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/mongo_client_auth_providers.js [app-client] (ecmascript)");
const mongo_logger_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/mongo_logger.js [app-client] (ecmascript)");
const mongo_types_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/mongo_types.js [app-client] (ecmascript)");
const executor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/client_bulk_write/executor.js [app-client] (ecmascript)");
const end_sessions_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/end_sessions.js [app-client] (ecmascript)");
const execute_operation_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/execute_operation.js [app-client] (ecmascript)");
const read_preference_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/read_preference.js [app-client] (ecmascript)");
const server_selection_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/sdam/server_selection.js [app-client] (ecmascript)");
const topology_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/sdam/topology.js [app-client] (ecmascript)");
const sessions_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/sessions.js [app-client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/utils.js [app-client] (ecmascript)");
/** @public */ exports.ServerApiVersion = Object.freeze({
    v1: '1'
});
/**
 * @public
 *
 * The **MongoClient** class is a class that allows for making Connections to MongoDB.
 *
 * **NOTE:** The programmatically provided options take precedence over the URI options.
 *
 * @remarks
 *
 * A MongoClient is the entry point to connecting to a MongoDB server.
 *
 * It handles a multitude of features on your application's behalf:
 * - **Server Host Connection Configuration**: A MongoClient is responsible for reading TLS cert, ca, and crl files if provided.
 * - **SRV Record Polling**: A "`mongodb+srv`" style connection string is used to have the MongoClient resolve DNS SRV records of all server hostnames which the driver periodically monitors for changes and adjusts its current view of hosts correspondingly.
 * - **Server Monitoring**: The MongoClient automatically keeps monitoring the health of server nodes in your cluster to reach out to the correct and lowest latency one available.
 * - **Connection Pooling**: To avoid paying the cost of rebuilding a connection to the server on every operation the MongoClient keeps idle connections preserved for reuse.
 * - **Session Pooling**: The MongoClient creates logical sessions that enable retryable writes, causal consistency, and transactions. It handles pooling these sessions for reuse in subsequent operations.
 * - **Cursor Operations**: A MongoClient's cursors use the health monitoring system to send the request for more documents to the same server the query began on.
 * - **Mongocryptd process**: When using auto encryption, a MongoClient will launch a `mongocryptd` instance for handling encryption if the mongocrypt shared library isn't in use.
 *
 * There are many more features of a MongoClient that are not listed above.
 *
 * In order to enable these features, a number of asynchronous Node.js resources are established by the driver: Timers, FS Requests, Sockets, etc.
 * For details on cleanup, please refer to the MongoClient `close()` documentation.
 *
 * @example
 * ```ts
 * import { MongoClient } from 'mongodb';
 * // Enable command monitoring for debugging
 * const client = new MongoClient('mongodb://localhost:27017?appName=mflix', { monitorCommands: true });
 * ```
 */ class MongoClient extends mongo_types_1.TypedEventEmitter {
    constructor(url, options){
        super();
        this.driverInfoList = [];
        this.on('error', utils_1.noop);
        this.options = (0, connection_string_1.parseOptions)(url, this, options);
        this.appendMetadata(this.options.driverInfo);
        const shouldSetLogger = Object.values(this.options.mongoLoggerOptions.componentSeverities).some((value)=>value !== mongo_logger_1.SeverityLevel.OFF);
        this.mongoLogger = shouldSetLogger ? new mongo_logger_1.MongoLogger(this.options.mongoLoggerOptions) : undefined;
        // eslint-disable-next-line @typescript-eslint/no-this-alias
        const client = this;
        // The internal state
        this.s = {
            url,
            bsonOptions: (0, bson_1.resolveBSONOptions)(this.options),
            namespace: (0, utils_1.ns)('admin'),
            hasBeenClosed: false,
            sessionPool: new sessions_1.ServerSessionPool(this),
            activeSessions: new Set(),
            activeCursors: new Set(),
            authProviders: new mongo_client_auth_providers_1.MongoClientAuthProviders(),
            get options () {
                return client.options;
            },
            get readConcern () {
                return client.options.readConcern;
            },
            get writeConcern () {
                return client.options.writeConcern;
            },
            get readPreference () {
                return client.options.readPreference;
            },
            get isMongoClient () {
                return true;
            }
        };
        this.checkForNonGenuineHosts();
    }
    /**
     * @experimental
     * An alias for {@link MongoClient.close|MongoClient.close()}.
     */ async [Symbol.asyncDispose]() {
        await this.close();
    }
    /**
     * Append metadata to the client metadata after instantiation.
     * @param driverInfo - Information about the application or library.
     */ appendMetadata(driverInfo) {
        const isDuplicateDriverInfo = this.driverInfoList.some((info)=>(0, client_metadata_1.isDriverInfoEqual)(info, driverInfo));
        if (isDuplicateDriverInfo) return;
        this.driverInfoList.push(driverInfo);
        this.options.metadata = (0, client_metadata_1.makeClientMetadata)(this.driverInfoList, this.options).then(undefined, utils_1.squashError).then((result)=>result ?? {}); // ensure Promise<Document>
    }
    /** @internal */ checkForNonGenuineHosts() {
        const documentDBHostnames = this.options.hosts.filter((hostAddress)=>(0, utils_1.isHostMatch)(utils_1.DOCUMENT_DB_CHECK, hostAddress.host));
        const srvHostIsDocumentDB = (0, utils_1.isHostMatch)(utils_1.DOCUMENT_DB_CHECK, this.options.srvHost);
        const cosmosDBHostnames = this.options.hosts.filter((hostAddress)=>(0, utils_1.isHostMatch)(utils_1.COSMOS_DB_CHECK, hostAddress.host));
        const srvHostIsCosmosDB = (0, utils_1.isHostMatch)(utils_1.COSMOS_DB_CHECK, this.options.srvHost);
        if (documentDBHostnames.length !== 0 || srvHostIsDocumentDB) {
            this.mongoLogger?.info('client', utils_1.DOCUMENT_DB_MSG);
        } else if (cosmosDBHostnames.length !== 0 || srvHostIsCosmosDB) {
            this.mongoLogger?.info('client', utils_1.COSMOS_DB_MSG);
        }
    }
    get serverApi() {
        return this.options.serverApi && Object.freeze({
            ...this.options.serverApi
        });
    }
    /**
     * Intended for APM use only
     * @internal
     */ get monitorCommands() {
        return this.options.monitorCommands;
    }
    set monitorCommands(value) {
        this.options.monitorCommands = value;
    }
    /** @internal */ get autoEncrypter() {
        return this.options.autoEncrypter;
    }
    get readConcern() {
        return this.s.readConcern;
    }
    get writeConcern() {
        return this.s.writeConcern;
    }
    get readPreference() {
        return this.s.readPreference;
    }
    get bsonOptions() {
        return this.s.bsonOptions;
    }
    get timeoutMS() {
        return this.s.options.timeoutMS;
    }
    /**
     * Executes a client bulk write operation, available on server 8.0+.
     * @param models - The client bulk write models.
     * @param options - The client bulk write options.
     * @returns A ClientBulkWriteResult for acknowledged writes and ok: 1 for unacknowledged writes.
     */ async bulkWrite(models, options) {
        if (this.autoEncrypter) {
            throw new error_1.MongoInvalidArgumentError('MongoClient bulkWrite does not currently support automatic encryption.');
        }
        // We do not need schema type information past this point ("as any" is fine)
        return await new executor_1.ClientBulkWriteExecutor(this, models, (0, utils_1.resolveOptions)(this, options)).execute();
    }
    /**
     * An optional method to verify a handful of assumptions that are generally useful at application boot-time before using a MongoClient.
     * For detailed information about the connect process see the MongoClient.connect static method documentation.
     *
     * @param url - The MongoDB connection string (supports `mongodb://` and `mongodb+srv://` schemes)
     * @param options - Optional configuration options for the client
     *
     * @see https://www.mongodb.com/docs/manual/reference/connection-string/
     */ async connect() {
        if (this.connectionLock) {
            return await this.connectionLock;
        }
        try {
            this.connectionLock = this._connect();
            await this.connectionLock;
        } finally{
            // release
            this.connectionLock = undefined;
        }
        return this;
    }
    /**
     * Create a topology to open the connection, must be locked to avoid topology leaks in concurrency scenario.
     * Locking is enforced by the connect method.
     *
     * @internal
     */ async _connect() {
        if (this.topology && this.topology.isConnected()) {
            return this;
        }
        const options = this.options;
        if (options.tls) {
            if (typeof options.tlsCAFile === 'string') {
                options.ca ??= await fs_1.promises.readFile(options.tlsCAFile);
            }
            if (typeof options.tlsCRLFile === 'string') {
                options.crl ??= await fs_1.promises.readFile(options.tlsCRLFile);
            }
            if (typeof options.tlsCertificateKeyFile === 'string') {
                if (!options.key || !options.cert) {
                    const contents = await fs_1.promises.readFile(options.tlsCertificateKeyFile);
                    options.key ??= contents;
                    options.cert ??= contents;
                }
            }
        }
        if (typeof options.srvHost === 'string') {
            const hosts = await (0, connection_string_1.resolveSRVRecord)(options);
            for (const [index, host] of hosts.entries()){
                options.hosts[index] = host;
            }
        }
        // It is important to perform validation of hosts AFTER SRV resolution, to check the real hostname,
        // but BEFORE we even attempt connecting with a potentially not allowed hostname
        if (options.credentials?.mechanism === providers_1.AuthMechanism.MONGODB_OIDC) {
            const allowedHosts = options.credentials?.mechanismProperties?.ALLOWED_HOSTS || mongo_credentials_1.DEFAULT_ALLOWED_HOSTS;
            const isServiceAuth = !!options.credentials?.mechanismProperties?.ENVIRONMENT;
            if (!isServiceAuth) {
                for (const host of options.hosts){
                    if (!(0, utils_1.hostMatchesWildcards)(host.toHostPort().host, allowedHosts)) {
                        throw new error_1.MongoInvalidArgumentError(`Host '${host}' is not valid for OIDC authentication with ALLOWED_HOSTS of '${allowedHosts.join(',')}'`);
                    }
                }
            }
        }
        this.topology = new topology_1.Topology(this, options.hosts, options);
        // Events can be emitted before initialization is complete so we have to
        // save the reference to the topology on the client ASAP if the event handlers need to access it
        this.topology.once(topology_1.Topology.OPEN, ()=>this.emit('open', this));
        for (const event of constants_1.MONGO_CLIENT_EVENTS){
            this.topology.on(event, (...args)=>this.emit(event, ...args));
        }
        const topologyConnect = async ()=>{
            try {
                await this.topology?.connect(options);
            } catch (error) {
                this.topology?.close();
                throw error;
            }
        };
        if (this.autoEncrypter) {
            await this.autoEncrypter?.init();
            await topologyConnect();
            await options.encrypter.connectInternalClient();
        } else {
            await topologyConnect();
        }
        return this;
    }
    /**
     * Cleans up resources managed by the MongoClient.
     *
     * The close method clears and closes all resources whose lifetimes are managed by the MongoClient.
     * Please refer to the `MongoClient` class documentation for a high level overview of the client's key features and responsibilities.
     *
     * **However,** the close method does not handle the cleanup of resources explicitly created by the user.
     * Any user-created driver resource with its own `close()` method should be explicitly closed by the user before calling MongoClient.close().
     * This method is written as a "best effort" attempt to leave behind the least amount of resources server-side when possible.
     *
     * The following list defines ideal preconditions and consequent pitfalls if they are not met.
     * The MongoClient, ClientSession, Cursors and ChangeStreams all support [explicit resource management](https://www.typescriptlang.org/docs/handbook/release-notes/typescript-5-2.html).
     * By using explicit resource management to manage the lifetime of driver resources instead of manually managing their lifetimes, the pitfalls outlined below can be avoided.
     *
     * The close method performs the following in the order listed:
     * - Client-side:
     *   - **Close in-use connections**: Any connections that are currently waiting on a response from the server will be closed.
     *     This is performed _first_ to avoid reaching the next step (server-side clean up) and having no available connections to check out.
     *     - _Ideal_: All operations have been awaited or cancelled, and the outcomes, regardless of success or failure, have been processed before closing the client servicing the operation.
     *     - _Pitfall_: When `client.close()` is called and all connections are in use, after closing them, the client must create new connections for cleanup operations, which comes at the cost of new TLS/TCP handshakes and authentication steps.
     * - Server-side:
     *   - **Close active cursors**: All cursors that haven't been completed will have a `killCursor` operation sent to the server they were initialized on, freeing the server-side resource.
     *     - _Ideal_: Cursors are explicitly closed or completed before `client.close()` is called.
     *     - _Pitfall_: `killCursors` may have to build a new connection if the in-use closure ended all pooled connections.
     *   - **End active sessions**: In-use sessions created with `client.startSession()` or `client.withSession()` or implicitly by the driver will have their `.endSession()` method called.
     *     Contrary to the name of the method, `endSession()` returns the session to the client's pool of sessions rather than end them on the server.
     *     - _Ideal_: Transaction outcomes are awaited and their corresponding explicit sessions are ended before `client.close()` is called.
     *     - _Pitfall_: **This step aborts in-progress transactions**. It is advisable to observe the outcome of a transaction before closing your client.
     *   - **End all pooled sessions**: The `endSessions` command with all session IDs the client has pooled is sent to the server to inform the cluster it can clean them up.
     *     - _Ideal_: No user intervention is expected.
     *     - _Pitfall_: None.
     *
     * The remaining shutdown is of the MongoClient resources that are intended to be entirely internal but is documented here as their existence relates to the JS event loop.
     *
     * - Client-side (again):
     *   - **Stop all server monitoring**: Connections kept live for detecting cluster changes and roundtrip time measurements are shutdown.
     *   - **Close all pooled connections**: Each server node in the cluster has a corresponding connection pool and all connections in the pool are closed. Any operations waiting to check out a connection will have an error thrown instead of a connection returned.
     *   - **Clear out server selection queue**: Any operations that are in the process of waiting for a server to be selected will have an error thrown instead of a server returned.
     *   - **Close encryption-related resources**: An internal MongoClient created for communicating with `mongocryptd` or other encryption purposes is closed. (Using this same method of course!)
     *
     * After the close method completes there should be no MongoClient related resources [ref-ed in Node.js' event loop](https://docs.libuv.org/en/v1.x/handle.html#reference-counting).
     * This should allow Node.js to exit gracefully if MongoClient resources were the only active handles in the event loop.
     *
     * @param _force - currently an unused flag that has no effect. Defaults to `false`.
     */ async close(_force = false) {
        if (this.closeLock) {
            return await this.closeLock;
        }
        try {
            this.closeLock = this._close();
            await this.closeLock;
        } finally{
            // release
            this.closeLock = undefined;
        }
    }
    /* @internal */ async _close() {
        // There's no way to set hasBeenClosed back to false
        Object.defineProperty(this.s, 'hasBeenClosed', {
            value: true,
            enumerable: true,
            configurable: false,
            writable: false
        });
        this.topology?.closeCheckedOutConnections();
        const activeCursorCloses = Array.from(this.s.activeCursors, (cursor)=>cursor.close());
        this.s.activeCursors.clear();
        await Promise.all(activeCursorCloses);
        const activeSessionEnds = Array.from(this.s.activeSessions, (session)=>session.endSession());
        this.s.activeSessions.clear();
        await Promise.all(activeSessionEnds);
        if (this.topology == null) {
            return;
        }
        const supportsSessions = this.topology.description.type === _1.TopologyType.LoadBalanced || this.topology.description.logicalSessionTimeoutMinutes != null;
        if (supportsSessions) {
            await endSessions(this, this.topology);
        }
        // clear out references to old topology
        const topology = this.topology;
        this.topology = undefined;
        topology.close();
        const { encrypter } = this.options;
        if (encrypter) {
            await encrypter.close(this);
        }
        async function endSessions(client, { description: topologyDescription }) {
            // If we would attempt to select a server and get nothing back we short circuit
            // to avoid the server selection timeout.
            const selector = (0, server_selection_1.readPreferenceServerSelector)(read_preference_1.ReadPreference.primaryPreferred);
            const serverDescriptions = Array.from(topologyDescription.servers.values());
            const servers = selector(topologyDescription, serverDescriptions);
            if (servers.length !== 0) {
                const endSessions = Array.from(client.s.sessionPool.sessions, ({ id })=>id);
                if (endSessions.length !== 0) {
                    try {
                        await (0, execute_operation_1.executeOperation)(client, new end_sessions_1.EndSessionsOperation(endSessions));
                    } catch (error) {
                        (0, utils_1.squashError)(error);
                    }
                }
            }
        }
    }
    /**
     * Create a new Db instance sharing the current socket connections.
     *
     * @param dbName - The name of the database we want to use. If not provided, use database name from connection string.
     * @param options - Optional settings for Db construction
     */ db(dbName, options) {
        options = options ?? {};
        // Default to db from connection string if not provided
        if (!dbName) {
            dbName = this.s.options.dbName;
        }
        // Copy the options and add out internal override of the not shared flag
        const finalOptions = Object.assign({}, this.options, options);
        // Return the db object
        const db = new db_1.Db(this, dbName, finalOptions);
        // Return the database
        return db;
    }
    /**
     * Creates a new MongoClient instance and immediately connects it to MongoDB.
     * This convenience method combines `new MongoClient(url, options)` and `client.connect()` in a single step.
     *
     * Connect can be helpful to detect configuration issues early by validating:
     * - **DNS Resolution**: Verifies that SRV records and hostnames in the connection string resolve DNS entries
     * - **Network Connectivity**: Confirms that host addresses are reachable and ports are open
     * - **TLS Configuration**: Validates SSL/TLS certificates, CA files, and encryption settings are correct
     * - **Authentication**: Verifies that provided credentials are valid
     * - **Server Compatibility**: Ensures the MongoDB server version is supported by this driver version
     * - **Load Balancer Setup**: For load-balanced deployments, confirms the service is properly configured
     *
     * @returns A promise that resolves to the same MongoClient instance once connected
     *
     * @remarks
     * **Connection is Optional:** Calling `connect` is optional since any operation method (`find`, `insertOne`, etc.)
     * will automatically perform these same validation steps if the client is not already connected.
     * However, explicitly calling `connect` can make sense for:
     * - **Fail-fast Error Detection**: Non-transient connection issues (hostname unresolved, port refused connection) are discovered immediately rather than during your first operation
     * - **Predictable Performance**: Eliminates first connection overhead from your first database operation
     *
     * @remarks
     * **Connection Pooling Impact:** Calling `connect` will populate the connection pool with one connection
     * to a server selected by the client's configured `readPreference` (defaults to primary).
     *
     * @remarks
     * **Timeout Behavior:** When using `timeoutMS`, the connection establishment time does not count against
     * the timeout for subsequent operations. This means `connect` runs without a `timeoutMS` limit, while
     * your database operations will still respect the configured timeout. If you need predictable operation
     * timing with `timeoutMS`, call `connect` explicitly before performing operations.
     *
     * @see https://www.mongodb.com/docs/manual/reference/connection-string/
     */ static async connect(url, options) {
        const client = new this(url, options);
        return await client.connect();
    }
    /**
     * Creates a new ClientSession. When using the returned session in an operation
     * a corresponding ServerSession will be created.
     *
     * @remarks
     * A ClientSession instance may only be passed to operations being performed on the same
     * MongoClient it was started from.
     */ startSession(options) {
        const session = new sessions_1.ClientSession(this, this.s.sessionPool, {
            explicit: true,
            ...options
        }, this.options);
        this.s.activeSessions.add(session);
        session.once('ended', ()=>{
            this.s.activeSessions.delete(session);
        });
        return session;
    }
    async withSession(optionsOrExecutor, executor) {
        const options = {
            // Always define an owner
            owner: Symbol(),
            // If it's an object inherit the options
            ...typeof optionsOrExecutor === 'object' ? optionsOrExecutor : {}
        };
        const withSessionCallback = typeof optionsOrExecutor === 'function' ? optionsOrExecutor : executor;
        if (withSessionCallback == null) {
            throw new error_1.MongoInvalidArgumentError('Missing required callback parameter');
        }
        const session = this.startSession(options);
        try {
            return await withSessionCallback(session);
        } finally{
            try {
                await session.endSession();
            } catch (error) {
                (0, utils_1.squashError)(error);
            }
        }
    }
    /**
     * Create a new Change Stream, watching for new changes (insertions, updates,
     * replacements, deletions, and invalidations) in this cluster. Will ignore all
     * changes to system collections, as well as the local, admin, and config databases.
     *
     * @remarks
     * watch() accepts two generic arguments for distinct use cases:
     * - The first is to provide the schema that may be defined for all the data within the current cluster
     * - The second is to override the shape of the change stream document entirely, if it is not provided the type will default to ChangeStreamDocument of the first argument
     *
     * @remarks
     * When `timeoutMS` is configured for a change stream, it will have different behaviour depending
     * on whether the change stream is in iterator mode or emitter mode. In both cases, a change
     * stream will time out if it does not receive a change event within `timeoutMS` of the last change
     * event.
     *
     * Note that if a change stream is consistently timing out when watching a collection, database or
     * client that is being changed, then this may be due to the server timing out before it can finish
     * processing the existing oplog. To address this, restart the change stream with a higher
     * `timeoutMS`.
     *
     * If the change stream times out the initial aggregate operation to establish the change stream on
     * the server, then the client will close the change stream. If the getMore calls to the server
     * time out, then the change stream will be left open, but will throw a MongoOperationTimeoutError
     * when in iterator mode and emit an error event that returns a MongoOperationTimeoutError in
     * emitter mode.
     *
     * To determine whether or not the change stream is still open following a timeout, check the
     * {@link ChangeStream.closed} getter.
     *
     * @example
     * In iterator mode, if a next() call throws a timeout error, it will attempt to resume the change stream.
     * The next call can just be retried after this succeeds.
     * ```ts
     * const changeStream = collection.watch([], { timeoutMS: 100 });
     * try {
     *     await changeStream.next();
     * } catch (e) {
     *     if (e instanceof MongoOperationTimeoutError && !changeStream.closed) {
     *       await changeStream.next();
     *     }
     *     throw e;
     * }
     * ```
     *
     * @example
     * In emitter mode, if the change stream goes `timeoutMS` without emitting a change event, it will
     * emit an error event that returns a MongoOperationTimeoutError, but will not close the change
     * stream unless the resume attempt fails. There is no need to re-establish change listeners as
     * this will automatically continue emitting change events once the resume attempt completes.
     *
     * ```ts
     * const changeStream = collection.watch([], { timeoutMS: 100 });
     * changeStream.on('change', console.log);
     * changeStream.on('error', e => {
     *     if (e instanceof MongoOperationTimeoutError && !changeStream.closed) {
     *         // do nothing
     *     } else {
     *         changeStream.close();
     *     }
     * });
     * ```
     * @param pipeline - An array of {@link https://www.mongodb.com/docs/manual/reference/operator/aggregation-pipeline/|aggregation pipeline stages} through which to pass change stream documents. This allows for filtering (using $match) and manipulating the change stream documents.
     * @param options - Optional settings for the command
     * @typeParam TSchema - Type of the data being detected by the change stream
     * @typeParam TChange - Type of the whole change stream document emitted
     */ watch(pipeline = [], options = {}) {
        // Allow optionally not specifying a pipeline
        if (!Array.isArray(pipeline)) {
            options = pipeline;
            pipeline = [];
        }
        return new change_stream_1.ChangeStream(this, pipeline, (0, utils_1.resolveOptions)(this, options));
    }
}
exports.MongoClient = MongoClient; //# sourceMappingURL=mongo_client.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/change_stream.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ChangeStream = void 0;
exports.filterOutOptions = filterOutOptions;
const collection_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/collection.js [app-client] (ecmascript)");
const constants_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/constants.js [app-client] (ecmascript)");
const abstract_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/abstract_cursor.js [app-client] (ecmascript)");
const change_stream_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/change_stream_cursor.js [app-client] (ecmascript)");
const db_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/db.js [app-client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
const mongo_client_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/mongo_client.js [app-client] (ecmascript)");
const mongo_types_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/mongo_types.js [app-client] (ecmascript)");
const timeout_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/timeout.js [app-client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/utils.js [app-client] (ecmascript)");
const CHANGE_DOMAIN_TYPES = {
    COLLECTION: Symbol('Collection'),
    DATABASE: Symbol('Database'),
    CLUSTER: Symbol('Cluster')
};
const CHANGE_STREAM_EVENTS = [
    constants_1.RESUME_TOKEN_CHANGED,
    constants_1.END,
    constants_1.CLOSE
];
const NO_RESUME_TOKEN_ERROR = 'A change stream document has been received that lacks a resume token (_id).';
const CHANGESTREAM_CLOSED_ERROR = 'ChangeStream is closed';
const INVALID_STAGE_OPTIONS = buildDisallowedChangeStreamOptions();
function filterOutOptions(options) {
    return Object.fromEntries(Object.entries(options).filter(([k, _])=>!INVALID_STAGE_OPTIONS.has(k)));
}
/**
 * Creates a new Change Stream instance. Normally created using {@link Collection#watch|Collection.watch()}.
 * @public
 */ class ChangeStream extends mongo_types_1.TypedEventEmitter {
    /**
     * @experimental
     * An alias for {@link ChangeStream.close|ChangeStream.close()}.
     */ async [Symbol.asyncDispose]() {
        await this.close();
    }
    /** @event */ static{
        this.RESPONSE = constants_1.RESPONSE;
    }
    /** @event */ static{
        this.MORE = constants_1.MORE;
    }
    /** @event */ static{
        this.INIT = constants_1.INIT;
    }
    /** @event */ static{
        this.CLOSE = constants_1.CLOSE;
    }
    /**
     * Fired for each new matching change in the specified namespace. Attaching a `change`
     * event listener to a Change Stream will switch the stream into flowing mode. Data will
     * then be passed as soon as it is available.
     * @event
     */ static{
        this.CHANGE = constants_1.CHANGE;
    }
    /** @event */ static{
        this.END = constants_1.END;
    }
    /** @event */ static{
        this.ERROR = constants_1.ERROR;
    }
    /**
     * Emitted each time the change stream stores a new resume token.
     * @event
     */ static{
        this.RESUME_TOKEN_CHANGED = constants_1.RESUME_TOKEN_CHANGED;
    }
    /**
     * @internal
     *
     * @param parent - The parent object that created this change stream
     * @param pipeline - An array of {@link https://www.mongodb.com/docs/manual/reference/operator/aggregation-pipeline/|aggregation pipeline stages} through which to pass change stream documents
     */ constructor(parent, pipeline = [], options = {}){
        super();
        this.pipeline = pipeline;
        this.options = {
            ...options
        };
        let serverSelectionTimeoutMS;
        delete this.options.writeConcern;
        if (parent instanceof collection_1.Collection) {
            this.type = CHANGE_DOMAIN_TYPES.COLLECTION;
            serverSelectionTimeoutMS = parent.s.db.client.options.serverSelectionTimeoutMS;
        } else if (parent instanceof db_1.Db) {
            this.type = CHANGE_DOMAIN_TYPES.DATABASE;
            serverSelectionTimeoutMS = parent.client.options.serverSelectionTimeoutMS;
        } else if (parent instanceof mongo_client_1.MongoClient) {
            this.type = CHANGE_DOMAIN_TYPES.CLUSTER;
            serverSelectionTimeoutMS = parent.options.serverSelectionTimeoutMS;
        } else {
            throw new error_1.MongoChangeStreamError('Parent provided to ChangeStream constructor must be an instance of Collection, Db, or MongoClient');
        }
        this.contextOwner = Symbol();
        this.parent = parent;
        this.namespace = parent.s.namespace;
        if (!this.options.readPreference && parent.readPreference) {
            this.options.readPreference = parent.readPreference;
        }
        // Create contained Change Stream cursor
        this.cursor = this._createChangeStreamCursor(options);
        this.isClosed = false;
        this.mode = false;
        // Listen for any `change` listeners being added to ChangeStream
        this.on('newListener', (eventName)=>{
            if (eventName === 'change' && this.cursor && this.listenerCount('change') === 0) {
                this._streamEvents(this.cursor);
            }
        });
        this.on('removeListener', (eventName)=>{
            if (eventName === 'change' && this.listenerCount('change') === 0 && this.cursor) {
                this.cursorStream?.removeAllListeners('data');
            }
        });
        if (this.options.timeoutMS != null) {
            this.timeoutContext = new timeout_1.CSOTTimeoutContext({
                timeoutMS: this.options.timeoutMS,
                serverSelectionTimeoutMS
            });
        }
    }
    /** The cached resume token that is used to resume after the most recently returned change. */ get resumeToken() {
        return this.cursor?.resumeToken;
    }
    /** Check if there is any document still available in the Change Stream */ async hasNext() {
        this._setIsIterator();
        // Change streams must resume indefinitely while each resume event succeeds.
        // This loop continues until either a change event is received or until a resume attempt
        // fails.
        this.timeoutContext?.refresh();
        try {
            while(true){
                try {
                    const hasNext = await this.cursor.hasNext();
                    return hasNext;
                } catch (error) {
                    try {
                        await this._processErrorIteratorMode(error, this.cursor.id != null);
                    } catch (error) {
                        if (error instanceof error_1.MongoOperationTimeoutError && this.cursor.id == null) {
                            throw error;
                        }
                        try {
                            await this.close();
                        } catch (error) {
                            (0, utils_1.squashError)(error);
                        }
                        throw error;
                    }
                }
            }
        } finally{
            this.timeoutContext?.clear();
        }
    }
    /** Get the next available document from the Change Stream. */ async next() {
        this._setIsIterator();
        // Change streams must resume indefinitely while each resume event succeeds.
        // This loop continues until either a change event is received or until a resume attempt
        // fails.
        this.timeoutContext?.refresh();
        try {
            while(true){
                try {
                    const change = await this.cursor.next();
                    const processedChange = this._processChange(change ?? null);
                    return processedChange;
                } catch (error) {
                    try {
                        await this._processErrorIteratorMode(error, this.cursor.id != null);
                    } catch (error) {
                        if (error instanceof error_1.MongoOperationTimeoutError && this.cursor.id == null) {
                            throw error;
                        }
                        try {
                            await this.close();
                        } catch (error) {
                            (0, utils_1.squashError)(error);
                        }
                        throw error;
                    }
                }
            }
        } finally{
            this.timeoutContext?.clear();
        }
    }
    /**
     * Try to get the next available document from the Change Stream's cursor or `null` if an empty batch is returned
     */ async tryNext() {
        this._setIsIterator();
        // Change streams must resume indefinitely while each resume event succeeds.
        // This loop continues until either a change event is received or until a resume attempt
        // fails.
        this.timeoutContext?.refresh();
        try {
            while(true){
                try {
                    const change = await this.cursor.tryNext();
                    if (!change) {
                        return null;
                    }
                    const processedChange = this._processChange(change);
                    return processedChange;
                } catch (error) {
                    try {
                        await this._processErrorIteratorMode(error, this.cursor.id != null);
                    } catch (error) {
                        if (error instanceof error_1.MongoOperationTimeoutError && this.cursor.id == null) throw error;
                        try {
                            await this.close();
                        } catch (error) {
                            (0, utils_1.squashError)(error);
                        }
                        throw error;
                    }
                }
            }
        } finally{
            this.timeoutContext?.clear();
        }
    }
    async *[Symbol.asyncIterator]() {
        if (this.closed) {
            return;
        }
        try {
            // Change streams run indefinitely as long as errors are resumable
            // So the only loop breaking condition is if `next()` throws
            while(true){
                yield await this.next();
            }
        } finally{
            try {
                await this.close();
            } catch (error) {
                (0, utils_1.squashError)(error);
            }
        }
    }
    /** Is the cursor closed */ get closed() {
        return this.isClosed || this.cursor.closed;
    }
    /**
     * Frees the internal resources used by the change stream.
     */ async close() {
        this.timeoutContext?.clear();
        this.timeoutContext = undefined;
        this.isClosed = true;
        const cursor = this.cursor;
        try {
            await cursor.close();
        } finally{
            this._endStream();
        }
    }
    /**
     * Return a modified Readable stream including a possible transform method.
     *
     * NOTE: When using a Stream to process change stream events, the stream will
     * NOT automatically resume in the case a resumable error is encountered.
     *
     * @throws MongoChangeStreamError if the underlying cursor or the change stream is closed
     */ stream() {
        if (this.closed) {
            throw new error_1.MongoChangeStreamError(CHANGESTREAM_CLOSED_ERROR);
        }
        return this.cursor.stream();
    }
    /** @internal */ _setIsEmitter() {
        if (this.mode === 'iterator') {
            // TODO(NODE-3485): Replace with MongoChangeStreamModeError
            throw new error_1.MongoAPIError('ChangeStream cannot be used as an EventEmitter after being used as an iterator');
        }
        this.mode = 'emitter';
    }
    /** @internal */ _setIsIterator() {
        if (this.mode === 'emitter') {
            // TODO(NODE-3485): Replace with MongoChangeStreamModeError
            throw new error_1.MongoAPIError('ChangeStream cannot be used as an iterator after being used as an EventEmitter');
        }
        this.mode = 'iterator';
    }
    /**
     * Create a new change stream cursor based on self's configuration
     * @internal
     */ _createChangeStreamCursor(options) {
        const changeStreamStageOptions = filterOutOptions(options);
        if (this.type === CHANGE_DOMAIN_TYPES.CLUSTER) {
            changeStreamStageOptions.allChangesForCluster = true;
        }
        const pipeline = [
            {
                $changeStream: changeStreamStageOptions
            },
            ...this.pipeline
        ];
        const client = this.type === CHANGE_DOMAIN_TYPES.CLUSTER ? this.parent : this.type === CHANGE_DOMAIN_TYPES.DATABASE ? this.parent.client : this.type === CHANGE_DOMAIN_TYPES.COLLECTION ? this.parent.client : null;
        if (client == null) {
            // This should never happen because of the assertion in the constructor
            throw new error_1.MongoRuntimeError(`Changestream type should only be one of cluster, database, collection. Found ${this.type.toString()}`);
        }
        const changeStreamCursor = new change_stream_cursor_1.ChangeStreamCursor(client, this.namespace, pipeline, {
            ...options,
            timeoutContext: this.timeoutContext ? new abstract_cursor_1.CursorTimeoutContext(this.timeoutContext, this.contextOwner) : undefined
        });
        for (const event of CHANGE_STREAM_EVENTS){
            changeStreamCursor.on(event, (e)=>this.emit(event, e));
        }
        if (this.listenerCount(ChangeStream.CHANGE) > 0) {
            this._streamEvents(changeStreamCursor);
        }
        return changeStreamCursor;
    }
    /** @internal */ _closeEmitterModeWithError(error) {
        this.emit(ChangeStream.ERROR, error);
        this.close().then(undefined, utils_1.squashError);
    }
    /** @internal */ _streamEvents(cursor) {
        this._setIsEmitter();
        const stream = this.cursorStream ?? cursor.stream();
        this.cursorStream = stream;
        stream.on('data', (change)=>{
            try {
                const processedChange = this._processChange(change);
                this.emit(ChangeStream.CHANGE, processedChange);
            } catch (error) {
                this.emit(ChangeStream.ERROR, error);
            }
            this.timeoutContext?.refresh();
        });
        stream.on('error', (error)=>this._processErrorStreamMode(error, this.cursor.id != null));
    }
    /** @internal */ _endStream() {
        this.cursorStream?.removeAllListeners('data');
        this.cursorStream?.removeAllListeners('close');
        this.cursorStream?.removeAllListeners('end');
        this.cursorStream?.destroy();
        this.cursorStream = undefined;
    }
    /** @internal */ _processChange(change) {
        if (this.isClosed) {
            // TODO(NODE-3485): Replace with MongoChangeStreamClosedError
            throw new error_1.MongoAPIError(CHANGESTREAM_CLOSED_ERROR);
        }
        // a null change means the cursor has been notified, implicitly closing the change stream
        if (change == null) {
            // TODO(NODE-3485): Replace with MongoChangeStreamClosedError
            throw new error_1.MongoRuntimeError(CHANGESTREAM_CLOSED_ERROR);
        }
        if (change && !change._id) {
            throw new error_1.MongoChangeStreamError(NO_RESUME_TOKEN_ERROR);
        }
        // cache the resume token
        this.cursor.cacheResumeToken(change._id);
        // wipe the startAtOperationTime if there was one so that there won't be a conflict
        // between resumeToken and startAtOperationTime if we need to reconnect the cursor
        this.options.startAtOperationTime = undefined;
        return change;
    }
    /** @internal */ _processErrorStreamMode(changeStreamError, cursorInitialized) {
        // If the change stream has been closed explicitly, do not process error.
        if (this.isClosed) return;
        if (cursorInitialized && ((0, error_1.isResumableError)(changeStreamError, this.cursor.maxWireVersion) || changeStreamError instanceof error_1.MongoOperationTimeoutError)) {
            this._endStream();
            this.cursor.close().then(()=>this._resume(changeStreamError), (e)=>{
                (0, utils_1.squashError)(e);
                return this._resume(changeStreamError);
            }).then(()=>{
                if (changeStreamError instanceof error_1.MongoOperationTimeoutError) this.emit(ChangeStream.ERROR, changeStreamError);
            }, ()=>this._closeEmitterModeWithError(changeStreamError));
        } else {
            this._closeEmitterModeWithError(changeStreamError);
        }
    }
    /** @internal */ async _processErrorIteratorMode(changeStreamError, cursorInitialized) {
        if (this.isClosed) {
            // TODO(NODE-3485): Replace with MongoChangeStreamClosedError
            throw new error_1.MongoAPIError(CHANGESTREAM_CLOSED_ERROR);
        }
        if (cursorInitialized && ((0, error_1.isResumableError)(changeStreamError, this.cursor.maxWireVersion) || changeStreamError instanceof error_1.MongoOperationTimeoutError)) {
            try {
                await this.cursor.close();
            } catch (error) {
                (0, utils_1.squashError)(error);
            }
            await this._resume(changeStreamError);
            if (changeStreamError instanceof error_1.MongoOperationTimeoutError) throw changeStreamError;
        } else {
            try {
                await this.close();
            } catch (error) {
                (0, utils_1.squashError)(error);
            }
            throw changeStreamError;
        }
    }
    async _resume(changeStreamError) {
        this.timeoutContext?.refresh();
        const topology = (0, utils_1.getTopology)(this.parent);
        try {
            await topology.selectServer(this.cursor.readPreference, {
                operationName: 'reconnect topology in change stream',
                timeoutContext: this.timeoutContext
            });
            this.cursor = this._createChangeStreamCursor(this.cursor.resumeOptions);
        } catch  {
            // if the topology can't reconnect, close the stream
            await this.close();
            throw changeStreamError;
        }
    }
}
exports.ChangeStream = ChangeStream;
/**
 * This function returns a list of options that are *not* supported by the $changeStream
 * aggregation stage.  This is best-effort - it uses the options "officially supported" by the driver
 * to derive a list of known, unsupported options for the $changeStream stage.
 *
 * Notably, at runtime, users can still provide options unknown to the driver and the driver will
 * *not* filter them out of the options object (see NODE-5510).
 */ function buildDisallowedChangeStreamOptions() {
    const denyList = {
        allowDiskUse: '',
        authdb: '',
        batchSize: '',
        bsonRegExp: '',
        bypassDocumentValidation: '',
        bypassPinningCheck: '',
        checkKeys: '',
        collation: '',
        comment: '',
        cursor: '',
        dbName: '',
        enableUtf8Validation: '',
        explain: '',
        fieldsAsRaw: '',
        hint: '',
        ignoreUndefined: '',
        let: '',
        maxAwaitTimeMS: '',
        maxTimeMS: '',
        omitMaxTimeMS: '',
        out: '',
        promoteBuffers: '',
        promoteLongs: '',
        promoteValues: '',
        raw: '',
        rawData: '',
        readConcern: '',
        readPreference: '',
        serializeFunctions: '',
        session: '',
        timeoutContext: '',
        timeoutMS: '',
        timeoutMode: '',
        useBigInt64: '',
        willRetryWrite: '',
        writeConcern: ''
    };
    return new Set(Object.keys(denyList));
} //# sourceMappingURL=change_stream.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/gridfs/download.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/Pathology/patho-client/node_modules/.pnpm/next@16.0.6_@babel+core@7.28.5_react-dom@19.2.0_react@19.2.0__react@19.2.0/node_modules/next/dist/compiled/buffer/index.js [app-client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.GridFSBucketReadStream = void 0;
const stream_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/next@16.0.6_@babel+core@7.28.5_react-dom@19.2.0_react@19.2.0__react@19.2.0/node_modules/next/dist/compiled/stream-browserify/index.js [app-client] (ecmascript)");
const abstract_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/abstract_cursor.js [app-client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
const timeout_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/timeout.js [app-client] (ecmascript)");
/**
 * A readable stream that enables you to read buffers from GridFS.
 *
 * Do not instantiate this class directly. Use `openDownloadStream()` instead.
 * @public
 */ class GridFSBucketReadStream extends stream_1.Readable {
    /**
     * Fires when the stream loaded the file document corresponding to the provided id.
     * @event
     */ static{
        this.FILE = 'file';
    }
    /**
     * @param chunks - Handle for chunks collection
     * @param files - Handle for files collection
     * @param readPreference - The read preference to use
     * @param filter - The filter to use to find the file document
     * @internal
     */ constructor(chunks, files, readPreference, filter, options){
        super({
            emitClose: true
        });
        this.s = {
            bytesToTrim: 0,
            bytesToSkip: 0,
            bytesRead: 0,
            chunks,
            expected: 0,
            files,
            filter,
            init: false,
            expectedEnd: 0,
            options: {
                start: 0,
                end: 0,
                ...options
            },
            readPreference,
            timeoutContext: options?.timeoutMS != null ? new timeout_1.CSOTTimeoutContext({
                timeoutMS: options.timeoutMS,
                serverSelectionTimeoutMS: 0
            }) : undefined
        };
    }
    /**
     * Reads from the cursor and pushes to the stream.
     * Private Impl, do not call directly
     * @internal
     */ _read() {
        if (this.destroyed) return;
        waitForFile(this, ()=>doRead(this));
    }
    /**
     * Sets the 0-based offset in bytes to start streaming from. Throws
     * an error if this stream has entered flowing mode
     * (e.g. if you've already called `on('data')`)
     *
     * @param start - 0-based offset in bytes to start streaming from
     */ start(start = 0) {
        throwIfInitialized(this);
        this.s.options.start = start;
        return this;
    }
    /**
     * Sets the 0-based offset in bytes to start streaming from. Throws
     * an error if this stream has entered flowing mode
     * (e.g. if you've already called `on('data')`)
     *
     * @param end - Offset in bytes to stop reading at
     */ end(end = 0) {
        throwIfInitialized(this);
        this.s.options.end = end;
        return this;
    }
    /**
     * Marks this stream as aborted (will never push another `data` event)
     * and kills the underlying cursor. Will emit the 'end' event, and then
     * the 'close' event once the cursor is successfully killed.
     */ async abort() {
        this.push(null);
        this.destroy();
        const remainingTimeMS = this.s.timeoutContext?.getRemainingTimeMSOrThrow();
        await this.s.cursor?.close({
            timeoutMS: remainingTimeMS
        });
    }
}
exports.GridFSBucketReadStream = GridFSBucketReadStream;
function throwIfInitialized(stream) {
    if (stream.s.init) {
        throw new error_1.MongoGridFSStreamError('Options cannot be changed after the stream is initialized');
    }
}
function doRead(stream) {
    if (stream.destroyed) return;
    if (!stream.s.cursor) return;
    if (!stream.s.file) return;
    const handleReadResult = (doc)=>{
        if (stream.destroyed) return;
        if (!doc) {
            stream.push(null);
            stream.s.cursor?.close().then(undefined, (error)=>stream.destroy(error));
            return;
        }
        if (!stream.s.file) return;
        const bytesRemaining = stream.s.file.length - stream.s.bytesRead;
        const expectedN = stream.s.expected++;
        const expectedLength = Math.min(stream.s.file.chunkSize, bytesRemaining);
        if (doc.n > expectedN) {
            return stream.destroy(new error_1.MongoGridFSChunkError(`ChunkIsMissing: Got unexpected n: ${doc.n}, expected: ${expectedN}`));
        }
        if (doc.n < expectedN) {
            return stream.destroy(new error_1.MongoGridFSChunkError(`ExtraChunk: Got unexpected n: ${doc.n}, expected: ${expectedN}`));
        }
        let buf = __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["Buffer"].isBuffer(doc.data) ? doc.data : doc.data.buffer;
        if (buf.byteLength !== expectedLength) {
            if (bytesRemaining <= 0) {
                return stream.destroy(new error_1.MongoGridFSChunkError(`ExtraChunk: Got unexpected n: ${doc.n}, expected file length ${stream.s.file.length} bytes but already read ${stream.s.bytesRead} bytes`));
            }
            return stream.destroy(new error_1.MongoGridFSChunkError(`ChunkIsWrongSize: Got unexpected length: ${buf.byteLength}, expected: ${expectedLength}`));
        }
        stream.s.bytesRead += buf.byteLength;
        if (buf.byteLength === 0) {
            return stream.push(null);
        }
        let sliceStart = null;
        let sliceEnd = null;
        if (stream.s.bytesToSkip != null) {
            sliceStart = stream.s.bytesToSkip;
            stream.s.bytesToSkip = 0;
        }
        const atEndOfStream = expectedN === stream.s.expectedEnd - 1;
        const bytesLeftToRead = stream.s.options.end - stream.s.bytesToSkip;
        if (atEndOfStream && stream.s.bytesToTrim != null) {
            sliceEnd = stream.s.file.chunkSize - stream.s.bytesToTrim;
        } else if (stream.s.options.end && bytesLeftToRead < doc.data.byteLength) {
            sliceEnd = bytesLeftToRead;
        }
        if (sliceStart != null || sliceEnd != null) {
            buf = buf.slice(sliceStart || 0, sliceEnd || buf.byteLength);
        }
        stream.push(buf);
        return;
    };
    stream.s.cursor.next().then(handleReadResult, (error)=>{
        if (stream.destroyed) return;
        stream.destroy(error);
    });
}
function init(stream) {
    const findOneOptions = {};
    if (stream.s.readPreference) {
        findOneOptions.readPreference = stream.s.readPreference;
    }
    if (stream.s.options && stream.s.options.sort) {
        findOneOptions.sort = stream.s.options.sort;
    }
    if (stream.s.options && stream.s.options.skip) {
        findOneOptions.skip = stream.s.options.skip;
    }
    const handleReadResult = (doc)=>{
        if (stream.destroyed) return;
        if (!doc) {
            const identifier = stream.s.filter._id ? stream.s.filter._id.toString() : stream.s.filter.filename;
            const errmsg = `FileNotFound: file ${identifier} was not found`;
            // TODO(NODE-3483)
            const err = new error_1.MongoRuntimeError(errmsg);
            err.code = 'ENOENT'; // TODO: NODE-3338 set property as part of constructor
            return stream.destroy(err);
        }
        // If document is empty, kill the stream immediately and don't
        // execute any reads
        if (doc.length <= 0) {
            stream.push(null);
            return;
        }
        if (stream.destroyed) {
            // If user destroys the stream before we have a cursor, wait
            // until the query is done to say we're 'closed' because we can't
            // cancel a query.
            stream.destroy();
            return;
        }
        try {
            stream.s.bytesToSkip = handleStartOption(stream, doc, stream.s.options);
        } catch (error) {
            return stream.destroy(error);
        }
        const filter = {
            files_id: doc._id
        };
        // Currently (MongoDB 3.4.4) skip function does not support the index,
        // it needs to retrieve all the documents first and then skip them. (CS-25811)
        // As work around we use $gte on the "n" field.
        if (stream.s.options && stream.s.options.start != null) {
            const skip = Math.floor(stream.s.options.start / doc.chunkSize);
            if (skip > 0) {
                filter['n'] = {
                    $gte: skip
                };
            }
        }
        let remainingTimeMS;
        try {
            remainingTimeMS = stream.s.timeoutContext?.getRemainingTimeMSOrThrow(`Download timed out after ${stream.s.timeoutContext?.timeoutMS}ms`);
        } catch (error) {
            return stream.destroy(error);
        }
        stream.s.cursor = stream.s.chunks.find(filter, {
            timeoutMode: stream.s.options.timeoutMS != null ? abstract_cursor_1.CursorTimeoutMode.LIFETIME : undefined,
            timeoutMS: remainingTimeMS
        }).sort({
            n: 1
        });
        if (stream.s.readPreference) {
            stream.s.cursor.withReadPreference(stream.s.readPreference);
        }
        stream.s.expectedEnd = Math.ceil(doc.length / doc.chunkSize);
        stream.s.file = doc;
        try {
            stream.s.bytesToTrim = handleEndOption(stream, doc, stream.s.cursor, stream.s.options);
        } catch (error) {
            return stream.destroy(error);
        }
        stream.emit(GridFSBucketReadStream.FILE, doc);
        return;
    };
    let remainingTimeMS;
    try {
        remainingTimeMS = stream.s.timeoutContext?.getRemainingTimeMSOrThrow(`Download timed out after ${stream.s.timeoutContext?.timeoutMS}ms`);
    } catch (error) {
        if (!stream.destroyed) stream.destroy(error);
        return;
    }
    findOneOptions.timeoutMS = remainingTimeMS;
    stream.s.files.findOne(stream.s.filter, findOneOptions).then(handleReadResult, (error)=>{
        if (stream.destroyed) return;
        stream.destroy(error);
    });
}
function waitForFile(stream, callback) {
    if (stream.s.file) {
        return callback();
    }
    if (!stream.s.init) {
        init(stream);
        stream.s.init = true;
    }
    stream.once('file', ()=>{
        callback();
    });
}
function handleStartOption(stream, doc, options) {
    if (options && options.start != null) {
        if (options.start > doc.length) {
            throw new error_1.MongoInvalidArgumentError(`Stream start (${options.start}) must not be more than the length of the file (${doc.length})`);
        }
        if (options.start < 0) {
            throw new error_1.MongoInvalidArgumentError(`Stream start (${options.start}) must not be negative`);
        }
        if (options.end != null && options.end < options.start) {
            throw new error_1.MongoInvalidArgumentError(`Stream start (${options.start}) must not be greater than stream end (${options.end})`);
        }
        stream.s.bytesRead = Math.floor(options.start / doc.chunkSize) * doc.chunkSize;
        stream.s.expected = Math.floor(options.start / doc.chunkSize);
        return options.start - stream.s.bytesRead;
    }
    throw new error_1.MongoInvalidArgumentError('Start option must be defined');
}
function handleEndOption(stream, doc, cursor, options) {
    if (options && options.end != null) {
        if (options.end > doc.length) {
            throw new error_1.MongoInvalidArgumentError(`Stream end (${options.end}) must not be more than the length of the file (${doc.length})`);
        }
        if (options.start == null || options.start < 0) {
            throw new error_1.MongoInvalidArgumentError(`Stream end (${options.end}) must not be negative`);
        }
        const start = options.start != null ? Math.floor(options.start / doc.chunkSize) : 0;
        cursor.limit(Math.ceil(options.end / doc.chunkSize) - start);
        stream.s.expectedEnd = Math.ceil(options.end / doc.chunkSize);
        return Math.ceil(options.end / doc.chunkSize) * doc.chunkSize - options.end;
    }
    throw new error_1.MongoInvalidArgumentError('End option must be defined');
} //# sourceMappingURL=download.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/gridfs/upload.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/Pathology/patho-client/node_modules/.pnpm/next@16.0.6_@babel+core@7.28.5_react-dom@19.2.0_react@19.2.0__react@19.2.0/node_modules/next/dist/compiled/buffer/index.js [app-client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/Pathology/patho-client/node_modules/.pnpm/next@16.0.6_@babel+core@7.28.5_react-dom@19.2.0_react@19.2.0__react@19.2.0/node_modules/next/dist/build/polyfills/process.js [app-client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.GridFSBucketWriteStream = void 0;
const stream_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/next@16.0.6_@babel+core@7.28.5_react-dom@19.2.0_react@19.2.0__react@19.2.0/node_modules/next/dist/compiled/stream-browserify/index.js [app-client] (ecmascript)");
const bson_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bson.js [app-client] (ecmascript)");
const abstract_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/abstract_cursor.js [app-client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
const timeout_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/timeout.js [app-client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/utils.js [app-client] (ecmascript)");
const write_concern_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/write_concern.js [app-client] (ecmascript)");
/**
 * A writable stream that enables you to write buffers to GridFS.
 *
 * Do not instantiate this class directly. Use `openUploadStream()` instead.
 * @public
 */ class GridFSBucketWriteStream extends stream_1.Writable {
    /**
     * @param bucket - Handle for this stream's corresponding bucket
     * @param filename - The value of the 'filename' key in the files doc
     * @param options - Optional settings.
     * @internal
     */ constructor(bucket, filename, options){
        super();
        /**
         * The document containing information about the inserted file.
         * This property is defined _after_ the finish event has been emitted.
         * It will remain `null` if an error occurs.
         *
         * @example
         * ```ts
         * fs.createReadStream('file.txt')
         *   .pipe(bucket.openUploadStream('file.txt'))
         *   .on('finish', function () {
         *     console.log(this.gridFSFile)
         *   })
         * ```
         */ this.gridFSFile = null;
        options = options ?? {};
        this.bucket = bucket;
        this.chunks = bucket.s._chunksCollection;
        this.filename = filename;
        this.files = bucket.s._filesCollection;
        this.options = options;
        this.writeConcern = write_concern_1.WriteConcern.fromOptions(options) || bucket.s.options.writeConcern;
        // Signals the write is all done
        this.done = false;
        this.id = options.id ? options.id : new bson_1.ObjectId();
        // properly inherit the default chunksize from parent
        this.chunkSizeBytes = options.chunkSizeBytes || this.bucket.s.options.chunkSizeBytes;
        this.bufToStore = __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["Buffer"].alloc(this.chunkSizeBytes);
        this.length = 0;
        this.n = 0;
        this.pos = 0;
        this.state = {
            streamEnd: false,
            outstandingRequests: 0,
            errored: false,
            aborted: false
        };
        if (options.timeoutMS != null) this.timeoutContext = new timeout_1.CSOTTimeoutContext({
            timeoutMS: options.timeoutMS,
            serverSelectionTimeoutMS: (0, utils_1.resolveTimeoutOptions)(this.bucket.s.db.client, {}).serverSelectionTimeoutMS
        });
    }
    /**
     * @internal
     *
     * The stream is considered constructed when the indexes are done being created
     */ _construct(callback) {
        if (!this.bucket.s.calledOpenUploadStream) {
            this.bucket.s.calledOpenUploadStream = true;
            checkIndexes(this).then(()=>{
                this.bucket.s.checkedIndexes = true;
                this.bucket.emit('index');
                callback();
            }, (error)=>{
                if (error instanceof error_1.MongoOperationTimeoutError) {
                    return handleError(this, error, callback);
                }
                (0, utils_1.squashError)(error);
                callback();
            });
        } else {
            return __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].nextTick(callback);
        }
    }
    /**
     * @internal
     * Write a buffer to the stream.
     *
     * @param chunk - Buffer to write
     * @param encoding - Optional encoding for the buffer
     * @param callback - Function to call when the chunk was added to the buffer, or if the entire chunk was persisted to MongoDB if this chunk caused a flush.
     */ _write(chunk, encoding, callback) {
        doWrite(this, chunk, encoding, callback);
    }
    /** @internal */ _final(callback) {
        if (this.state.streamEnd) {
            return __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].nextTick(callback);
        }
        this.state.streamEnd = true;
        writeRemnant(this, callback);
    }
    /**
     * Places this write stream into an aborted state (all future writes fail)
     * and deletes all chunks that have already been written.
     */ async abort() {
        if (this.state.streamEnd) {
            // TODO(NODE-3485): Replace with MongoGridFSStreamClosed
            throw new error_1.MongoAPIError('Cannot abort a stream that has already completed');
        }
        if (this.state.aborted) {
            // TODO(NODE-3485): Replace with MongoGridFSStreamClosed
            throw new error_1.MongoAPIError('Cannot call abort() on a stream twice');
        }
        this.state.aborted = true;
        const remainingTimeMS = this.timeoutContext?.getRemainingTimeMSOrThrow(`Upload timed out after ${this.timeoutContext?.timeoutMS}ms`);
        await this.chunks.deleteMany({
            files_id: this.id
        }, {
            timeoutMS: remainingTimeMS
        });
    }
}
exports.GridFSBucketWriteStream = GridFSBucketWriteStream;
function handleError(stream, error, callback) {
    if (stream.state.errored) {
        __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].nextTick(callback);
        return;
    }
    stream.state.errored = true;
    __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].nextTick(callback, error);
}
function createChunkDoc(filesId, n, data) {
    return {
        _id: new bson_1.ObjectId(),
        files_id: filesId,
        n,
        data
    };
}
async function checkChunksIndex(stream) {
    const index = {
        files_id: 1,
        n: 1
    };
    let remainingTimeMS;
    remainingTimeMS = stream.timeoutContext?.getRemainingTimeMSOrThrow(`Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`);
    let indexes;
    try {
        indexes = await stream.chunks.listIndexes({
            timeoutMode: remainingTimeMS != null ? abstract_cursor_1.CursorTimeoutMode.LIFETIME : undefined,
            timeoutMS: remainingTimeMS
        }).toArray();
    } catch (error) {
        if (error instanceof error_1.MongoError && error.code === error_1.MONGODB_ERROR_CODES.NamespaceNotFound) {
            indexes = [];
        } else {
            throw error;
        }
    }
    const hasChunksIndex = !!indexes.find((index)=>{
        const keys = Object.keys(index.key);
        if (keys.length === 2 && index.key.files_id === 1 && index.key.n === 1) {
            return true;
        }
        return false;
    });
    if (!hasChunksIndex) {
        remainingTimeMS = stream.timeoutContext?.getRemainingTimeMSOrThrow(`Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`);
        await stream.chunks.createIndex(index, {
            ...stream.writeConcern,
            background: true,
            unique: true,
            timeoutMS: remainingTimeMS
        });
    }
}
function checkDone(stream, callback) {
    if (stream.done) {
        return __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].nextTick(callback);
    }
    if (stream.state.streamEnd && stream.state.outstandingRequests === 0 && !stream.state.errored) {
        // Set done so we do not trigger duplicate createFilesDoc
        stream.done = true;
        // Create a new files doc
        const gridFSFile = createFilesDoc(stream.id, stream.length, stream.chunkSizeBytes, stream.filename, stream.options.metadata);
        if (isAborted(stream, callback)) {
            return;
        }
        const remainingTimeMS = stream.timeoutContext?.remainingTimeMS;
        if (remainingTimeMS != null && remainingTimeMS <= 0) {
            return handleError(stream, new error_1.MongoOperationTimeoutError(`Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`), callback);
        }
        stream.files.insertOne(gridFSFile, {
            writeConcern: stream.writeConcern,
            timeoutMS: remainingTimeMS
        }).then(()=>{
            stream.gridFSFile = gridFSFile;
            callback();
        }, (error)=>{
            return handleError(stream, error, callback);
        });
        return;
    }
    __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].nextTick(callback);
}
async function checkIndexes(stream) {
    let remainingTimeMS = stream.timeoutContext?.getRemainingTimeMSOrThrow(`Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`);
    const doc = await stream.files.findOne({}, {
        projection: {
            _id: 1
        },
        timeoutMS: remainingTimeMS
    });
    if (doc != null) {
        // If at least one document exists assume the collection has the required index
        return;
    }
    const index = {
        filename: 1,
        uploadDate: 1
    };
    let indexes;
    remainingTimeMS = stream.timeoutContext?.getRemainingTimeMSOrThrow(`Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`);
    const listIndexesOptions = {
        timeoutMode: remainingTimeMS != null ? abstract_cursor_1.CursorTimeoutMode.LIFETIME : undefined,
        timeoutMS: remainingTimeMS
    };
    try {
        indexes = await stream.files.listIndexes(listIndexesOptions).toArray();
    } catch (error) {
        if (error instanceof error_1.MongoError && error.code === error_1.MONGODB_ERROR_CODES.NamespaceNotFound) {
            indexes = [];
        } else {
            throw error;
        }
    }
    const hasFileIndex = !!indexes.find((index)=>{
        const keys = Object.keys(index.key);
        if (keys.length === 2 && index.key.filename === 1 && index.key.uploadDate === 1) {
            return true;
        }
        return false;
    });
    if (!hasFileIndex) {
        remainingTimeMS = stream.timeoutContext?.getRemainingTimeMSOrThrow(`Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`);
        await stream.files.createIndex(index, {
            background: false,
            timeoutMS: remainingTimeMS
        });
    }
    await checkChunksIndex(stream);
}
function createFilesDoc(_id, length, chunkSize, filename, metadata) {
    const ret = {
        _id,
        length,
        chunkSize,
        uploadDate: new Date(),
        filename
    };
    if (metadata) {
        ret.metadata = metadata;
    }
    return ret;
}
function doWrite(stream, chunk, encoding, callback) {
    if (isAborted(stream, callback)) {
        return;
    }
    const inputBuf = __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["Buffer"].isBuffer(chunk) ? chunk : __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["Buffer"].from(chunk, encoding);
    stream.length += inputBuf.length;
    // Input is small enough to fit in our buffer
    if (stream.pos + inputBuf.length < stream.chunkSizeBytes) {
        inputBuf.copy(stream.bufToStore, stream.pos);
        stream.pos += inputBuf.length;
        __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].nextTick(callback);
        return;
    }
    // Otherwise, buffer is too big for current chunk, so we need to flush
    // to MongoDB.
    let inputBufRemaining = inputBuf.length;
    let spaceRemaining = stream.chunkSizeBytes - stream.pos;
    let numToCopy = Math.min(spaceRemaining, inputBuf.length);
    let outstandingRequests = 0;
    while(inputBufRemaining > 0){
        const inputBufPos = inputBuf.length - inputBufRemaining;
        inputBuf.copy(stream.bufToStore, stream.pos, inputBufPos, inputBufPos + numToCopy);
        stream.pos += numToCopy;
        spaceRemaining -= numToCopy;
        let doc;
        if (spaceRemaining === 0) {
            doc = createChunkDoc(stream.id, stream.n, __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["Buffer"].from(stream.bufToStore));
            const remainingTimeMS = stream.timeoutContext?.remainingTimeMS;
            if (remainingTimeMS != null && remainingTimeMS <= 0) {
                return handleError(stream, new error_1.MongoOperationTimeoutError(`Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`), callback);
            }
            ++stream.state.outstandingRequests;
            ++outstandingRequests;
            if (isAborted(stream, callback)) {
                return;
            }
            stream.chunks.insertOne(doc, {
                writeConcern: stream.writeConcern,
                timeoutMS: remainingTimeMS
            }).then(()=>{
                --stream.state.outstandingRequests;
                --outstandingRequests;
                if (!outstandingRequests) {
                    checkDone(stream, callback);
                }
            }, (error)=>{
                return handleError(stream, error, callback);
            });
            spaceRemaining = stream.chunkSizeBytes;
            stream.pos = 0;
            ++stream.n;
        }
        inputBufRemaining -= numToCopy;
        numToCopy = Math.min(spaceRemaining, inputBufRemaining);
    }
}
function writeRemnant(stream, callback) {
    // Buffer is empty, so don't bother to insert
    if (stream.pos === 0) {
        return checkDone(stream, callback);
    }
    // Create a new buffer to make sure the buffer isn't bigger than it needs
    // to be.
    const remnant = __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["Buffer"].alloc(stream.pos);
    stream.bufToStore.copy(remnant, 0, 0, stream.pos);
    const doc = createChunkDoc(stream.id, stream.n, remnant);
    // If the stream was aborted, do not write remnant
    if (isAborted(stream, callback)) {
        return;
    }
    const remainingTimeMS = stream.timeoutContext?.remainingTimeMS;
    if (remainingTimeMS != null && remainingTimeMS <= 0) {
        return handleError(stream, new error_1.MongoOperationTimeoutError(`Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`), callback);
    }
    ++stream.state.outstandingRequests;
    stream.chunks.insertOne(doc, {
        writeConcern: stream.writeConcern,
        timeoutMS: remainingTimeMS
    }).then(()=>{
        --stream.state.outstandingRequests;
        checkDone(stream, callback);
    }, (error)=>{
        return handleError(stream, error, callback);
    });
}
function isAborted(stream, callback) {
    if (stream.state.aborted) {
        __TURBOPACK__imported__module__$5b$project$5d2f$Pathology$2f$patho$2d$client$2f$node_modules$2f2e$pnpm$2f$next$40$16$2e$0$2e$6_$40$babel$2b$core$40$7$2e$28$2e$5_react$2d$dom$40$19$2e$2$2e$0_react$40$19$2e$2$2e$0_$5f$react$40$19$2e$2$2e$0$2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$app$2d$client$5d$__$28$ecmascript$29$__["default"].nextTick(callback, new error_1.MongoAPIError('Stream has been aborted'));
        return true;
    }
    return false;
} //# sourceMappingURL=upload.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/gridfs/index.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.GridFSBucket = void 0;
const error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
const mongo_types_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/mongo_types.js [app-client] (ecmascript)");
const timeout_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/timeout.js [app-client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/utils.js [app-client] (ecmascript)");
const write_concern_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/write_concern.js [app-client] (ecmascript)");
const download_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/gridfs/download.js [app-client] (ecmascript)");
const upload_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/gridfs/upload.js [app-client] (ecmascript)");
const DEFAULT_GRIDFS_BUCKET_OPTIONS = {
    bucketName: 'fs',
    chunkSizeBytes: 255 * 1024
};
/**
 * Constructor for a streaming GridFS interface
 * @public
 */ class GridFSBucket extends mongo_types_1.TypedEventEmitter {
    /**
     * When the first call to openUploadStream is made, the upload stream will
     * check to see if it needs to create the proper indexes on the chunks and
     * files collections. This event is fired either when 1) it determines that
     * no index creation is necessary, 2) when it successfully creates the
     * necessary indexes.
     * @event
     */ static{
        this.INDEX = 'index';
    }
    constructor(db, options){
        super();
        this.on('error', utils_1.noop);
        this.setMaxListeners(0);
        const privateOptions = (0, utils_1.resolveOptions)(db, {
            ...DEFAULT_GRIDFS_BUCKET_OPTIONS,
            ...options,
            writeConcern: write_concern_1.WriteConcern.fromOptions(options)
        });
        this.s = {
            db,
            options: privateOptions,
            _chunksCollection: db.collection(privateOptions.bucketName + '.chunks'),
            _filesCollection: db.collection(privateOptions.bucketName + '.files'),
            checkedIndexes: false,
            calledOpenUploadStream: false
        };
    }
    /**
     * Returns a writable stream (GridFSBucketWriteStream) for writing
     * buffers to GridFS. The stream's 'id' property contains the resulting
     * file's id.
     *
     * @param filename - The value of the 'filename' key in the files doc
     * @param options - Optional settings.
     */ openUploadStream(filename, options) {
        return new upload_1.GridFSBucketWriteStream(this, filename, {
            timeoutMS: this.s.options.timeoutMS,
            ...options
        });
    }
    /**
     * Returns a writable stream (GridFSBucketWriteStream) for writing
     * buffers to GridFS for a custom file id. The stream's 'id' property contains the resulting
     * file's id.
     */ openUploadStreamWithId(id, filename, options) {
        return new upload_1.GridFSBucketWriteStream(this, filename, {
            timeoutMS: this.s.options.timeoutMS,
            ...options,
            id
        });
    }
    /** Returns a readable stream (GridFSBucketReadStream) for streaming file data from GridFS. */ openDownloadStream(id, options) {
        return new download_1.GridFSBucketReadStream(this.s._chunksCollection, this.s._filesCollection, this.s.options.readPreference, {
            _id: id
        }, {
            timeoutMS: this.s.options.timeoutMS,
            ...options
        });
    }
    /**
     * Deletes a file with the given id
     *
     * @param id - The id of the file doc
     */ async delete(id, options) {
        const { timeoutMS } = (0, utils_1.resolveOptions)(this.s.db, options);
        let timeoutContext = undefined;
        if (timeoutMS) {
            timeoutContext = new timeout_1.CSOTTimeoutContext({
                timeoutMS,
                serverSelectionTimeoutMS: this.s.db.client.s.options.serverSelectionTimeoutMS
            });
        }
        const { deletedCount } = await this.s._filesCollection.deleteOne({
            _id: id
        }, {
            timeoutMS: timeoutContext?.remainingTimeMS
        });
        const remainingTimeMS = timeoutContext?.remainingTimeMS;
        if (remainingTimeMS != null && remainingTimeMS <= 0) throw new error_1.MongoOperationTimeoutError(`Timed out after ${timeoutMS}ms`);
        // Delete orphaned chunks before returning FileNotFound
        await this.s._chunksCollection.deleteMany({
            files_id: id
        }, {
            timeoutMS: remainingTimeMS
        });
        if (deletedCount === 0) {
            // TODO(NODE-3483): Replace with more appropriate error
            // Consider creating new error MongoGridFSFileNotFoundError
            throw new error_1.MongoRuntimeError(`File not found for id ${id}`);
        }
    }
    /** Convenience wrapper around find on the files collection */ find(filter = {}, options = {}) {
        return this.s._filesCollection.find(filter, options);
    }
    /**
     * Returns a readable stream (GridFSBucketReadStream) for streaming the
     * file with the given name from GridFS. If there are multiple files with
     * the same name, this will stream the most recent file with the given name
     * (as determined by the `uploadDate` field). You can set the `revision`
     * option to change this behavior.
     */ openDownloadStreamByName(filename, options) {
        let sort = {
            uploadDate: -1
        };
        let skip = undefined;
        if (options && options.revision != null) {
            if (options.revision >= 0) {
                sort = {
                    uploadDate: 1
                };
                skip = options.revision;
            } else {
                skip = -options.revision - 1;
            }
        }
        return new download_1.GridFSBucketReadStream(this.s._chunksCollection, this.s._filesCollection, this.s.options.readPreference, {
            filename
        }, {
            timeoutMS: this.s.options.timeoutMS,
            ...options,
            sort,
            skip
        });
    }
    /**
     * Renames the file with the given _id to the given string
     *
     * @param id - the id of the file to rename
     * @param filename - new name for the file
     */ async rename(id, filename, options) {
        const filter = {
            _id: id
        };
        const update = {
            $set: {
                filename
            }
        };
        const { matchedCount } = await this.s._filesCollection.updateOne(filter, update, options);
        if (matchedCount === 0) {
            throw new error_1.MongoRuntimeError(`File with id ${id} not found`);
        }
    }
    /** Removes this bucket's files collection, followed by its chunks collection. */ async drop(options) {
        const { timeoutMS } = (0, utils_1.resolveOptions)(this.s.db, options);
        let timeoutContext = undefined;
        if (timeoutMS) {
            timeoutContext = new timeout_1.CSOTTimeoutContext({
                timeoutMS,
                serverSelectionTimeoutMS: this.s.db.client.s.options.serverSelectionTimeoutMS
            });
        }
        if (timeoutContext) {
            await this.s._filesCollection.drop({
                timeoutMS: timeoutContext.remainingTimeMS
            });
            const remainingTimeMS = timeoutContext.getRemainingTimeMSOrThrow(`Timed out after ${timeoutMS}ms`);
            await this.s._chunksCollection.drop({
                timeoutMS: remainingTimeMS
            });
        } else {
            await this.s._filesCollection.drop();
            await this.s._chunksCollection.drop();
        }
    }
}
exports.GridFSBucket = GridFSBucket; //# sourceMappingURL=index.js.map
}),
"[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/index.js [app-client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.MongoRuntimeError = exports.MongoParseError = exports.MongoOperationTimeoutError = exports.MongoOIDCError = exports.MongoNotConnectedError = exports.MongoNetworkTimeoutError = exports.MongoNetworkError = exports.MongoMissingDependencyError = exports.MongoMissingCredentialsError = exports.MongoKerberosError = exports.MongoInvalidArgumentError = exports.MongoGridFSStreamError = exports.MongoGridFSChunkError = exports.MongoGCPError = exports.MongoExpiredSessionError = exports.MongoError = exports.MongoDriverError = exports.MongoDecompressionError = exports.MongoCursorInUseError = exports.MongoCursorExhaustedError = exports.MongoCompatibilityError = exports.MongoClientClosedError = exports.MongoClientBulkWriteExecutionError = exports.MongoClientBulkWriteError = exports.MongoClientBulkWriteCursorError = exports.MongoChangeStreamError = exports.MongoBatchReExecutionError = exports.MongoAzureError = exports.MongoAWSError = exports.MongoAPIError = exports.ExplainableCursor = exports.ChangeStreamCursor = exports.ClientEncryption = exports.MongoBulkWriteError = exports.UUID = exports.Timestamp = exports.ObjectId = exports.MinKey = exports.MaxKey = exports.Long = exports.Int32 = exports.Double = exports.Decimal128 = exports.DBRef = exports.Code = exports.BSONType = exports.BSONSymbol = exports.BSONRegExp = exports.Binary = exports.BSON = void 0;
exports.CommandStartedEvent = exports.CommandFailedEvent = exports.WriteConcern = exports.ReadPreference = exports.ReadConcern = exports.TopologyType = exports.ServerType = exports.ReadPreferenceMode = exports.ReadConcernLevel = exports.ProfilingLevel = exports.ReturnDocument = exports.SeverityLevel = exports.MongoLoggableComponent = exports.ServerApiVersion = exports.ExplainVerbosity = exports.MongoErrorLabel = exports.CursorTimeoutMode = exports.CURSOR_FLAGS = exports.Compressor = exports.AuthMechanism = exports.GSSAPICanonicalizationValue = exports.AutoEncryptionLoggerLevel = exports.BatchType = exports.UnorderedBulkOperation = exports.OrderedBulkOperation = exports.MongoClient = exports.ListIndexesCursor = exports.ListCollectionsCursor = exports.GridFSBucketWriteStream = exports.GridFSBucketReadStream = exports.GridFSBucket = exports.FindCursor = exports.Db = exports.Collection = exports.ClientSession = exports.ChangeStream = exports.CancellationToken = exports.AggregationCursor = exports.Admin = exports.AbstractCursor = exports.MongoWriteConcernError = exports.MongoUnexpectedServerResponseError = exports.MongoTransactionError = exports.MongoTopologyClosedError = exports.MongoTailableCursorError = exports.MongoSystemError = exports.MongoStalePrimaryError = exports.MongoServerSelectionError = exports.MongoServerError = exports.MongoServerClosedError = void 0;
exports.MongoClientAuthProviders = exports.MongoCryptKMSRequestNetworkTimeoutError = exports.MongoCryptInvalidArgumentError = exports.MongoCryptError = exports.MongoCryptCreateEncryptedCollectionError = exports.MongoCryptCreateDataKeyError = exports.MongoCryptAzureKMSRequestError = exports.SrvPollingEvent = exports.WaitingForSuitableServerEvent = exports.ServerSelectionSucceededEvent = exports.ServerSelectionStartedEvent = exports.ServerSelectionFailedEvent = exports.ServerSelectionEvent = exports.TopologyOpeningEvent = exports.TopologyDescriptionChangedEvent = exports.TopologyClosedEvent = exports.ServerOpeningEvent = exports.ServerHeartbeatSucceededEvent = exports.ServerHeartbeatStartedEvent = exports.ServerHeartbeatFailedEvent = exports.ServerDescriptionChangedEvent = exports.ServerClosedEvent = exports.ConnectionReadyEvent = exports.ConnectionPoolReadyEvent = exports.ConnectionPoolMonitoringEvent = exports.ConnectionPoolCreatedEvent = exports.ConnectionPoolClosedEvent = exports.ConnectionPoolClearedEvent = exports.ConnectionCreatedEvent = exports.ConnectionClosedEvent = exports.ConnectionCheckOutStartedEvent = exports.ConnectionCheckOutFailedEvent = exports.ConnectionCheckedOutEvent = exports.ConnectionCheckedInEvent = exports.CommandSucceededEvent = void 0;
const admin_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/admin.js [app-client] (ecmascript)");
Object.defineProperty(exports, "Admin", {
    enumerable: true,
    get: function() {
        return admin_1.Admin;
    }
});
const ordered_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bulk/ordered.js [app-client] (ecmascript)");
Object.defineProperty(exports, "OrderedBulkOperation", {
    enumerable: true,
    get: function() {
        return ordered_1.OrderedBulkOperation;
    }
});
const unordered_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bulk/unordered.js [app-client] (ecmascript)");
Object.defineProperty(exports, "UnorderedBulkOperation", {
    enumerable: true,
    get: function() {
        return unordered_1.UnorderedBulkOperation;
    }
});
const change_stream_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/change_stream.js [app-client] (ecmascript)");
Object.defineProperty(exports, "ChangeStream", {
    enumerable: true,
    get: function() {
        return change_stream_1.ChangeStream;
    }
});
const collection_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/collection.js [app-client] (ecmascript)");
Object.defineProperty(exports, "Collection", {
    enumerable: true,
    get: function() {
        return collection_1.Collection;
    }
});
const abstract_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/abstract_cursor.js [app-client] (ecmascript)");
Object.defineProperty(exports, "AbstractCursor", {
    enumerable: true,
    get: function() {
        return abstract_cursor_1.AbstractCursor;
    }
});
const aggregation_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/aggregation_cursor.js [app-client] (ecmascript)");
Object.defineProperty(exports, "AggregationCursor", {
    enumerable: true,
    get: function() {
        return aggregation_cursor_1.AggregationCursor;
    }
});
const find_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/find_cursor.js [app-client] (ecmascript)");
Object.defineProperty(exports, "FindCursor", {
    enumerable: true,
    get: function() {
        return find_cursor_1.FindCursor;
    }
});
const list_collections_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/list_collections_cursor.js [app-client] (ecmascript)");
Object.defineProperty(exports, "ListCollectionsCursor", {
    enumerable: true,
    get: function() {
        return list_collections_cursor_1.ListCollectionsCursor;
    }
});
const list_indexes_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/list_indexes_cursor.js [app-client] (ecmascript)");
Object.defineProperty(exports, "ListIndexesCursor", {
    enumerable: true,
    get: function() {
        return list_indexes_cursor_1.ListIndexesCursor;
    }
});
const db_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/db.js [app-client] (ecmascript)");
Object.defineProperty(exports, "Db", {
    enumerable: true,
    get: function() {
        return db_1.Db;
    }
});
const gridfs_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/gridfs/index.js [app-client] (ecmascript)");
Object.defineProperty(exports, "GridFSBucket", {
    enumerable: true,
    get: function() {
        return gridfs_1.GridFSBucket;
    }
});
const download_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/gridfs/download.js [app-client] (ecmascript)");
Object.defineProperty(exports, "GridFSBucketReadStream", {
    enumerable: true,
    get: function() {
        return download_1.GridFSBucketReadStream;
    }
});
const upload_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/gridfs/upload.js [app-client] (ecmascript)");
Object.defineProperty(exports, "GridFSBucketWriteStream", {
    enumerable: true,
    get: function() {
        return upload_1.GridFSBucketWriteStream;
    }
});
const mongo_client_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/mongo_client.js [app-client] (ecmascript)");
Object.defineProperty(exports, "MongoClient", {
    enumerable: true,
    get: function() {
        return mongo_client_1.MongoClient;
    }
});
const mongo_types_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/mongo_types.js [app-client] (ecmascript)");
Object.defineProperty(exports, "CancellationToken", {
    enumerable: true,
    get: function() {
        return mongo_types_1.CancellationToken;
    }
});
const sessions_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/sessions.js [app-client] (ecmascript)");
Object.defineProperty(exports, "ClientSession", {
    enumerable: true,
    get: function() {
        return sessions_1.ClientSession;
    }
});
/** @public */ var bson_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bson.js [app-client] (ecmascript)");
Object.defineProperty(exports, "BSON", {
    enumerable: true,
    get: function() {
        return bson_1.BSON;
    }
});
var bson_2 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bson.js [app-client] (ecmascript)");
Object.defineProperty(exports, "Binary", {
    enumerable: true,
    get: function() {
        return bson_2.Binary;
    }
});
Object.defineProperty(exports, "BSONRegExp", {
    enumerable: true,
    get: function() {
        return bson_2.BSONRegExp;
    }
});
Object.defineProperty(exports, "BSONSymbol", {
    enumerable: true,
    get: function() {
        return bson_2.BSONSymbol;
    }
});
Object.defineProperty(exports, "BSONType", {
    enumerable: true,
    get: function() {
        return bson_2.BSONType;
    }
});
Object.defineProperty(exports, "Code", {
    enumerable: true,
    get: function() {
        return bson_2.Code;
    }
});
Object.defineProperty(exports, "DBRef", {
    enumerable: true,
    get: function() {
        return bson_2.DBRef;
    }
});
Object.defineProperty(exports, "Decimal128", {
    enumerable: true,
    get: function() {
        return bson_2.Decimal128;
    }
});
Object.defineProperty(exports, "Double", {
    enumerable: true,
    get: function() {
        return bson_2.Double;
    }
});
Object.defineProperty(exports, "Int32", {
    enumerable: true,
    get: function() {
        return bson_2.Int32;
    }
});
Object.defineProperty(exports, "Long", {
    enumerable: true,
    get: function() {
        return bson_2.Long;
    }
});
Object.defineProperty(exports, "MaxKey", {
    enumerable: true,
    get: function() {
        return bson_2.MaxKey;
    }
});
Object.defineProperty(exports, "MinKey", {
    enumerable: true,
    get: function() {
        return bson_2.MinKey;
    }
});
Object.defineProperty(exports, "ObjectId", {
    enumerable: true,
    get: function() {
        return bson_2.ObjectId;
    }
});
Object.defineProperty(exports, "Timestamp", {
    enumerable: true,
    get: function() {
        return bson_2.Timestamp;
    }
});
Object.defineProperty(exports, "UUID", {
    enumerable: true,
    get: function() {
        return bson_2.UUID;
    }
});
var common_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bulk/common.js [app-client] (ecmascript)");
Object.defineProperty(exports, "MongoBulkWriteError", {
    enumerable: true,
    get: function() {
        return common_1.MongoBulkWriteError;
    }
});
var client_encryption_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/client-side-encryption/client_encryption.js [app-client] (ecmascript)");
Object.defineProperty(exports, "ClientEncryption", {
    enumerable: true,
    get: function() {
        return client_encryption_1.ClientEncryption;
    }
});
var change_stream_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/change_stream_cursor.js [app-client] (ecmascript)");
Object.defineProperty(exports, "ChangeStreamCursor", {
    enumerable: true,
    get: function() {
        return change_stream_cursor_1.ChangeStreamCursor;
    }
});
var explainable_cursor_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/explainable_cursor.js [app-client] (ecmascript)");
Object.defineProperty(exports, "ExplainableCursor", {
    enumerable: true,
    get: function() {
        return explainable_cursor_1.ExplainableCursor;
    }
});
var error_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
Object.defineProperty(exports, "MongoAPIError", {
    enumerable: true,
    get: function() {
        return error_1.MongoAPIError;
    }
});
Object.defineProperty(exports, "MongoAWSError", {
    enumerable: true,
    get: function() {
        return error_1.MongoAWSError;
    }
});
Object.defineProperty(exports, "MongoAzureError", {
    enumerable: true,
    get: function() {
        return error_1.MongoAzureError;
    }
});
Object.defineProperty(exports, "MongoBatchReExecutionError", {
    enumerable: true,
    get: function() {
        return error_1.MongoBatchReExecutionError;
    }
});
Object.defineProperty(exports, "MongoChangeStreamError", {
    enumerable: true,
    get: function() {
        return error_1.MongoChangeStreamError;
    }
});
Object.defineProperty(exports, "MongoClientBulkWriteCursorError", {
    enumerable: true,
    get: function() {
        return error_1.MongoClientBulkWriteCursorError;
    }
});
Object.defineProperty(exports, "MongoClientBulkWriteError", {
    enumerable: true,
    get: function() {
        return error_1.MongoClientBulkWriteError;
    }
});
Object.defineProperty(exports, "MongoClientBulkWriteExecutionError", {
    enumerable: true,
    get: function() {
        return error_1.MongoClientBulkWriteExecutionError;
    }
});
Object.defineProperty(exports, "MongoClientClosedError", {
    enumerable: true,
    get: function() {
        return error_1.MongoClientClosedError;
    }
});
Object.defineProperty(exports, "MongoCompatibilityError", {
    enumerable: true,
    get: function() {
        return error_1.MongoCompatibilityError;
    }
});
Object.defineProperty(exports, "MongoCursorExhaustedError", {
    enumerable: true,
    get: function() {
        return error_1.MongoCursorExhaustedError;
    }
});
Object.defineProperty(exports, "MongoCursorInUseError", {
    enumerable: true,
    get: function() {
        return error_1.MongoCursorInUseError;
    }
});
Object.defineProperty(exports, "MongoDecompressionError", {
    enumerable: true,
    get: function() {
        return error_1.MongoDecompressionError;
    }
});
Object.defineProperty(exports, "MongoDriverError", {
    enumerable: true,
    get: function() {
        return error_1.MongoDriverError;
    }
});
Object.defineProperty(exports, "MongoError", {
    enumerable: true,
    get: function() {
        return error_1.MongoError;
    }
});
Object.defineProperty(exports, "MongoExpiredSessionError", {
    enumerable: true,
    get: function() {
        return error_1.MongoExpiredSessionError;
    }
});
Object.defineProperty(exports, "MongoGCPError", {
    enumerable: true,
    get: function() {
        return error_1.MongoGCPError;
    }
});
Object.defineProperty(exports, "MongoGridFSChunkError", {
    enumerable: true,
    get: function() {
        return error_1.MongoGridFSChunkError;
    }
});
Object.defineProperty(exports, "MongoGridFSStreamError", {
    enumerable: true,
    get: function() {
        return error_1.MongoGridFSStreamError;
    }
});
Object.defineProperty(exports, "MongoInvalidArgumentError", {
    enumerable: true,
    get: function() {
        return error_1.MongoInvalidArgumentError;
    }
});
Object.defineProperty(exports, "MongoKerberosError", {
    enumerable: true,
    get: function() {
        return error_1.MongoKerberosError;
    }
});
Object.defineProperty(exports, "MongoMissingCredentialsError", {
    enumerable: true,
    get: function() {
        return error_1.MongoMissingCredentialsError;
    }
});
Object.defineProperty(exports, "MongoMissingDependencyError", {
    enumerable: true,
    get: function() {
        return error_1.MongoMissingDependencyError;
    }
});
Object.defineProperty(exports, "MongoNetworkError", {
    enumerable: true,
    get: function() {
        return error_1.MongoNetworkError;
    }
});
Object.defineProperty(exports, "MongoNetworkTimeoutError", {
    enumerable: true,
    get: function() {
        return error_1.MongoNetworkTimeoutError;
    }
});
Object.defineProperty(exports, "MongoNotConnectedError", {
    enumerable: true,
    get: function() {
        return error_1.MongoNotConnectedError;
    }
});
Object.defineProperty(exports, "MongoOIDCError", {
    enumerable: true,
    get: function() {
        return error_1.MongoOIDCError;
    }
});
Object.defineProperty(exports, "MongoOperationTimeoutError", {
    enumerable: true,
    get: function() {
        return error_1.MongoOperationTimeoutError;
    }
});
Object.defineProperty(exports, "MongoParseError", {
    enumerable: true,
    get: function() {
        return error_1.MongoParseError;
    }
});
Object.defineProperty(exports, "MongoRuntimeError", {
    enumerable: true,
    get: function() {
        return error_1.MongoRuntimeError;
    }
});
Object.defineProperty(exports, "MongoServerClosedError", {
    enumerable: true,
    get: function() {
        return error_1.MongoServerClosedError;
    }
});
Object.defineProperty(exports, "MongoServerError", {
    enumerable: true,
    get: function() {
        return error_1.MongoServerError;
    }
});
Object.defineProperty(exports, "MongoServerSelectionError", {
    enumerable: true,
    get: function() {
        return error_1.MongoServerSelectionError;
    }
});
Object.defineProperty(exports, "MongoStalePrimaryError", {
    enumerable: true,
    get: function() {
        return error_1.MongoStalePrimaryError;
    }
});
Object.defineProperty(exports, "MongoSystemError", {
    enumerable: true,
    get: function() {
        return error_1.MongoSystemError;
    }
});
Object.defineProperty(exports, "MongoTailableCursorError", {
    enumerable: true,
    get: function() {
        return error_1.MongoTailableCursorError;
    }
});
Object.defineProperty(exports, "MongoTopologyClosedError", {
    enumerable: true,
    get: function() {
        return error_1.MongoTopologyClosedError;
    }
});
Object.defineProperty(exports, "MongoTransactionError", {
    enumerable: true,
    get: function() {
        return error_1.MongoTransactionError;
    }
});
Object.defineProperty(exports, "MongoUnexpectedServerResponseError", {
    enumerable: true,
    get: function() {
        return error_1.MongoUnexpectedServerResponseError;
    }
});
Object.defineProperty(exports, "MongoWriteConcernError", {
    enumerable: true,
    get: function() {
        return error_1.MongoWriteConcernError;
    }
});
// enums
var common_2 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/bulk/common.js [app-client] (ecmascript)");
Object.defineProperty(exports, "BatchType", {
    enumerable: true,
    get: function() {
        return common_2.BatchType;
    }
});
var auto_encrypter_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/client-side-encryption/auto_encrypter.js [app-client] (ecmascript)");
Object.defineProperty(exports, "AutoEncryptionLoggerLevel", {
    enumerable: true,
    get: function() {
        return auto_encrypter_1.AutoEncryptionLoggerLevel;
    }
});
var gssapi_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cmap/auth/gssapi.js [app-client] (ecmascript)");
Object.defineProperty(exports, "GSSAPICanonicalizationValue", {
    enumerable: true,
    get: function() {
        return gssapi_1.GSSAPICanonicalizationValue;
    }
});
var providers_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cmap/auth/providers.js [app-client] (ecmascript)");
Object.defineProperty(exports, "AuthMechanism", {
    enumerable: true,
    get: function() {
        return providers_1.AuthMechanism;
    }
});
var compression_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cmap/wire_protocol/compression.js [app-client] (ecmascript)");
Object.defineProperty(exports, "Compressor", {
    enumerable: true,
    get: function() {
        return compression_1.Compressor;
    }
});
var abstract_cursor_2 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cursor/abstract_cursor.js [app-client] (ecmascript)");
Object.defineProperty(exports, "CURSOR_FLAGS", {
    enumerable: true,
    get: function() {
        return abstract_cursor_2.CURSOR_FLAGS;
    }
});
Object.defineProperty(exports, "CursorTimeoutMode", {
    enumerable: true,
    get: function() {
        return abstract_cursor_2.CursorTimeoutMode;
    }
});
var error_2 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/error.js [app-client] (ecmascript)");
Object.defineProperty(exports, "MongoErrorLabel", {
    enumerable: true,
    get: function() {
        return error_2.MongoErrorLabel;
    }
});
var explain_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/explain.js [app-client] (ecmascript)");
Object.defineProperty(exports, "ExplainVerbosity", {
    enumerable: true,
    get: function() {
        return explain_1.ExplainVerbosity;
    }
});
var mongo_client_2 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/mongo_client.js [app-client] (ecmascript)");
Object.defineProperty(exports, "ServerApiVersion", {
    enumerable: true,
    get: function() {
        return mongo_client_2.ServerApiVersion;
    }
});
var mongo_logger_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/mongo_logger.js [app-client] (ecmascript)");
Object.defineProperty(exports, "MongoLoggableComponent", {
    enumerable: true,
    get: function() {
        return mongo_logger_1.MongoLoggableComponent;
    }
});
Object.defineProperty(exports, "SeverityLevel", {
    enumerable: true,
    get: function() {
        return mongo_logger_1.SeverityLevel;
    }
});
var find_and_modify_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/find_and_modify.js [app-client] (ecmascript)");
Object.defineProperty(exports, "ReturnDocument", {
    enumerable: true,
    get: function() {
        return find_and_modify_1.ReturnDocument;
    }
});
var set_profiling_level_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/operations/set_profiling_level.js [app-client] (ecmascript)");
Object.defineProperty(exports, "ProfilingLevel", {
    enumerable: true,
    get: function() {
        return set_profiling_level_1.ProfilingLevel;
    }
});
var read_concern_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/read_concern.js [app-client] (ecmascript)");
Object.defineProperty(exports, "ReadConcernLevel", {
    enumerable: true,
    get: function() {
        return read_concern_1.ReadConcernLevel;
    }
});
var read_preference_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/read_preference.js [app-client] (ecmascript)");
Object.defineProperty(exports, "ReadPreferenceMode", {
    enumerable: true,
    get: function() {
        return read_preference_1.ReadPreferenceMode;
    }
});
var common_3 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/sdam/common.js [app-client] (ecmascript)");
Object.defineProperty(exports, "ServerType", {
    enumerable: true,
    get: function() {
        return common_3.ServerType;
    }
});
Object.defineProperty(exports, "TopologyType", {
    enumerable: true,
    get: function() {
        return common_3.TopologyType;
    }
});
var read_concern_2 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/read_concern.js [app-client] (ecmascript)");
Object.defineProperty(exports, "ReadConcern", {
    enumerable: true,
    get: function() {
        return read_concern_2.ReadConcern;
    }
});
var read_preference_2 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/read_preference.js [app-client] (ecmascript)");
Object.defineProperty(exports, "ReadPreference", {
    enumerable: true,
    get: function() {
        return read_preference_2.ReadPreference;
    }
});
var write_concern_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/write_concern.js [app-client] (ecmascript)");
Object.defineProperty(exports, "WriteConcern", {
    enumerable: true,
    get: function() {
        return write_concern_1.WriteConcern;
    }
});
// events
var command_monitoring_events_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cmap/command_monitoring_events.js [app-client] (ecmascript)");
Object.defineProperty(exports, "CommandFailedEvent", {
    enumerable: true,
    get: function() {
        return command_monitoring_events_1.CommandFailedEvent;
    }
});
Object.defineProperty(exports, "CommandStartedEvent", {
    enumerable: true,
    get: function() {
        return command_monitoring_events_1.CommandStartedEvent;
    }
});
Object.defineProperty(exports, "CommandSucceededEvent", {
    enumerable: true,
    get: function() {
        return command_monitoring_events_1.CommandSucceededEvent;
    }
});
var connection_pool_events_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/cmap/connection_pool_events.js [app-client] (ecmascript)");
Object.defineProperty(exports, "ConnectionCheckedInEvent", {
    enumerable: true,
    get: function() {
        return connection_pool_events_1.ConnectionCheckedInEvent;
    }
});
Object.defineProperty(exports, "ConnectionCheckedOutEvent", {
    enumerable: true,
    get: function() {
        return connection_pool_events_1.ConnectionCheckedOutEvent;
    }
});
Object.defineProperty(exports, "ConnectionCheckOutFailedEvent", {
    enumerable: true,
    get: function() {
        return connection_pool_events_1.ConnectionCheckOutFailedEvent;
    }
});
Object.defineProperty(exports, "ConnectionCheckOutStartedEvent", {
    enumerable: true,
    get: function() {
        return connection_pool_events_1.ConnectionCheckOutStartedEvent;
    }
});
Object.defineProperty(exports, "ConnectionClosedEvent", {
    enumerable: true,
    get: function() {
        return connection_pool_events_1.ConnectionClosedEvent;
    }
});
Object.defineProperty(exports, "ConnectionCreatedEvent", {
    enumerable: true,
    get: function() {
        return connection_pool_events_1.ConnectionCreatedEvent;
    }
});
Object.defineProperty(exports, "ConnectionPoolClearedEvent", {
    enumerable: true,
    get: function() {
        return connection_pool_events_1.ConnectionPoolClearedEvent;
    }
});
Object.defineProperty(exports, "ConnectionPoolClosedEvent", {
    enumerable: true,
    get: function() {
        return connection_pool_events_1.ConnectionPoolClosedEvent;
    }
});
Object.defineProperty(exports, "ConnectionPoolCreatedEvent", {
    enumerable: true,
    get: function() {
        return connection_pool_events_1.ConnectionPoolCreatedEvent;
    }
});
Object.defineProperty(exports, "ConnectionPoolMonitoringEvent", {
    enumerable: true,
    get: function() {
        return connection_pool_events_1.ConnectionPoolMonitoringEvent;
    }
});
Object.defineProperty(exports, "ConnectionPoolReadyEvent", {
    enumerable: true,
    get: function() {
        return connection_pool_events_1.ConnectionPoolReadyEvent;
    }
});
Object.defineProperty(exports, "ConnectionReadyEvent", {
    enumerable: true,
    get: function() {
        return connection_pool_events_1.ConnectionReadyEvent;
    }
});
var events_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/sdam/events.js [app-client] (ecmascript)");
Object.defineProperty(exports, "ServerClosedEvent", {
    enumerable: true,
    get: function() {
        return events_1.ServerClosedEvent;
    }
});
Object.defineProperty(exports, "ServerDescriptionChangedEvent", {
    enumerable: true,
    get: function() {
        return events_1.ServerDescriptionChangedEvent;
    }
});
Object.defineProperty(exports, "ServerHeartbeatFailedEvent", {
    enumerable: true,
    get: function() {
        return events_1.ServerHeartbeatFailedEvent;
    }
});
Object.defineProperty(exports, "ServerHeartbeatStartedEvent", {
    enumerable: true,
    get: function() {
        return events_1.ServerHeartbeatStartedEvent;
    }
});
Object.defineProperty(exports, "ServerHeartbeatSucceededEvent", {
    enumerable: true,
    get: function() {
        return events_1.ServerHeartbeatSucceededEvent;
    }
});
Object.defineProperty(exports, "ServerOpeningEvent", {
    enumerable: true,
    get: function() {
        return events_1.ServerOpeningEvent;
    }
});
Object.defineProperty(exports, "TopologyClosedEvent", {
    enumerable: true,
    get: function() {
        return events_1.TopologyClosedEvent;
    }
});
Object.defineProperty(exports, "TopologyDescriptionChangedEvent", {
    enumerable: true,
    get: function() {
        return events_1.TopologyDescriptionChangedEvent;
    }
});
Object.defineProperty(exports, "TopologyOpeningEvent", {
    enumerable: true,
    get: function() {
        return events_1.TopologyOpeningEvent;
    }
});
var server_selection_events_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/sdam/server_selection_events.js [app-client] (ecmascript)");
Object.defineProperty(exports, "ServerSelectionEvent", {
    enumerable: true,
    get: function() {
        return server_selection_events_1.ServerSelectionEvent;
    }
});
Object.defineProperty(exports, "ServerSelectionFailedEvent", {
    enumerable: true,
    get: function() {
        return server_selection_events_1.ServerSelectionFailedEvent;
    }
});
Object.defineProperty(exports, "ServerSelectionStartedEvent", {
    enumerable: true,
    get: function() {
        return server_selection_events_1.ServerSelectionStartedEvent;
    }
});
Object.defineProperty(exports, "ServerSelectionSucceededEvent", {
    enumerable: true,
    get: function() {
        return server_selection_events_1.ServerSelectionSucceededEvent;
    }
});
Object.defineProperty(exports, "WaitingForSuitableServerEvent", {
    enumerable: true,
    get: function() {
        return server_selection_events_1.WaitingForSuitableServerEvent;
    }
});
var srv_polling_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/sdam/srv_polling.js [app-client] (ecmascript)");
Object.defineProperty(exports, "SrvPollingEvent", {
    enumerable: true,
    get: function() {
        return srv_polling_1.SrvPollingEvent;
    }
});
var errors_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/client-side-encryption/errors.js [app-client] (ecmascript)");
Object.defineProperty(exports, "MongoCryptAzureKMSRequestError", {
    enumerable: true,
    get: function() {
        return errors_1.MongoCryptAzureKMSRequestError;
    }
});
Object.defineProperty(exports, "MongoCryptCreateDataKeyError", {
    enumerable: true,
    get: function() {
        return errors_1.MongoCryptCreateDataKeyError;
    }
});
Object.defineProperty(exports, "MongoCryptCreateEncryptedCollectionError", {
    enumerable: true,
    get: function() {
        return errors_1.MongoCryptCreateEncryptedCollectionError;
    }
});
Object.defineProperty(exports, "MongoCryptError", {
    enumerable: true,
    get: function() {
        return errors_1.MongoCryptError;
    }
});
Object.defineProperty(exports, "MongoCryptInvalidArgumentError", {
    enumerable: true,
    get: function() {
        return errors_1.MongoCryptInvalidArgumentError;
    }
});
Object.defineProperty(exports, "MongoCryptKMSRequestNetworkTimeoutError", {
    enumerable: true,
    get: function() {
        return errors_1.MongoCryptKMSRequestNetworkTimeoutError;
    }
});
var mongo_client_auth_providers_1 = __turbopack_context__.r("[project]/Pathology/patho-client/node_modules/.pnpm/mongodb@7.0.0/node_modules/mongodb/lib/mongo_client_auth_providers.js [app-client] (ecmascript)");
Object.defineProperty(exports, "MongoClientAuthProviders", {
    enumerable: true,
    get: function() {
        return mongo_client_auth_providers_1.MongoClientAuthProviders;
    }
}); //# sourceMappingURL=index.js.map
}),
]);

//# sourceMappingURL=83fca_mongodb_lib_b7da8927._.js.map